2023-11-26 11:23:47,891 [ERROR]: An error occurred: name 'Scraper' is not defined
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 17, in main
    scraper = Scraper()
              ^^^^^^^
NameError: name 'Scraper' is not defined
2023-11-26 11:24:11,613 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO_INCR' at line 1
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO_INCR' at line 1

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 20, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 16, in __init__
    self.cursor.execute(f'''
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO_INCR' at line 1
2023-11-26 11:26:30,152 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO INCR' at line 1
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO INCR' at line 1

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 20, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 16, in __init__
    self.cursor.execute(f'''
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO INCR' at line 1
2023-11-26 11:26:41,597 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTOINCRE' at line 1
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTOINCRE' at line 1

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 20, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 16, in __init__
    self.cursor.execute(f'''
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTOINCRE' at line 1
2023-11-26 11:27:11,670 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO_INCR' at line 1
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO_INCR' at line 1

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 20, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 16, in __init__
    self.cursor.execute(f'''
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO_INCR' at line 1
2023-11-26 20:15:16,582 [ERROR]: An error occurred: 'Scraper4' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 30, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper4' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-11-26 20:15:48,059 [INFO]: Scraping and storing data completed successfully.
2023-11-26 20:16:57,169 [INFO]: Scraping and storing data completed successfully.
2023-11-26 20:26:18,470 [ERROR]: An error occurred: cannot access local variable 'continue_to_next_name' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 33, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper6.py", line 116, in scrape_with_names
    if continue_to_next_name:
       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'continue_to_next_name' where it is not associated with a value
2023-11-27 22:08:00,632 [ERROR]: An error occurred: 'Scraper8' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 34, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper8' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-11-27 22:08:55,609 [ERROR]: An error occurred: name 'HTMLSession' is not defined
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 139, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 130, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 52, in scrape_single
    session = HTMLSession()
              ^^^^^^^^^^^
NameError: name 'HTMLSession' is not defined
2023-11-27 22:26:22,555 [ERROR]: An error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/packages/six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 139, in scrape_with_refcodes
    for code in refcodes:
        ^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 130, in scrape_single_thread
    time.sleep(0.3)
             ^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 53, in scrape_single
    # session = HTMLSession()
               ^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2023-11-27 22:54:17,317 [ERROR]: An error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/packages/six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 140, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 131, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 54, in scrape_single
    response = self.session.post(f'{url}?RefCode={data['refCode']}',headers=headers,allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2023-11-27 23:37:57,325 [ERROR]: An error occurred: 'Scraper9' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper9' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-11-28 00:03:09,445 [ERROR]: An error occurred: 'Scraper3' object has no attribute 'scrape_with_names'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_names():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper3' object has no attribute 'scrape_with_names'. Did you mean: 'scrape_with_refcodes'?
2023-11-28 00:03:46,521 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 140, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 131, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 101, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-11-30 17:55:48,613 [ERROR]: An error occurred: string indices must be integers, not 'str'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 125, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 115, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 70, in scrape_single
    response = requests.post(url, headers=headers,cookies=self.extracted_cookies, data=data, proxies=proxies, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 473, in prepare_request
    cookies = cookiejar_from_dict(cookies)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/cookies.py", line 537, in cookiejar_from_dict
    cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))
                                             ~~~~~~~~~~~^^^^^^
TypeError: string indices must be integers, not 'str'
2023-11-30 18:45:51,017 [ERROR]: An error occurred: 'Scraper4' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper4' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-11-30 18:46:02,414 [INFO]: Scraping and storing data completed successfully.
2023-11-30 19:56:35,181 [INFO]: Scraping and storing data completed successfully.
2023-11-30 20:01:05,199 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 42, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 34, in store_data
    for data in data_list:
TypeError: 'NoneType' object is not iterable
2023-12-01 18:58:58,557 [ERROR]: An error occurred: 'Scraper2' object has no attribute 'scrape_with_names'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 46, in main
    for batch_results in scraper.scrape_with_names():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper2' object has no attribute 'scrape_with_names'. Did you mean: 'scrape_with_refcodes'?
2023-12-01 18:59:37,811 [ERROR]: An error occurred: 'Scraper2' object has no attribute 'scrape'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 47, in main
    for batch_results in scraper.scrape():
                         ^^^^^^^^^^^^^^
AttributeError: 'Scraper2' object has no attribute 'scrape'. Did you mean: 'scraper'?
2023-12-01 18:59:51,488 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 47, in main
    for batch_results in scraper.scrape():
TypeError: 'NoneType' object is not iterable
2023-12-01 19:15:14,397 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 47, in main
    for batch_results in scraper.scrape():
TypeError: 'NoneType' object is not iterable
2023-12-01 19:39:48,426 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 51, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 38, in store_data
    ''', (data['first_name'], data['last_name'], data['address'], data['city'], data['state'], data['zip_code']))
          ~~~~^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-01 19:55:12,416 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 51, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 38, in store_data
    ''', (data['first_name'], data['last_name'], data['address'], data['city'], data['state'], data['zip_code']))
          ~~~~^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-01 20:00:23,745 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 51, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 38, in store_data
    ''', (data['first_name'], data['last_name'], data['address'], data['city'], data['state'], data['zip_code']))
          ~~~~^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-01 20:29:55,737 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 51, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 38, in store_data
    ''', (data['first_name'], data['last_name'], data['address'], data['city'], data['state'], data['zip_code']))
          ~~~~^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-01 20:41:30,801 [ERROR]: An error occurred: 'method' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 33, in main
    scraper = Scraper()
              ^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 24, in __init__
    self.renew_cookies()
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 37, in renew_cookies
    for name,value in response.cookies.items:
TypeError: 'method' object is not iterable
2023-12-01 20:42:22,293 [ERROR]: An error occurred: 'method' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 33, in main
    scraper = Scraper()
              ^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 24, in __init__
    self.renew_cookies()
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 38, in renew_cookies
    for name,value in response.cookies.items:
TypeError: 'method' object is not iterable
2023-12-01 20:42:57,639 [ERROR]: An error occurred: 'method' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 33, in main
    scraper = Scraper()
              ^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 24, in __init__
    self.renew_cookies()
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 38, in renew_cookies
    for name,value in response.cookies.items:
TypeError: 'method' object is not iterable
2023-12-01 20:44:06,853 [ERROR]: An error occurred: 'method' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 33, in main
    scraper = Scraper()
              ^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 25, in __init__
    self.renew_cookies()
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 39, in renew_cookies
    for name,value in response.cookies.items:
TypeError: 'method' object is not iterable
2023-12-01 20:44:32,330 [ERROR]: An error occurred: object of type 'generator' has no len()
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 51, in main
    if len(batch_results) == 0:
       ^^^^^^^^^^^^^^^^^^
TypeError: object of type 'generator' has no len()
2023-12-04 08:24:27,315 [ERROR]: An error occurred: 'Scraper29' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper29' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-12-04 09:00:09,677 [ERROR]: An error occurred: 'Scraper2' object has no attribute 'scrape_with_names'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_names():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper2' object has no attribute 'scrape_with_names'. Did you mean: 'scrape_with_refcodes'?
2023-12-04 09:01:05,721 [ERROR]: An error occurred: Invalid URL 'xmydebt.com?RefCode=RD0000011': No scheme supplied. Perhaps you meant https://xmydebt.com?RefCode=RD0000011?
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 151, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 142, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 64, in scrape_single
    response = self.session.get(f'{url}?RefCode={data['refCode']}',allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL 'xmydebt.com?RefCode=RD0000011': No scheme supplied. Perhaps you meant https://xmydebt.com?RefCode=RD0000011?
2023-12-04 09:01:32,078 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 151, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 142, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 112, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:01:37,370 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 151, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 142, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 112, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:02:08,291 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 151, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 142, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 112, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:02:22,691 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 151, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 142, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 112, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:18:39,518 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 166, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 157, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 127, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:31:58,598 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f713d3f6cc0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f713d3f6cc0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f713d3f6cc0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 168, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 159, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 80, in scrape_single
    response = self.session.get(f'https://{url}',headers=headers_caspio, allow_redirects=True) # '?RefCode={data['refCode']}
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f713d3f6cc0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 09:33:21,259 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc2bfb9f800>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fc2bfb9f800>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc2bfb9f800>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 168, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 159, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 80, in scrape_single
    response = self.session.get(f'https://{url}?RefCode={data['refCode']}',headers=headers_caspio, allow_redirects=True) # ?RefCode={data['refCode']}
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc2bfb9f800>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 09:42:56,561 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f35d57a0e00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f35d57a0e00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f35d57a0e00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 189, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 180, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 101, in scrape_single
    response = self.session.post(f'https://{url}?RefCode={data['refCode']}',headers=headers_caspio, allow_redirects=True) # ?RefCode={data['refCode']}
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f35d57a0e00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 09:48:06,422 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fccc5a045f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fccc5a045f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fccc5a045f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 189, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 180, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 101, in scrape_single
    response = self.session.get(f'https://{url}?RefCode={data['refCode']}', allow_redirects=True) # ?RefCode={data['refCode']}
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fccc5a045f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 09:49:35,728 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 192, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 183, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 153, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:50:05,438 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 192, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 183, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 153, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:50:15,477 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 192, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 183, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 153, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:50:19,120 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 192, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 183, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 153, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:51:58,566 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 195, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 186, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 156, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:53:51,176 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 196, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 187, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 157, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:54:54,790 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ff0ce47d520>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7ff0ce47d520>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ff0ce47d520>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 198, in scrape_with_refcodes
    #     for i in range(0,len(refcodes),batch_size):
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 189, in scrape_single_thread
    if result is not None:
             ^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 108, in scrape_single
    response = self.session.get(f'https://{url}?RefCode={data['refCode']}', cookies=self.jar, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ff0ce47d520>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 10:00:50,859 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f84f5bfee10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f84f5bfee10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f84f5bfee10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 198, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 189, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 110, in scrape_single
    response = self.session.get(f'https://{url}?RefCode={data['refCode']}', headers=headers_caspio, cookies=self.jar, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f84f5bfee10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 10:01:37,445 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc1e6b3aa50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fc1e6b3aa50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc1e6b3aa50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 198, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 189, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 110, in scrape_single
    response = self.session.post(f'https://{url}?RefCode={data['refCode']}', headers=headers_caspio, cookies=self.jar, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc1e6b3aa50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 10:25:24,234 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdf18fa3950>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fdf18fa3950>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdf18fa3950>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 147, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 138, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 59, in scrape_single
    response = self.session.get(f'https://{url}?RefCode={data['refCode']}', headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdf18fa3950>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 10:27:13,906 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f35fd9f8cb0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f35fd9f8cb0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f35fd9f8cb0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 147, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 138, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 59, in scrape_single
    response = self.session.post(f'https://{url}',data=data, headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f35fd9f8cb0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-05 02:34:49,570 [ERROR]: An error occurred: 'Scraper35' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper35' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-12-05 02:35:31,049 [WARNING]: Certificate did not match expected hostname: jamessmith2.goadvloan.com. Certificate: {'subject': ((('commonName', '24hrwire.com'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', "Let's Encrypt"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '0320EA253F223168A3341B75AC5D118EC0D0', 'notBefore': 'Oct 12 13:29:42 2023 GMT', 'notAfter': 'Jan 10 13:29:41 2024 GMT', 'subjectAltName': (('DNS', '*.24hrwire.com'), ('DNS', '24hrwire.com')), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}
2023-12-05 02:35:31,088 [WARNING]: Certificate did not match expected hostname: jamessmith.goadvloan.com. Certificate: {'subject': ((('commonName', '24hrwire.com'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', "Let's Encrypt"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '0320EA253F223168A3341B75AC5D118EC0D0', 'notBefore': 'Oct 12 13:29:42 2023 GMT', 'notAfter': 'Jan 10 13:29:41 2024 GMT', 'subjectAltName': (('DNS', '*.24hrwire.com'), ('DNS', '24hrwire.com')), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}
2023-12-05 02:35:31,089 [WARNING]: Certificate did not match expected hostname: jamessmith1.goadvloan.com. Certificate: {'subject': ((('commonName', '24hrwire.com'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', "Let's Encrypt"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '0320EA253F223168A3341B75AC5D118EC0D0', 'notBefore': 'Oct 12 13:29:42 2023 GMT', 'notAfter': 'Jan 10 13:29:41 2024 GMT', 'subjectAltName': (('DNS', '*.24hrwire.com'), ('DNS', '24hrwire.com')), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}
2023-12-05 02:35:51,850 [WARNING]: Certificate did not match expected hostname: jamessmith.goadvloan.com. Certificate: {'subject': ((('commonName', '24hrwire.com'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', "Let's Encrypt"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '0320EA253F223168A3341B75AC5D118EC0D0', 'notBefore': 'Oct 12 13:29:42 2023 GMT', 'notAfter': 'Jan 10 13:29:41 2024 GMT', 'subjectAltName': (('DNS', '*.24hrwire.com'), ('DNS', '24hrwire.com')), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}
2023-12-05 02:35:51,862 [WARNING]: Certificate did not match expected hostname: jamessmith2.goadvloan.com. Certificate: {'subject': ((('commonName', '24hrwire.com'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', "Let's Encrypt"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '0320EA253F223168A3341B75AC5D118EC0D0', 'notBefore': 'Oct 12 13:29:42 2023 GMT', 'notAfter': 'Jan 10 13:29:41 2024 GMT', 'subjectAltName': (('DNS', '*.24hrwire.com'), ('DNS', '24hrwire.com')), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}
2023-12-05 02:35:51,863 [WARNING]: Certificate did not match expected hostname: jamessmith1.goadvloan.com. Certificate: {'subject': ((('commonName', '24hrwire.com'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', "Let's Encrypt"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '0320EA253F223168A3341B75AC5D118EC0D0', 'notBefore': 'Oct 12 13:29:42 2023 GMT', 'notAfter': 'Jan 10 13:29:41 2024 GMT', 'subjectAltName': (('DNS', '*.24hrwire.com'), ('DNS', '24hrwire.com')), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}
2023-12-06 02:12:59,938 [ERROR]: An error occurred: 'Scraper2' object has no attribute 'scrape_with_names'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_names():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper2' object has no attribute 'scrape_with_names'. Did you mean: 'scrape_with_refcodes'?
2023-12-06 02:13:44,597 [ERROR]: An error occurred: Error: 201 - <!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<!-- Favicon -->
	<link href="/campaigns/loanpro/images/loanpro-favicon.png" rel="shortcut icon">
	<!-- meta View port tag -->
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<!-- Title -->
	<title>Online Loan Pro</title>
	<!-- Bootstrap Css -->
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet"
		integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
	<!-- Common & Responsive CSS -->
	<link href="/common/css/index.css" rel="stylesheet" type="text/css">
	<link href="/campaigns/loanpro/css/index.css" rel="stylesheet" type="text/css">
	<link href="/campaigns/loanpro/css/responsive.css" rel="stylesheet" type="text/css">
	<!-- Google Icons -->
	<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0" />
	<!-- Bootstrap JS -->
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"
		integrity="sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm"
		crossorigin="anonymous"></script>
	<!-- Iconnode JS -->
	<script src="//scripts.iconnode.com/78875.js"></script>
	

</head>

<body>
	
<div class="spinner-container"  style="display:none;">
    <div class="loading-spinner">
         <div style="text-align:center;position:fixed;top:0;left:0;right:0;bottom:0;z-index:100;background:rgba(0,0,0,0.7);">
            <div style="height:100%;width:100%;background:url('/common/icons/loading.gif') no-repeat center;margin-top:-233px;"></div>
        </div>
    </div>
</div>
<div>
	<section class="survey">
		<div class="container">
			<div class="row">
				<div class="col-xs-12 col-sm-8 col-sm-offset-1 col-md-5 col-md-offset-1 col-lg-4 col-lg-offset-1">
					<form id="surveyForm" method="POST" action="/form/survey/">
						<div class="form-group">
							<p><strong>First Name</strong></p>
							<input type="text" id="firstName" name="firstName" value="" class="form-control" placeholder="First Name" required />
						</div>
						<div class="form-group">
							<p><strong>Last Name</strong></p>
							<input type="text" name="lastName" value="" class="form-control" placeholder="Last Name" required id="lastName" />
						</div>
						<div class="form-group">
							<p><strong>Date of Birth</strong></p>
							<input 
								type="date" 
								id="dateOfBirth" 
								name="dateOfBirth" 
								class="form-control"
								required 
							/>
						</div>
						<div class="form-group">
							<p><strong>Email Address</strong></p>
							<input type="email" name="email" value="" class="form-control" placeholder="Email Address" required id="email" />
						</div>
						<div class="form-group">
							<p><strong>Re-Enter Email Address</strong></p>
							<input type="email" value="" class="form-control" placeholder="Re-Enter Email Address" required id="confirmEmail" name="confirmEmail" />
						</div>
						<div class="form-group">
							<p><strong>Street Address</strong></p>
							<input type="text" name="address" value="" class="form-control" placeholder="Street Address" required id="address" />
						</div>
						<div class="form-group">
							<p><strong>City</strong></p>
							<input type="text" name="city" value="" class="form-control" placeholder="City" required id="city" />
						</div>
						<div class="form-group">
							<p><strong>State</strong></p>
							<select class="form-control" id="state" name="state">
								<option selected disabled>Select your State</option>
								
								<option value="AL" >Alabama</option>
								
								<option value="AK" >Alaska</option>
								
								<option value="AZ" >Arizona</option>
								
								<option value="AR" >Arkansas</option>
								
								<option value="CA" >California</option>
								
								<option value="CO" >Colorado</option>
								
								<option value="CT" >Connecticut</option>
								
								<option value="DE" >Delaware</option>
								
								<option value="DC" >District of Columbia</option>
								
								<option value="FL" >Florida</option>
								
								<option value="GA" >Georgia</option>
								
								<option value="HI" >Hawaii</option>
								
								<option value="ID" >Idaho</option>
								
								<option value="IL" >Illinois</option>
								
								<option value="IN" >Indiana</option>
								
								<option value="IA" >Iowa</option>
								
								<option value="KS" >Kansas</option>
								
								<option value="KY" >Kentucky</option>
								
								<option value="LA" >Louisiana</option>
								
								<option value="ME" >Maine</option>
								
								<option value="MD" >Maryland</option>
								
								<option value="MA" >Massachusetts</option>
								
								<option value="MI" >Michigan</option>
								
								<option value="MN" >Minnesota</option>
								
								<option value="MS" >Mississippi</option>
								
								<option value="MO" >Missouri</option>
								
								<option value="MT" >Montana</option>
								
								<option value="NE" >Nebraska</option>
								
								<option value="NV" >Nevada</option>
								
								<option value="NH" >New Hampshire</option>
								
								<option value="NJ" >New Jersey</option>
								
								<option value="NM" >New Mexico</option>
								
								<option value="NY" >New York</option>
								
								<option value="NC" >North Carolina</option>
								
								<option value="ND" >North Dakota</option>
								
								<option value="OH" >Ohio</option>
								
								<option value="OK" >Oklahoma</option>
								
								<option value="OR" >Oregon</option>
								
								<option value="PA" >Pennsylvania</option>
								
								<option value="RI" >Rhode Island</option>
								
								<option value="SC" >South Carolina</option>
								
								<option value="SD" >South Dakota</option>
								
								<option value="TN" >Tennessee</option>
								
								<option value="TX" >Texas</option>
								
								<option value="UT" >Utah</option>
								
								<option value="VT" >Vermont</option>
								
								<option value="VA" >Virginia</option>
								
								<option value="WA" >Washington</option>
								
								<option value="WV" >West Virginia</option>
								
								<option value="WI" >Wisconsin</option>
								
								<option value="WY" >Wyoming</option>
								
							</select>
						</div>
						<div class="form-group">
							<p><strong>Zip Code</strong></p>
							<input type="text" name="zipCode" value="" class="form-control" placeholder="Zip Code" required id="zipCode" />
						</div>
						<div class="form-group">
							<p><strong>Requested Loan Amount</strong></p>
							<div id="loanamount-box" class="input-group">
								<span class="input-group-text fs-4">$</span>
								<input type="text" name="loanAmount" value="" class="form-control text-start" placeholder="Requested Loan Amount" required id="loanAmount" />
							</div>
						</div>
						<div class="form-group">
							<p><strong>Mobile Phone</strong></p>
							<input type="text" name="phone" value="" class="form-control phone-format" placeholder="Mobile Phone" id="phone" required />
						</div>
						<div class="form-group">
							<p><strong>Invite Code</strong></p>
							<input type="text" name="refCode" value="" class="form-control" placeholder="Invite Code" id="refCode" />
						</div>
						<div class="form-group" id="terms-box">
							<input name="terms" id="terms" type="checkbox" />
							<label for="terms" style="display:inline;cursor:pointer;">I agree to be contacted by Online Loan Pro at the email address and phone number I provided.</label>
						</div>
						<br/>
						<button id="submitButton" class="btn bg-primary-color text-white hover-effect w-100 p-4" type="submit">
							<span class="text-white fs-3 fw-bold">Continue</span>
						</button>
						<label class="server-error error mt-3"></label>
					</form>
					<input id="campaign" class="d-none" type="text" name="campaign" value="loanpro" disabled>
				</div>
				<div class="col-xs-12 col-md-8 d-flex flex-column align-items-center justify-content-start mt-5 mt-md-0">
					<img class="img-responsive py-5" width="250" src="/campaigns/loanpro/images/loanpro-logo.png" />
					</br>
					<h3>Have any questions?</h3>
					<h3>Call us <a href="tel:1-877-864-5191" style="color:#100F0D">(877) 864-5191</a></h3>
				</div>
			</div>
		</div>
	</section>
	<footer class="h-auto">
		<div class="container">
			<div class="d-flex flex-column gap-5 flex-lg-row-reverse align-items-center align-items-md-start justify-content-center">
				<div>
					<p class="fs-5">
						This correspondence is for a debt consolidation loan referred by Online Loan Pro</span>.
						All loan requests are funded by a third party. Online Loan Pro has no
						control over participating lender creditworthiness eligibility criteria. APR/Interest
						rates will vary depending on individual lender terms.
					</p>
					<p class="fs-5">
						Online Loan Pro does not endorse any participating lenders or brokers, and will
						not charge you for referring you to a participating lender. Online Loan Pro does
						not act as a loan broker. Online Loan Pro research and refers you to
						participating lenders, to whom you deal directly. For details, questions, or concerns
						regarding your requested loan, please contact your lender directly. Lenders may
						perform credit checks in order to evaluate your eligibility. By submitting a request
						to a participating lender, you are authorizing the lender to independently verify the
						information you submitted and your creditworthiness. This service and qualified
						participating lenders are not available in all states. This service does not constitute
						an offer or solicitation for loan products, which are prohibited by any state law. Void
						where prohibited.
					</p>
					<p class="fs-5">
						Lenders we refer to you may be unable to extend credit if the lender determines
						that you do not meet their lending criteria. Account approval is subject to
						verification and confirmation of your credit history and acceptance by the lender.
						We, or the lender, may request verification of income and employment, or obtain a
						consumer credit report to verify your creditworthiness and identity. Upon request,
						you will be informed of the name and address of the credit reporting agency
						furnishing any such report. Subsequent consumer credit reports may be requested
						and used in connection with an update, renewal, or extension of credit. Interest
						rates will vary and are based on lender terms. Your APR will be determined based
						on your credit at time of application.
					</p>
					<p class="fs-5">
						Loans are offered by Online Loan Pro affiliates. Kuber Financial, LLC dba
						Mobilend is an affiliate of Online Loan Pro. Loans are not offered in all states.
					</p>
					<p class="fs-5">
						Loans are offered in Alabama, California, Idaho, Missouri, and Utah.
					</p>
					<p class="fs-5">
						Arkansas: All loans made to Arkansas residents must qualify for an annual
						percentage rate (APR) of 17% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Georgia: All loans made to Georgia residents must qualify for an annual
						percentage rate (APR) of 7% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Illinois: All loans made to Illinois residents must qualify for an annual
						percentage rate (APR) of 9% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Iowa: All loans made to Iowa residents must qualify for an annual
						percentage rate (APR) of 5% or less to be considered for an approval, and loan
						amount cannot be greater than $25,000.
					</p>
					<p class="fs-5">
						Kansas: All loans made to Kansas residents must qualify for an annual
						percentage rate (APR) of 12% or less to be considered for an approval, and loan
						amount cannot be greater than $25,000.
					</p>
					<p class="fs-5">
						Kentucky: All loans made to Kentucky residents must qualify for an annual
						percentage rate (APR) of 8% or less to be considered for an approval, and loan
						amount cannot be greater than $15,000.
					</p>
					<p class="fs-5">
						Massachusetts: All loans made to Massachusetts residents must qualify for an
						annual percentage rate (APR) of 12% or less to be considered for an approval, and loan
						amount cannot be greater than $6,000.
					</p>
					<p class="fs-5">
						Michigan: All loans made to Michigan residents must qualify for an annual
						percentage rate (APR) of 7% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Minnesota: All loans made to Minnesota residents must qualify for an annual
						percentage rate (APR) of 8% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Nebraska: Residents of Nebraska must be 19 years old to apply for credit. All loans
						made to Nebraska residents must qualify for an annual percentage rate (APR) of
						16% or less and loan amount cannot be greater than $25,000 to be considered for
						approval.
					</p>
					<p class="fs-5">
						New Jersey: All loans made to New Jersey residents must qualify for an annual
						percentage rate (APR) of 8% or less and loan amount must be greater than $50,000
						to be considered for an approval.
					</p>
					<p class="fs-5">
						New Mexico: All loans made to New Mexico residents must qualify for an annual
						percentage rate (APR) of 15% or less and loan amount must be greater than $2501
						to be considered for an approval.
					</p>
					<p class="fs-5">
						New York: All loans made to New York residents must qualify for an annual
						percentage rate (APR) of 16% or less and loan amount cannot be greater than
						$25,000 to be considered for an approval.
					</p>
					<p class="fs-5">
						North Carolina: All loans made to North Carolina residents must qualify for an
						annual percentage rate (APR) of 8% or less and loan amount cannot be greater than
						$25,000 to be considered for an approval.
					</p>
					<p class="fs-5">
						South Carolina: All loans made to South Carolina residents must qualify for an annual
						percentage rate (APR) of 12% or less and loan amount cannot be greater than $90,000.
					</p>
					<p class="fs-5">
						South Dakota: All loans made to South Dakota residents must qualify for an annual
						percentage rate (APR) of 36% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Tennessee: All loans made to Tennessee residents must qualify for an annual
						percentage rate (APR) of 7.52% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Texas: All loans made to Texas residents must qualify for an annual percentage rate
						(APR) of 10% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Virginia: All loans made to Virginia residents must qualify for an annual percentage rate
						(APR) of 10% or less to be considered for an approval.
					</p>
					<p class="fs-5">Online Loan Pro is not a banking or lending institution in itself. The information you provide on this
						website and to our advisors is solely used to suggest suitable financial institutions from our network based
						on your individual needs and credit profile. We will refrain from sending you any promotional or marketing
						materials. Please be aware that submitting your documents does not guarantee approval; however, qualified
						lenders will review your paperwork during the approval process. There is no charge or additional fee for this
						service. Please note that we will share information about financial institutions within our business network
						so that you can directly engage with lenders, as we do not act on your behalf in any financial matters or
						transactions with lenders. By using this website and our services, you agree to our terms, conditions, and
						policies. Additionally, it’s important to understand that some moneylenders in our network may not offer their
						services in certain states. We strongly advise you to familiarize yourself with our privacy practices and
						other disclaimers before utilizing our services.
					</p>
					<p class="fs-5">Address : United States Email : <a class="text-reset" href="mailto:contact@onlineloanpro.com">contact@onlineloanpro.com</a></p>
					<p class="fs-5">© Copyright 2023  <a class="text-reset text-decoration-underline" target="_blank" href="https://onlineloanpro.com">onlineloanpro.com</a> - Online Loan Pro LLC. All Rights Reserved.</p>
				</div>
				<div class="d-none d-md-block line"></div>
				<div class="d-flex flex-column align-items-center">
					<img class="img-responsive pb-5" width="150" src="/campaigns/loanpro/images/loanpro-logo.png" />
					<a class="d-flex align-items-center text-reset text-decoration-none fs-4" href="tel:1-877-864-5191">
						<span class="material-symbols-outlined phone-icon">phone_iphone</span>
						<span class="fw-bold fs-4">(877) 864-5191</span>
					</a>
					<div class="vl"></div>
				</div>
			</div>
		</div>
	</footer>
</div>

	<script src="/common/js/inputmask.min.js" type="text/javascript"></script>
	<!-- Custom JS -->
	<script src="/common/js/form-submission.js" type="text/javascript"></script>
	<script src="/common/js/validate.js" type="text/javascript"></script>
	<script src="/common/js/loading.js" type="text/javascript"></script>
	
	
</body>

</html>
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 175, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 160, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 148, in scrape_single
    raise Exception(f"Error: {response.status_code} - {response.text}")
Exception: Error: 201 - <!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<!-- Favicon -->
	<link href="/campaigns/loanpro/images/loanpro-favicon.png" rel="shortcut icon">
	<!-- meta View port tag -->
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<!-- Title -->
	<title>Online Loan Pro</title>
	<!-- Bootstrap Css -->
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet"
		integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
	<!-- Common & Responsive CSS -->
	<link href="/common/css/index.css" rel="stylesheet" type="text/css">
	<link href="/campaigns/loanpro/css/index.css" rel="stylesheet" type="text/css">
	<link href="/campaigns/loanpro/css/responsive.css" rel="stylesheet" type="text/css">
	<!-- Google Icons -->
	<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0" />
	<!-- Bootstrap JS -->
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"
		integrity="sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm"
		crossorigin="anonymous"></script>
	<!-- Iconnode JS -->
	<script src="//scripts.iconnode.com/78875.js"></script>
	

</head>

<body>
	
<div class="spinner-container"  style="display:none;">
    <div class="loading-spinner">
         <div style="text-align:center;position:fixed;top:0;left:0;right:0;bottom:0;z-index:100;background:rgba(0,0,0,0.7);">
            <div style="height:100%;width:100%;background:url('/common/icons/loading.gif') no-repeat center;margin-top:-233px;"></div>
        </div>
    </div>
</div>
<div>
	<section class="survey">
		<div class="container">
			<div class="row">
				<div class="col-xs-12 col-sm-8 col-sm-offset-1 col-md-5 col-md-offset-1 col-lg-4 col-lg-offset-1">
					<form id="surveyForm" method="POST" action="/form/survey/">
						<div class="form-group">
							<p><strong>First Name</strong></p>
							<input type="text" id="firstName" name="firstName" value="" class="form-control" placeholder="First Name" required />
						</div>
						<div class="form-group">
							<p><strong>Last Name</strong></p>
							<input type="text" name="lastName" value="" class="form-control" placeholder="Last Name" required id="lastName" />
						</div>
						<div class="form-group">
							<p><strong>Date of Birth</strong></p>
							<input 
								type="date" 
								id="dateOfBirth" 
								name="dateOfBirth" 
								class="form-control"
								required 
							/>
						</div>
						<div class="form-group">
							<p><strong>Email Address</strong></p>
							<input type="email" name="email" value="" class="form-control" placeholder="Email Address" required id="email" />
						</div>
						<div class="form-group">
							<p><strong>Re-Enter Email Address</strong></p>
							<input type="email" value="" class="form-control" placeholder="Re-Enter Email Address" required id="confirmEmail" name="confirmEmail" />
						</div>
						<div class="form-group">
							<p><strong>Street Address</strong></p>
							<input type="text" name="address" value="" class="form-control" placeholder="Street Address" required id="address" />
						</div>
						<div class="form-group">
							<p><strong>City</strong></p>
							<input type="text" name="city" value="" class="form-control" placeholder="City" required id="city" />
						</div>
						<div class="form-group">
							<p><strong>State</strong></p>
							<select class="form-control" id="state" name="state">
								<option selected disabled>Select your State</option>
								
								<option value="AL" >Alabama</option>
								
								<option value="AK" >Alaska</option>
								
								<option value="AZ" >Arizona</option>
								
								<option value="AR" >Arkansas</option>
								
								<option value="CA" >California</option>
								
								<option value="CO" >Colorado</option>
								
								<option value="CT" >Connecticut</option>
								
								<option value="DE" >Delaware</option>
								
								<option value="DC" >District of Columbia</option>
								
								<option value="FL" >Florida</option>
								
								<option value="GA" >Georgia</option>
								
								<option value="HI" >Hawaii</option>
								
								<option value="ID" >Idaho</option>
								
								<option value="IL" >Illinois</option>
								
								<option value="IN" >Indiana</option>
								
								<option value="IA" >Iowa</option>
								
								<option value="KS" >Kansas</option>
								
								<option value="KY" >Kentucky</option>
								
								<option value="LA" >Louisiana</option>
								
								<option value="ME" >Maine</option>
								
								<option value="MD" >Maryland</option>
								
								<option value="MA" >Massachusetts</option>
								
								<option value="MI" >Michigan</option>
								
								<option value="MN" >Minnesota</option>
								
								<option value="MS" >Mississippi</option>
								
								<option value="MO" >Missouri</option>
								
								<option value="MT" >Montana</option>
								
								<option value="NE" >Nebraska</option>
								
								<option value="NV" >Nevada</option>
								
								<option value="NH" >New Hampshire</option>
								
								<option value="NJ" >New Jersey</option>
								
								<option value="NM" >New Mexico</option>
								
								<option value="NY" >New York</option>
								
								<option value="NC" >North Carolina</option>
								
								<option value="ND" >North Dakota</option>
								
								<option value="OH" >Ohio</option>
								
								<option value="OK" >Oklahoma</option>
								
								<option value="OR" >Oregon</option>
								
								<option value="PA" >Pennsylvania</option>
								
								<option value="RI" >Rhode Island</option>
								
								<option value="SC" >South Carolina</option>
								
								<option value="SD" >South Dakota</option>
								
								<option value="TN" >Tennessee</option>
								
								<option value="TX" >Texas</option>
								
								<option value="UT" >Utah</option>
								
								<option value="VT" >Vermont</option>
								
								<option value="VA" >Virginia</option>
								
								<option value="WA" >Washington</option>
								
								<option value="WV" >West Virginia</option>
								
								<option value="WI" >Wisconsin</option>
								
								<option value="WY" >Wyoming</option>
								
							</select>
						</div>
						<div class="form-group">
							<p><strong>Zip Code</strong></p>
							<input type="text" name="zipCode" value="" class="form-control" placeholder="Zip Code" required id="zipCode" />
						</div>
						<div class="form-group">
							<p><strong>Requested Loan Amount</strong></p>
							<div id="loanamount-box" class="input-group">
								<span class="input-group-text fs-4">$</span>
								<input type="text" name="loanAmount" value="" class="form-control text-start" placeholder="Requested Loan Amount" required id="loanAmount" />
							</div>
						</div>
						<div class="form-group">
							<p><strong>Mobile Phone</strong></p>
							<input type="text" name="phone" value="" class="form-control phone-format" placeholder="Mobile Phone" id="phone" required />
						</div>
						<div class="form-group">
							<p><strong>Invite Code</strong></p>
							<input type="text" name="refCode" value="" class="form-control" placeholder="Invite Code" id="refCode" />
						</div>
						<div class="form-group" id="terms-box">
							<input name="terms" id="terms" type="checkbox" />
							<label for="terms" style="display:inline;cursor:pointer;">I agree to be contacted by Online Loan Pro at the email address and phone number I provided.</label>
						</div>
						<br/>
						<button id="submitButton" class="btn bg-primary-color text-white hover-effect w-100 p-4" type="submit">
							<span class="text-white fs-3 fw-bold">Continue</span>
						</button>
						<label class="server-error error mt-3"></label>
					</form>
					<input id="campaign" class="d-none" type="text" name="campaign" value="loanpro" disabled>
				</div>
				<div class="col-xs-12 col-md-8 d-flex flex-column align-items-center justify-content-start mt-5 mt-md-0">
					<img class="img-responsive py-5" width="250" src="/campaigns/loanpro/images/loanpro-logo.png" />
					</br>
					<h3>Have any questions?</h3>
					<h3>Call us <a href="tel:1-877-864-5191" style="color:#100F0D">(877) 864-5191</a></h3>
				</div>
			</div>
		</div>
	</section>
	<footer class="h-auto">
		<div class="container">
			<div class="d-flex flex-column gap-5 flex-lg-row-reverse align-items-center align-items-md-start justify-content-center">
				<div>
					<p class="fs-5">
						This correspondence is for a debt consolidation loan referred by Online Loan Pro</span>.
						All loan requests are funded by a third party. Online Loan Pro has no
						control over participating lender creditworthiness eligibility criteria. APR/Interest
						rates will vary depending on individual lender terms.
					</p>
					<p class="fs-5">
						Online Loan Pro does not endorse any participating lenders or brokers, and will
						not charge you for referring you to a participating lender. Online Loan Pro does
						not act as a loan broker. Online Loan Pro research and refers you to
						participating lenders, to whom you deal directly. For details, questions, or concerns
						regarding your requested loan, please contact your lender directly. Lenders may
						perform credit checks in order to evaluate your eligibility. By submitting a request
						to a participating lender, you are authorizing the lender to independently verify the
						information you submitted and your creditworthiness. This service and qualified
						participating lenders are not available in all states. This service does not constitute
						an offer or solicitation for loan products, which are prohibited by any state law. Void
						where prohibited.
					</p>
					<p class="fs-5">
						Lenders we refer to you may be unable to extend credit if the lender determines
						that you do not meet their lending criteria. Account approval is subject to
						verification and confirmation of your credit history and acceptance by the lender.
						We, or the lender, may request verification of income and employment, or obtain a
						consumer credit report to verify your creditworthiness and identity. Upon request,
						you will be informed of the name and address of the credit reporting agency
						furnishing any such report. Subsequent consumer credit reports may be requested
						and used in connection with an update, renewal, or extension of credit. Interest
						rates will vary and are based on lender terms. Your APR will be determined based
						on your credit at time of application.
					</p>
					<p class="fs-5">
						Loans are offered by Online Loan Pro affiliates. Kuber Financial, LLC dba
						Mobilend is an affiliate of Online Loan Pro. Loans are not offered in all states.
					</p>
					<p class="fs-5">
						Loans are offered in Alabama, California, Idaho, Missouri, and Utah.
					</p>
					<p class="fs-5">
						Arkansas: All loans made to Arkansas residents must qualify for an annual
						percentage rate (APR) of 17% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Georgia: All loans made to Georgia residents must qualify for an annual
						percentage rate (APR) of 7% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Illinois: All loans made to Illinois residents must qualify for an annual
						percentage rate (APR) of 9% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Iowa: All loans made to Iowa residents must qualify for an annual
						percentage rate (APR) of 5% or less to be considered for an approval, and loan
						amount cannot be greater than $25,000.
					</p>
					<p class="fs-5">
						Kansas: All loans made to Kansas residents must qualify for an annual
						percentage rate (APR) of 12% or less to be considered for an approval, and loan
						amount cannot be greater than $25,000.
					</p>
					<p class="fs-5">
						Kentucky: All loans made to Kentucky residents must qualify for an annual
						percentage rate (APR) of 8% or less to be considered for an approval, and loan
						amount cannot be greater than $15,000.
					</p>
					<p class="fs-5">
						Massachusetts: All loans made to Massachusetts residents must qualify for an
						annual percentage rate (APR) of 12% or less to be considered for an approval, and loan
						amount cannot be greater than $6,000.
					</p>
					<p class="fs-5">
						Michigan: All loans made to Michigan residents must qualify for an annual
						percentage rate (APR) of 7% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Minnesota: All loans made to Minnesota residents must qualify for an annual
						percentage rate (APR) of 8% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Nebraska: Residents of Nebraska must be 19 years old to apply for credit. All loans
						made to Nebraska residents must qualify for an annual percentage rate (APR) of
						16% or less and loan amount cannot be greater than $25,000 to be considered for
						approval.
					</p>
					<p class="fs-5">
						New Jersey: All loans made to New Jersey residents must qualify for an annual
						percentage rate (APR) of 8% or less and loan amount must be greater than $50,000
						to be considered for an approval.
					</p>
					<p class="fs-5">
						New Mexico: All loans made to New Mexico residents must qualify for an annual
						percentage rate (APR) of 15% or less and loan amount must be greater than $2501
						to be considered for an approval.
					</p>
					<p class="fs-5">
						New York: All loans made to New York residents must qualify for an annual
						percentage rate (APR) of 16% or less and loan amount cannot be greater than
						$25,000 to be considered for an approval.
					</p>
					<p class="fs-5">
						North Carolina: All loans made to North Carolina residents must qualify for an
						annual percentage rate (APR) of 8% or less and loan amount cannot be greater than
						$25,000 to be considered for an approval.
					</p>
					<p class="fs-5">
						South Carolina: All loans made to South Carolina residents must qualify for an annual
						percentage rate (APR) of 12% or less and loan amount cannot be greater than $90,000.
					</p>
					<p class="fs-5">
						South Dakota: All loans made to South Dakota residents must qualify for an annual
						percentage rate (APR) of 36% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Tennessee: All loans made to Tennessee residents must qualify for an annual
						percentage rate (APR) of 7.52% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Texas: All loans made to Texas residents must qualify for an annual percentage rate
						(APR) of 10% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Virginia: All loans made to Virginia residents must qualify for an annual percentage rate
						(APR) of 10% or less to be considered for an approval.
					</p>
					<p class="fs-5">Online Loan Pro is not a banking or lending institution in itself. The information you provide on this
						website and to our advisors is solely used to suggest suitable financial institutions from our network based
						on your individual needs and credit profile. We will refrain from sending you any promotional or marketing
						materials. Please be aware that submitting your documents does not guarantee approval; however, qualified
						lenders will review your paperwork during the approval process. There is no charge or additional fee for this
						service. Please note that we will share information about financial institutions within our business network
						so that you can directly engage with lenders, as we do not act on your behalf in any financial matters or
						transactions with lenders. By using this website and our services, you agree to our terms, conditions, and
						policies. Additionally, it’s important to understand that some moneylenders in our network may not offer their
						services in certain states. We strongly advise you to familiarize yourself with our privacy practices and
						other disclaimers before utilizing our services.
					</p>
					<p class="fs-5">Address : United States Email : <a class="text-reset" href="mailto:contact@onlineloanpro.com">contact@onlineloanpro.com</a></p>
					<p class="fs-5">© Copyright 2023  <a class="text-reset text-decoration-underline" target="_blank" href="https://onlineloanpro.com">onlineloanpro.com</a> - Online Loan Pro LLC. All Rights Reserved.</p>
				</div>
				<div class="d-none d-md-block line"></div>
				<div class="d-flex flex-column align-items-center">
					<img class="img-responsive pb-5" width="150" src="/campaigns/loanpro/images/loanpro-logo.png" />
					<a class="d-flex align-items-center text-reset text-decoration-none fs-4" href="tel:1-877-864-5191">
						<span class="material-symbols-outlined phone-icon">phone_iphone</span>
						<span class="fw-bold fs-4">(877) 864-5191</span>
					</a>
					<div class="vl"></div>
				</div>
			</div>
		</div>
	</footer>
</div>

	<script src="/common/js/inputmask.min.js" type="text/javascript"></script>
	<!-- Custom JS -->
	<script src="/common/js/form-submission.js" type="text/javascript"></script>
	<script src="/common/js/validate.js" type="text/javascript"></script>
	<script src="/common/js/loading.js" type="text/javascript"></script>
	
	
</body>

</html>
2023-12-06 02:39:16,004 [ERROR]: An error occurred: 'Response' object has no attribute 'header'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 175, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 160, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 129, in scrape_single
    remaining_requests = int(response.header.get('X-RateLimit-Remaining',0))
                             ^^^^^^^^^^^^^^^
AttributeError: 'Response' object has no attribute 'header'. Did you mean: 'headers'?
2023-12-06 02:40:07,964 [ERROR]: An error occurred: Error: 429 - {"statusCode":429,"message":"ThrottlerException: Too Many Requests"}
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 175, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 160, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 145, in scrape_single
    raise Exception(f"Error: {response.status_code} - {response.text}")
Exception: Error: 429 - {"statusCode":429,"message":"ThrottlerException: Too Many Requests"}
2023-12-06 02:41:41,250 [ERROR]: An error occurred: Error: 429 - {"statusCode":429,"message":"ThrottlerException: Too Many Requests"}
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 176, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 161, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 146, in scrape_single
    raise Exception(f"Error: {response.status_code} - {response.text}")
Exception: Error: 429 - {"statusCode":429,"message":"ThrottlerException: Too Many Requests"}
2023-12-06 03:40:56,641 [ERROR]: An error occurred: 'Scraper2' object has no attribute 'last_time_check'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 188, in scrape_with_refcodes
    if now-self.last_time_check > 180:
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper2' object has no attribute 'last_time_check'
2023-12-06 03:48:53,449 [ERROR]: An error occurred: Expecting value: line 1 column 1 (char 0)
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 971, in json
    return complexjson.loads(self.text, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 193, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 178, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 121, in scrape_single
    print(f'Origin: {response.json()['origin']}')
                     ^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 975, in json
    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)
requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2023-12-06 03:53:05,042 [ERROR]: An error occurred: Expecting value: line 1 column 1 (char 0)
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 971, in json
    return complexjson.loads(self.text, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 193, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 178, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 121, in scrape_single
    print(f'Origin: {response.json()['origin']}')
                     ^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 975, in json
    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)
requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2023-12-06 03:53:49,522 [ERROR]: An error occurred: Expecting value: line 1 column 1 (char 0)
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 971, in json
    return complexjson.loads(self.text, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 193, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 178, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 121, in scrape_single
    print(f'Origin: {response.json()['headers']['User-Agent']}')
                     ^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 975, in json
    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)
requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2023-12-06 23:07:28,222 [ERROR]: An error occurred: 'Scraper42' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper42' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-12-07 02:00:31,794 [ERROR]: An error occurred: object of type 'NoneType' has no len()
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 92, in scrape_with_names
    print(f"There {len(names)} names to rotate!")
                   ^^^^^^^^^^
TypeError: object of type 'NoneType' has no len()
2023-12-07 03:24:43,194 [ERROR]: An error occurred: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 28, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
2023-12-07 03:34:25,319 [ERROR]: An error occurred: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 28, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
2023-12-07 03:36:59,907 [ERROR]: An error occurred: DatabaseHandler.__init__() missing 1 required positional argument: 'table_names'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 28, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() missing 1 required positional argument: 'table_names'
2023-12-07 03:41:14,742 [ERROR]: An error occurred: DatabaseHandler.__init__() missing 1 required positional argument: 'table_names'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 28, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() missing 1 required positional argument: 'table_names'
2023-12-07 03:43:01,976 [ERROR]: An error occurred: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 28, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
2023-12-07 03:43:05,635 [ERROR]: An error occurred: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 28, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
2023-12-07 03:45:38,204 [ERROR]: An error occurred: HTTPConnectionPool(host="a'ishaaaberg1.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f32080934d0>: Failed to establish a new connection: [Errno -2] Name or service not known'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f3208090e90>: Failed to establish a new connection: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host="a'ishaaaberg1.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3208090e90>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 41, in scrape_single
    response = requests.get(f"http://{url}", headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host="a'ishaaaberg1.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3208090e90>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f32080934d0>: Failed to establish a new connection: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host="a'ishaaaberg1.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f32080934d0>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 118, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 98, in scrape_single_with_increment
    result = self.scrape_single(base_url)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 45, in scrape_single
    response = requests.get(f"http://{url}", headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host="a'ishaaaberg1.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f32080934d0>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-12-07 04:01:11,289 [ERROR]: An error occurred: HTTPConnectionPool(host="a'ishaaaberg2.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb04fa36a80>: Failed to establish a new connection: [Errno -2] Name or service not known'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fb04fa34b60>: Failed to establish a new connection: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host="a'ishaaaberg2.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb04fa34b60>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 42, in scrape_single
    response = requests.get(f"http://{url}", headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host="a'ishaaaberg2.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb04fa34b60>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fb04fa36a80>: Failed to establish a new connection: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host="a'ishaaaberg2.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb04fa36a80>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 120, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 100, in scrape_single_with_increment
    result = self.scrape_single(base_url)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 47, in scrape_single
    response = requests.get(f"http://{url}", headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host="a'ishaaaberg2.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb04fa36a80>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-12-07 04:07:53,166 [ERROR]: An error occurred: cannot access local variable 'response' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 125, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 105, in scrape_single_with_increment
    result = self.scrape_single(base_url)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 55, in scrape_single
    print(f'response headers: {response.headers}')
                               ^^^^^^^^
UnboundLocalError: cannot access local variable 'response' where it is not associated with a value
2023-12-07 04:15:22,672 [ERROR]: An error occurred: 1054 (42S22): Unknown column 'middle_name' in 'field list'
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Unknown column 'middle_name' in 'field list'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 46, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_bhgelite.py", line 39, in store_data
    self.cursor.execute(f'''
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1054 (42S22): Unknown column 'middle_name' in 'field list'
2023-12-07 04:23:50,094 [ERROR]: An error occurred: 'NoneType' object has no attribute 'split'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 129, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 109, in scrape_single_with_increment
    result = self.scrape_single(base_url.replace("'",""))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 74, in scrape_single
    state = get_us_state.get_state(str(zip_code_el.split("-")[0]))
                                       ^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'split'
2023-12-07 04:24:54,413 [ERROR]: An error occurred: 'NoneType' object has no attribute 'split'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 110, in scrape_single_with_increment
    result = self.scrape_single(base_url.replace("'",""))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 75, in scrape_single
    state = get_us_state.get_state(str(zip_code_el.split("-")[0]))
                                       ^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'split'
2023-12-07 04:25:18,459 [ERROR]: An error occurred: 'NoneType' object has no attribute 'split'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 110, in scrape_single_with_increment
    result = self.scrape_single(base_url.replace("'",""))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 75, in scrape_single
    state = get_us_state.get_state(str(zip_code_el.split("-")[0]))
                                       ^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'split'
2023-12-08 02:10:53,092 [ERROR]: An error occurred: cannot access free variable 'start_index_first' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 102, in scrape_with_names
    names = name_generator_large_file.generate_names()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/name_generator_large_file.py", line 61, in generate_names
    for batch in generate_batches(first_names,last_names):
  File "/home/terence/projects/corey-scraping/src/scraping/name_generator_large_file.py", line 80, in generate_batches
    for chunk in iter(lambda :list(islice(product(first_names[start_index_first:],last_names[start_index_last:]),batch_size)),[]):
  File "/home/terence/projects/corey-scraping/src/scraping/name_generator_large_file.py", line 80, in <lambda>
    for chunk in iter(lambda :list(islice(product(first_names[start_index_first:],last_names[start_index_last:]),batch_size)),[]):
                                                              ^^^^^^^^^^^^^^^^^
NameError: cannot access free variable 'start_index_first' where it is not associated with a value in enclosing scope
2023-12-08 02:11:51,954 [ERROR]: An error occurred: cannot access free variable 'start_index_first' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 102, in scrape_with_names
    names = name_generator_large_file.generate_names()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/name_generator_large_file.py", line 61, in generate_names
    for batch in generate_batches(first_names,last_names):
  File "/home/terence/projects/corey-scraping/src/scraping/name_generator_large_file.py", line 80, in generate_batches
    for chunk in iter(lambda :list(islice(product(first_names[start_index_first:],last_names[start_index_last:]),batch_size)),[]):
  File "/home/terence/projects/corey-scraping/src/scraping/name_generator_large_file.py", line 80, in <lambda>
    for chunk in iter(lambda :list(islice(product(first_names[start_index_first:],last_names[start_index_last:]),batch_size)),[]):
                                                              ^^^^^^^^^^^^^^^^^
NameError: cannot access free variable 'start_index_first' where it is not associated with a value in enclosing scope
2023-12-08 12:46:57,781 [ERROR]: An error occurred: DatabaseHandler.__init__() got an unexpected keyword argument 'table_names'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 28, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() got an unexpected keyword argument 'table_names'
2023-12-08 12:48:37,317 [ERROR]: An error occurred: DatabaseHandler.__init__() got an unexpected keyword argument 'table_names'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 29, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() got an unexpected keyword argument 'table_names'
2023-12-08 12:48:46,165 [ERROR]: An error occurred: DatabaseHandler.__init__() got an unexpected keyword argument 'table_names'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 29, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() got an unexpected keyword argument 'table_names'
2023-12-08 13:34:48,721 [INFO]: package: mysql.connector.plugins
2023-12-08 13:34:48,721 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:34:48,723 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:34:48,893 [ERROR]: An error occurred: object of type 'generator' has no len()
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 104, in scrape_with_names
    print(f"There {len(names)} names to rotate!")
                   ^^^^^^^^^^
TypeError: object of type 'generator' has no len()
2023-12-08 13:35:12,021 [INFO]: package: mysql.connector.plugins
2023-12-08 13:35:12,021 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:35:12,021 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:35:18,747 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 13:36:01,691 [INFO]: package: mysql.connector.plugins
2023-12-08 13:36:01,691 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:36:01,692 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:36:07,935 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 131, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 109, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 13:38:16,805 [INFO]: package: mysql.connector.plugins
2023-12-08 13:38:16,805 [INFO]: plugin_name: caching_sha2_password
2023-12-08 13:38:16,806 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLCachingSHA2PasswordAuthPlugin
2023-12-08 13:38:16,806 [INFO]: package: mysql.connector.plugins
2023-12-08 13:38:16,806 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:38:16,806 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:38:16,909 [ERROR]: An error occurred: 'Scraper41' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper41' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-12-08 13:38:31,753 [INFO]: package: mysql.connector.plugins
2023-12-08 13:38:31,753 [INFO]: plugin_name: caching_sha2_password
2023-12-08 13:38:31,753 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLCachingSHA2PasswordAuthPlugin
2023-12-08 13:38:31,754 [INFO]: package: mysql.connector.plugins
2023-12-08 13:38:31,754 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:38:31,754 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:38:31,771 [INFO]: Scraping and storing data completed successfully.
2023-12-08 13:40:06,843 [INFO]: package: mysql.connector.plugins
2023-12-08 13:40:06,844 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:40:06,844 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:40:10,389 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 13:43:07,962 [INFO]: package: mysql.connector.plugins
2023-12-08 13:43:07,962 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:43:07,962 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:43:12,802 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 13:51:08,795 [INFO]: package: mysql.connector.plugins
2023-12-08 13:51:08,795 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:51:08,795 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:51:13,526 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 13:52:09,909 [INFO]: package: mysql.connector.plugins
2023-12-08 13:52:09,909 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:52:09,910 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:52:14,678 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 13:53:07,663 [INFO]: package: mysql.connector.plugins
2023-12-08 13:53:07,663 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:53:07,663 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:53:11,404 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 13:57:03,692 [INFO]: package: mysql.connector.plugins
2023-12-08 13:57:03,693 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:57:03,693 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:57:07,824 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:03:36,023 [INFO]: package: mysql.connector.plugins
2023-12-08 14:03:36,023 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:03:36,024 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:04:33,864 [INFO]: package: mysql.connector.plugins
2023-12-08 14:04:33,865 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:04:33,865 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:04:38,983 [INFO]: package: mysql.connector.plugins
2023-12-08 14:04:38,983 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:04:38,983 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:04:42,304 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:05:38,575 [INFO]: package: mysql.connector.plugins
2023-12-08 14:05:38,575 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:05:38,575 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:05:42,901 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:05:59,101 [INFO]: package: mysql.connector.plugins
2023-12-08 14:05:59,101 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:05:59,102 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:06:02,503 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:33:18,296 [INFO]: package: mysql.connector.plugins
2023-12-08 14:33:18,296 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:33:18,296 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:33:21,518 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:33:41,302 [INFO]: package: mysql.connector.plugins
2023-12-08 14:33:41,302 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:33:41,303 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:33:44,727 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:37:47,014 [INFO]: package: mysql.connector.plugins
2023-12-08 14:37:47,014 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:37:47,015 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:37:50,801 [ERROR]: An error occurred: 'generator' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'split'
2023-12-08 14:38:08,831 [INFO]: package: mysql.connector.plugins
2023-12-08 14:38:08,831 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:38:08,832 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:38:12,560 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:39:09,103 [INFO]: package: mysql.connector.plugins
2023-12-08 14:39:09,103 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:39:09,103 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:39:12,567 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:39:52,039 [INFO]: package: mysql.connector.plugins
2023-12-08 14:39:52,039 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:39:52,039 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:39:56,078 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 131, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 109, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:40:37,160 [INFO]: package: mysql.connector.plugins
2023-12-08 14:40:37,160 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:40:37,160 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:40:37,167 [ERROR]: An error occurred: object of type 'generator' has no len()
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 103, in scrape_with_names
    print(len(names))
          ^^^^^^^^^^
TypeError: object of type 'generator' has no len()
2023-12-08 14:41:28,808 [INFO]: package: mysql.connector.plugins
2023-12-08 14:41:28,808 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:41:28,808 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:41:32,854 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 131, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 109, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:42:00,872 [INFO]: package: mysql.connector.plugins
2023-12-08 14:42:00,872 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:42:00,872 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:42:04,114 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 131, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 109, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:42:53,457 [INFO]: package: mysql.connector.plugins
2023-12-08 14:42:53,457 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:42:53,458 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:42:57,353 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 132, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 110, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:44:29,328 [INFO]: package: mysql.connector.plugins
2023-12-08 14:44:29,328 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:44:29,328 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:45:48,459 [INFO]: package: mysql.connector.plugins
2023-12-08 14:45:48,459 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:45:48,460 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:45:56,135 [INFO]: Scraping and storing data completed successfully.
2023-12-08 15:23:20,707 [INFO]: package: mysql.connector.plugins
2023-12-08 15:23:20,707 [INFO]: plugin_name: mysql_native_password
2023-12-08 15:23:20,708 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 15:23:26,372 [ERROR]: An error occurred: generator raised StopIteration
Traceback (most recent call last):
  File "/root/projects/corey/src/scraping/scraper42.py", line 124, in scrape_with_names
    name = next(names_generator)
           ^^^^^^^^^^^^^^^^^^^^^
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
RuntimeError: generator raised StopIteration
2023-12-08 15:24:35,790 [INFO]: package: mysql.connector.plugins
2023-12-08 15:24:35,791 [INFO]: plugin_name: mysql_native_password
2023-12-08 15:24:35,791 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 15:24:44,178 [ERROR]: An error occurred: generator raised StopIteration
Traceback (most recent call last):
  File "/root/projects/corey/src/scraping/scraper42.py", line 124, in scrape_with_names
    for name in next(names_generator):
                ^^^^^^^^^^^^^^^^^^^^^
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
RuntimeError: generator raised StopIteration
2023-12-08 15:26:27,322 [INFO]: package: mysql.connector.plugins
2023-12-08 15:26:27,322 [INFO]: plugin_name: mysql_native_password
2023-12-08 15:26:27,323 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 15:26:34,986 [ERROR]: An error occurred: generator raised StopIteration
Traceback (most recent call last):
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    for name in next(names_generator):
                ^^^^^^^^^^^^^^^^^^^^^
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
RuntimeError: generator raised StopIteration
2023-12-08 15:28:09,609 [INFO]: package: mysql.connector.plugins
2023-12-08 15:28:09,609 [INFO]: plugin_name: mysql_native_password
2023-12-08 15:28:09,610 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 15:29:33,434 [INFO]: package: mysql.connector.plugins
2023-12-08 15:29:33,434 [INFO]: plugin_name: mysql_native_password
2023-12-08 15:29:33,435 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 15:29:41,467 [ERROR]: An error occurred: name 'names' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 131, in scrape_with_names
    for name in names:
                ^^^^^
NameError: name 'names' is not defined. Did you mean: 'name'?
2023-12-08 15:30:55,445 [INFO]: package: mysql.connector.plugins
2023-12-08 15:30:55,445 [INFO]: plugin_name: mysql_native_password
2023-12-08 15:30:55,446 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 19:53:40,292 [ERROR]: An error occurred: object of type 'generator' has no len()
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper41.py", line 86, in scrape_with_names
    print(f"There {len(names)} names to rotate!")
                   ^^^^^^^^^^
TypeError: object of type 'generator' has no len()
2023-12-08 19:53:51,410 [INFO]: Scraping and storing data completed successfully.
2023-12-08 19:54:10,619 [INFO]: Scraping and storing data completed successfully.
2023-12-08 19:55:14,069 [INFO]: Scraping and storing data completed successfully.
2023-12-08 19:58:57,560 [INFO]: Scraping and storing data completed successfully.
2023-12-10 13:15:30,797 [ERROR]: An error occurred: HTTPSConnectionPool(host="a'ishaaaberg.moneyladdernow.com", port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fa5fbda1c40>: Failed to establish a new connection: [Errno -2] Name or service not known'))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fa5fbda1c40>: Failed to establish a new connection: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host="a'ishaaaberg.moneyladdernow.com", port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fa5fbda1c40>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper4.py", line 109, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper4.py", line 87, in scrape_single_with_increment
    result = self.scrape_single(base_url)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper4.py", line 41, in scrape_single
    response = requests.get(f"https://{url}",headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host="a'ishaaaberg.moneyladdernow.com", port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fa5fbda1c40>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-12-10 15:16:29,913 [ERROR]: An error occurred: HTTPSConnectionPool(host="a'ishaaaberg.moneyladdernow.com", port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f31f8b73620>: Failed to establish a new connection: [Errno -2] Name or service not known'))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f31f8b73620>: Failed to establish a new connection: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host="a'ishaaaberg.moneyladdernow.com", port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f31f8b73620>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper4.py", line 109, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper4.py", line 87, in scrape_single_with_increment
    result = self.scrape_single(base_url)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper4.py", line 41, in scrape_single
    response = requests.get(f"https://{url}",headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host="a'ishaaaberg.moneyladdernow.com", port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f31f8b73620>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-12-10 16:26:09,528 [ERROR]: An error occurred: name 'old_names' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper4.py", line 101, in scrape_with_names
    for name in next(names_generator):
                ^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/name_generator_large_file.py", line 89, in generate_names
    batch = [" ".join(name) for name in list(iter(product([first_names[idx]],last_names[start_index_last:]))) if " ".join(name) not in old_names]
                                                                                                                                       ^^^^^^^^^
NameError: name 'old_names' is not defined
2023-12-10 16:27:41,458 [ERROR]: An error occurred: name 'old_names' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper4.py", line 101, in scrape_with_names
    for name in next(names_generator):
                ^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/name_generator_large_file.py", line 89, in generate_names
    batch = [" ".join(name) for name in list(iter(product([first_names[idx]],last_names[start_index_last:]))) if " ".join(name) not in old_names]
                                                                                                                                       ^^^^^^^^^
NameError: name 'old_names' is not defined
<<<<<<< HEAD
2023-12-11 01:41:24,329 [ERROR]: An error occurred: 'Scraper3' object has no attribute 'scrape_with_names'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper3' object has no attribute 'scrape_with_names'. Did you mean: 'scrape_with_refcodes'?
2023-12-11 01:42:12,891 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f17a7116000>: Failed to establish a new connection: [Errno -2] Name or service not known'))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f17a7116000>: Failed to establish a new connection: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f17a7116000>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 147, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 138, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 59, in scrape_single
    response = self.session.get(f'https://{url}',data=data, headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f17a7116000>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-12-11 01:45:45,417 [ERROR]: An error occurred: 'Scraper3' object has no attribute 'url'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 21, in main
    scraper = Scraper()
              ^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 23, in __init__
    print(self.url)
          ^^^^^^^^
AttributeError: 'Scraper3' object has no attribute 'url'
2023-12-11 01:46:22,509 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 146, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 137, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 107, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 02:05:27,268 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 146, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 137, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 107, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 02:05:35,550 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 146, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 137, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 107, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 02:44:16,148 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 194, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 185, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 106, in scrape_single
    response = self.session.post(f'https://c0hcb177.caspio.com',data=data,cookies=cookies,headers=headers2)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 473, in prepare_request
    cookies = cookiejar_from_dict(cookies)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/cookies.py", line 537, in cookiejar_from_dict
    cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))
                                             ~~~~~~~~~~~^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-11 02:47:42,593 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 195, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 186, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 156, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 02:49:09,784 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 196, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 187, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 157, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 02:49:56,292 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 196, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 187, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 157, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 02:50:28,515 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 197, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 188, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 158, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 02:50:48,726 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 197, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 188, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 158, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 02:57:41,007 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 197, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 188, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 158, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:02:02,506 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 198, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 159, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:07:49,574 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 198, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 159, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:08:47,215 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 198, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 159, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:10:46,598 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 198, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 159, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:17:06,796 [ERROR]: An error occurred: name 'cookies' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 198, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 109, in scrape_single
    response = self.session.post(f'https://c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d',data=data,cookies=cookies,headers=headers2,allow_redirects=True)
                                                                                                                  ^^^^^^^
NameError: name 'cookies' is not defined
2023-12-11 03:23:45,810 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 201, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 192, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 162, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:24:04,060 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 201, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 192, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 162, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:24:45,012 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 201, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 192, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 162, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:24:58,586 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 201, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 192, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 162, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:34:07,235 [ERROR]: An error occurred: object of type 'Response' has no len()
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 207, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 198, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 114, in scrape_single
    soup = BeautifulSoup(response,'html.parser')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/bs4/__init__.py", line 315, in __init__
    elif len(markup) <= 256 and (
         ^^^^^^^^^^^
TypeError: object of type 'Response' has no len()
2023-12-11 03:34:36,649 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 207, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 198, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 168, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:35:07,954 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 207, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 198, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 168, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:36:00,370 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 208, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 199, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 169, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:37:09,930 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 209, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 200, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 170, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:37:45,298 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 210, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 201, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 171, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:37:52,736 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 210, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 201, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 171, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:41:19,537 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 201, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 192, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 162, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:43:49,983 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 211, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 202, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 172, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 03:47:33,959 [ERROR]: An error occurred: ReferenceError: "src" is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 214, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 205, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 127, in scrape_single
    src_url = context.eval('src')
              ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 27, in eval
    return self._eval(source)
           ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 78, in _eval
    return self.exec_(code)
           ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 18, in exec_
    return self._exec_(source)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 88, in _exec_
    return self._extract_result(output)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 167, in _extract_result
    raise ProgramError(value)
execjs._exceptions.ProgramError: ReferenceError: "src" is not defined
2023-12-11 03:48:44,826 [ERROR]: An error occurred: unsupported operand type(s) for +: 'Tag' and 'str'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 215, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 206, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 127, in scrape_single
    src_url = context.eval('src')
              ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 27, in eval
    return self._eval(source)
           ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 78, in _eval
    return self.exec_(code)
           ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 18, in exec_
    return self._exec_(source)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 82, in _exec_
    source = self._source + '\n' + source
             ~~~~~~~~~~~~~^~~~~~
TypeError: unsupported operand type(s) for +: 'Tag' and 'str'
2023-12-11 03:49:23,432 [ERROR]: An error occurred: unsupported operand type(s) for +: 'Tag' and 'str'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 215, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 206, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 127, in scrape_single
    src_url = context.eval('src')
              ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 27, in eval
    return self._eval(source)
           ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 78, in _eval
    return self.exec_(code)
           ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 18, in exec_
    return self._exec_(source)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 82, in _exec_
    source = self._source + '\n' + source
             ~~~~~~~~~~~~~^~~~~~
TypeError: unsupported operand type(s) for +: 'Tag' and 'str'
2023-12-11 03:56:13,498 [ERROR]: An error occurred: AbstractRuntimeContext.eval() missing 1 required positional argument: 'source'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 215, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 206, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 127, in scrape_single
    result = context.eval()
             ^^^^^^^^^^^^^^
TypeError: AbstractRuntimeContext.eval() missing 1 required positional argument: 'source'
2023-12-11 03:57:18,020 [ERROR]: An error occurred: ReferenceError: "document" is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 215, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 206, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 127, in scrape_single
    result = context.eval("new requestDataPage('c0hcb177.caspio.com', 'e9ac8000d5813b5789dc4353ad8d', false, '').start();")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 27, in eval
    return self._eval(source)
           ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 78, in _eval
    return self.exec_(code)
           ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 18, in exec_
    return self._exec_(source)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 88, in _exec_
    return self._extract_result(output)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 167, in _extract_result
    raise ProgramError(value)
execjs._exceptions.ProgramError: ReferenceError: "document" is not defined
2023-12-11 03:58:11,454 [ERROR]: An error occurred: ReferenceError: "document" is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 215, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 206, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 127, in scrape_single
    result = context.eval(response.text)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 27, in eval
    return self._eval(source)
           ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 78, in _eval
    return self.exec_(code)
           ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 18, in exec_
    return self._exec_(source)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 88, in _exec_
    return self._extract_result(output)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 167, in _extract_result
    raise ProgramError(value)
execjs._exceptions.ProgramError: ReferenceError: "document" is not defined
2023-12-11 04:06:24,548 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 215, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 206, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 176, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 04:07:00,999 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 215, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 206, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 176, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-11 04:08:04,703 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 215, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 206, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 176, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 02:07:37,154 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 216, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 207, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 177, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 02:09:56,929 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 223, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 214, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 184, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 02:10:28,239 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 224, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 215, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 185, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 02:11:23,384 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 225, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 216, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 186, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 02:12:53,089 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 225, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 216, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 186, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 02:15:27,023 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 225, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 216, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 186, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 02:15:41,527 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 225, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 216, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 186, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 02:43:47,318 [ERROR]: An error occurred: cannot access local variable 'cookies' where it is not associated with a value
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 225, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 216, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 107, in scrape_single
    response = self.session.get(f'https://c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d',data=data,cookies=cookies, headers=headers1)
                                                                                                                 ^^^^^^^
UnboundLocalError: cannot access local variable 'cookies' where it is not associated with a value
2023-12-12 02:44:00,097 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 225, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 216, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 186, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 02:44:20,491 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 226, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 217, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 187, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 02:52:31,254 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 226, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 217, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 187, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 03:02:43,247 [ERROR]: An error occurred: name 'cookies' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 226, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 217, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 114, in scrape_single
    response = self.session.post(f'https://c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d', data=data, params=params, cookies=cookies,headers=headers1,allow_redirects=True)
                                                                                                                                   ^^^^^^^
NameError: name 'cookies' is not defined
2023-12-12 03:02:57,993 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 226, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 217, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 187, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:13:42,413 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 226, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 217, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 187, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:14:23,662 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 226, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 217, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 187, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:42:12,231 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 228, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 219, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:42:35,162 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 228, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 219, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:43:27,471 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 230, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 221, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 191, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:43:55,171 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 228, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 219, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:44:02,263 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 228, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 219, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:44:12,941 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 228, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 219, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:48:04,523 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 228, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 219, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:48:09,130 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 228, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 219, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:48:33,729 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 228, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 219, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:50:22,935 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 228, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 219, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:50:30,670 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 228, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 219, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:50:54,489 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 228, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 219, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 189, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:53:00,130 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 231, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 222, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 192, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:53:11,336 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 231, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 222, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 192, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:53:25,511 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 231, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 222, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 192, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:56:05,603 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 232, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 223, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 193, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 04:57:02,525 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 232, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 223, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 193, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 05:11:14,608 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 233, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 224, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 194, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 05:11:30,594 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 233, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 224, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 194, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 13:27:03,919 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 235, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 226, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 196, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 13:28:09,581 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 236, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 227, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 197, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 13:28:36,666 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 237, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 228, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 198, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 13:28:58,764 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 237, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 228, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 198, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 13:29:34,087 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 238, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 229, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 199, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 14:51:25,816 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 238, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 229, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 126, in scrape_single
    script_src = soup.find('script').get('src')
                 ^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2023-12-12 14:52:18,261 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 238, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 229, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 199, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 14:52:37,286 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 239, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 230, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 200, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 14:53:25,170 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 240, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 231, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 201, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 14:53:45,889 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 240, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 231, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 201, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 16:46:19,596 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 241, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 232, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 202, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 16:47:13,130 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 241, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 232, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 202, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 16:59:21,291 [ERROR]: An error occurred: 'Response' object has no attribute 'coockies_items'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 242, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 233, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 127, in scrape_single
    print(response.coockies_items())
          ^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Response' object has no attribute 'coockies_items'
2023-12-12 16:59:40,844 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 242, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 233, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 203, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 17:13:32,065 [ERROR]: An error occurred: ReferenceError: "document" is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 246, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 237, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 131, in scrape_single
    output = context.call(None)
             ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 37, in call
    return self._call(name, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 92, in _call
    return self._eval("{identifier}.apply(this, {args})".format(identifier=identifier, args=args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 78, in _eval
    return self.exec_(code)
           ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 18, in exec_
    return self._exec_(source)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 88, in _exec_
    return self._extract_result(output)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 167, in _extract_result
    raise ProgramError(value)
execjs._exceptions.ProgramError: ReferenceError: "document" is not defined
2023-12-12 18:20:00,000 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 242, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 233, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 203, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 18:21:28,611 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 242, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 233, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 203, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 18:21:35,864 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 242, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 233, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 203, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 18:33:34,482 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 249, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 240, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 210, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 18:34:10,999 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 251, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/root/projects/corey/src/scraping/scraper3.py", line 242, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper3.py", line 212, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
=======
2023-12-12 23:33:43,275 [ERROR]: An error occurred: 'Scraper3' object has no attribute 'scrape_with_names'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper3' object has no attribute 'scrape_with_names'. Did you mean: 'scrape_with_refcodes'?
2023-12-12 23:52:13,640 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 206, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 197, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 167, in scrape_single
    zip_code_el = soup.find(attrs={"name":"EditRecordZip"})['value'] if 'value' in soup.find(attrs={"name":"EditRecordZip"}).attrs else None
                                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 23:52:53,834 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 206, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 197, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 167, in scrape_single
    zip_code_el = soup.find(attrs={"name":"EditRecordZip"})['value'] if 'value' in soup.find(attrs={"name":"EditRecordZip"}).attrs else None
                                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 23:53:26,932 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 207, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 198, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 168, in scrape_single
    zip_code_el = soup.find(attrs={"name":"EditRecordZip"})['value'] if 'value' in soup.find(attrs={"name":"EditRecordZip"}).attrs else None
                                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-12 23:54:46,195 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 207, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 198, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 168, in scrape_single
    zip_code_el = soup.find(attrs={"name":"EditRecordZip"})['value'] if  soup.find(attrs={"name":"EditRecordZip"}).attrs and 'value' in soup.find(attrs={"name":"EditRecordZip"}) else None
                                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-13 00:00:51,545 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 211, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 202, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 146, in scrape_single
    print(input_element.get('value'))
          ^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2023-12-13 00:11:50,366 [ERROR]: An error occurred: 'NoneType' object has no attribute 'text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 212, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 203, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 146, in scrape_single
    print(input_element.text)
          ^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'text'
2023-12-13 00:15:48,554 [ERROR]: An error occurred: expected string or bytes-like object, got 'Tag'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 212, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 203, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 145, in scrape_single
    match = re.search(r'''<input id='\"EditRecordFirstName\"' name='\"EditRecordFirstName\"' type='\"hidden\"' value='\".?/>''',soup)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/re/__init__.py", line 177, in search
    return _compile(pattern, flags).search(string)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: expected string or bytes-like object, got 'Tag'
2023-12-13 00:17:50,432 [ERROR]: An error occurred: 'NoneType' object is not callable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 212, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 203, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 145, in scrape_single
    print(soup.replace("\\",""))
          ^^^^^^^^^^^^^^^^^^^^^
TypeError: 'NoneType' object is not callable
2023-12-13 00:27:45,075 [ERROR]: An error occurred: 'value'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 216, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 207, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 145, in scrape_single
    first = soup.find('input',attrs={'name':r'\"EditRecordFirstName\"'}['value'])
                                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
KeyError: 'value'
2023-12-13 00:35:45,865 [ERROR]: An error occurred: name 'cookies' is not defined
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 203, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 194, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 123, in scrape_single
    response = requests.post(f"https://c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode={data['refCode']}",params=params,data=data,cookies=cookies,allow_redirects=True)
                                                                                                                                                      ^^^^^^^
NameError: name 'cookies' is not defined
2023-12-13 00:43:39,366 [ERROR]: An error occurred: name 'I' is not defined
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 144, in scrape_with_refcodes
    batch_result = [result for result in executor.map(scrape_single_thread,refcodes[I:i+batch_size]) if result is not None ]
                                                                                    ^
NameError: name 'I' is not defined
2023-12-13 00:48:19,110 [ERROR]: An error occurred: 'middle_name'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 46, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_bhgelite.py", line 42, in store_data
    ''', (data['first_name'],data['middle_name'], data['last_name'], data['email'], data['phone'], data['address'], data['city'], data['state'], data['zip_code']))
                             ~~~~^^^^^^^^^^^^^^^
KeyError: 'middle_name'
2023-12-13 03:38:03,579 [ERROR]: An error occurred: object of type 'int' has no len()
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_bizfileonline.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper48.py", line 104, in scrape_with_refcodes
    for i in range(1,len(codes),batch_size):
                     ^^^^^^^^^^
TypeError: object of type 'int' has no len()
2023-12-13 03:38:38,657 [ERROR]: An error occurred: 'int' object is not subscriptable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_bizfileonline.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper48.py", line 105, in scrape_with_refcodes
    batch_results = [results for results in list(executor.map(scrape_single_thread,codes[i:i+batch_size])) if len(results) > 0]
                                                                                   ~~~~~^^^^^^^^^^^^^^^^
TypeError: 'int' object is not subscriptable
2023-12-13 03:57:56,489 [ERROR]: An error occurred: 'Debtor Name'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_bizfileonline.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper48.py", line 110, in scrape_with_refcodes
    batch_results = [results for results in list(executor.map(scrape_single_thread,list(range(i,i+batch_size)))) if len(results) > 0]
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper48.py", line 102, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper48.py", line 52, in scrape_single
    print(parsed_data_list[i]['Debtor Name'])
          ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
KeyError: 'Debtor Name'
2023-12-13 03:58:21,559 [ERROR]: An error occurred: 'Label'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_bizfileonline.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper48.py", line 110, in scrape_with_refcodes
    batch_results = [results for results in list(executor.map(scrape_single_thread,list(range(i,i+batch_size)))) if len(results) > 0]
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper48.py", line 102, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper48.py", line 52, in scrape_single
    print(parsed_data_list[i]['Label'])
          ~~~~~~~~~~~~~~~~~~~^^^^^^^^^
KeyError: 'Label'
2023-12-13 03:59:45,918 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_bizfileonline.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper48.py", line 112, in scrape_with_refcodes
    batch_results = [results for results in list(executor.map(scrape_single_thread,list(range(i,i+batch_size)))) if len(results) > 0]
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper48.py", line 104, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper48.py", line 53, in scrape_single
    name = parsed_data_list['i']['VALUE']
           ~~~~~~~~~~~~~~~~^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-13 05:32:09,421 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURDATE(),
                                    time_created TIME DEFAULT CURTIME' at line 7
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURDATE(),
                                    time_created TIME DEFAULT CURTIME' at line 7

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_bizfileonline.py", line 27, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/database/database_handler_bizfileonline.py", line 17, in __init__
    self.cursor.execute(f'''
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURDATE(),
                                    time_created TIME DEFAULT CURTIME' at line 7
2023-12-13 05:37:58,916 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_bizfileonline.py", line 27, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/database/database_handler_bizfileonline.py", line 17, in __init__
    self.cursor.execute(f'''
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
2023-12-13 05:39:40,245 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_bizfileonline.py", line 27, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/database/database_handler_bizfileonline.py", line 17, in __init__
    self.cursor.execute(f'''
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
2023-12-13 05:39:50,017 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_bizfileonline.py", line 27, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/database/database_handler_bizfileonline.py", line 17, in __init__
    self.cursor.execute(f'''
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
>>>>>>> 389b6b38a32bcbcba896cff4de1ac02e38048880
2023-12-13 17:05:39,975 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 27, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 17, in __init__
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
2023-12-13 17:19:33,299 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 27, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 17, in __init__
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
2023-12-13 17:19:37,410 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 27, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 17, in __init__
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
2023-12-13 17:23:40,992 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 27, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 17, in __init__
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
2023-12-13 17:26:50,410 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 27, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 17, in __init__
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CURRENT_DATE,
                                    time_created TIME DEFAULT CURR' at line 7
2023-12-13 17:30:19,964 [ERROR]: An error occurred: name 'name' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper48.py", line 144, in scrape_with_refcodes
    batch_results = [item for sublist in executor.map(scrape_single_thread, range(i, i+batch_size)) for item in sublist if len(sublist) > 0]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper48.py", line 135, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper48.py", line 105, in scrape_single
    if any(value is None for value in [name, address, secured_party_name, secured_party_address]):
                                       ^^^^
NameError: name 'name' is not defined
2023-12-13 18:51:26,260 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper48.py", line 122, in scrape_with_refcodes
    batch_results = [item for sublist in executor.map(scrape_single_thread, range(i, i+batch_size)) for item in sublist if len(sublist) > 0]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper48.py", line 113, in scrape_single_thread
    results = self.scrape_single(self.url,data)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper48.py", line 64, in scrape_single
    result_entry.update(current_secured_party)
TypeError: 'NoneType' object is not iterable
2023-12-13 18:52:04,974 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper48.py", line 122, in scrape_with_refcodes
    batch_results = [item for sublist in executor.map(scrape_single_thread, range(i, i+batch_size)) for item in sublist if len(sublist) > 0]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper48.py", line 113, in scrape_single_thread
    results = self.scrape_single(self.url,data)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper48.py", line 64, in scrape_single
    result_entry.update(current_secured_party)
TypeError: 'NoneType' object is not iterable
2023-12-13 19:23:46,526 [ERROR]: An error occurred: 'name'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 42, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 38, in store_data
    ''', (data['name'], data['address'], data['secured_party_name'], data['secured_party_address']))
          ~~~~^^^^^^^^
KeyError: 'name'
2023-12-13 19:30:02,119 [ERROR]: An error occurred: 'name'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 42, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 38, in store_data
    ''', (data['name'], data['address'], data['secured_party_name'], data['secured_party_address']))
          ~~~~^^^^^^^^
KeyError: 'name'
2023-12-13 19:36:12,528 [ERROR]: An error occurred: 1054 (42S22): Unknown column 'debtor_address' in 'field list'
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Unknown column 'debtor_address' in 'field list'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 42, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 35, in store_data
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1054 (42S22): Unknown column 'debtor_address' in 'field list'
2023-12-13 19:46:47,351 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '(NOW()),
                                    time_created TIME DEFAULT CURTIME()' at line 7
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '(NOW()),
                                    time_created TIME DEFAULT CURTIME()' at line 7

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 27, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 17, in __init__
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '(NOW()),
                                    time_created TIME DEFAULT CURTIME()' at line 7
2023-12-13 19:47:37,033 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '(NOW()),
                                    time_created TIME DEFAULT CURRENT_T' at line 7
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '(NOW()),
                                    time_created TIME DEFAULT CURRENT_T' at line 7

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 27, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 17, in __init__
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '(NOW()),
                                    time_created TIME DEFAULT CURRENT_T' at line 7
2023-12-13 19:47:40,906 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '(NOW()),
                                    time_created TIME DEFAULT CURRENT_T' at line 7
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '(NOW()),
                                    time_created TIME DEFAULT CURRENT_T' at line 7

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 27, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 17, in __init__
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '(NOW()),
                                    time_created TIME DEFAULT CURRENT_T' at line 7
2023-12-13 19:48:27,538 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '(NOW()),
                                    time_created TIME DEFAULT CURRENT_T' at line 7
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '(NOW()),
                                    time_created TIME DEFAULT CURRENT_T' at line 7

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 27, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 17, in __init__
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '(NOW()),
                                    time_created TIME DEFAULT CURRENT_T' at line 7
2023-12-13 19:50:11,636 [ERROR]: An error occurred: 1054 (42S22): Unknown column 'date_created' in 'field list'
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Unknown column 'date_created' in 'field list'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 42, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 34, in store_data
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1054 (42S22): Unknown column 'date_created' in 'field list'
2023-12-13 23:57:22,412 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:01:11,862 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:01:25,662 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:03:53,731 [ERROR]: An error occurred: memoryview: a bytes-like object is required, not 'str'
Traceback (most recent call last):
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1038, in send
    self.sock.sendall(data)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1209, in sendall
    with memoryview(data) as view, view.cast("B") as byte_view:
         ^^^^^^^^^^^^^^^^
TypeError: memoryview: a bytes-like object is required, not 'str'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 100, in scrape_with_refcodes
    self.scrape_single(self.url,data)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 47, in scrape_single
    response = requests.post(f'{url}',data=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1113, in _send_output
    self.send(chunk)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1042, in send
    self.sock.sendall(d)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1209, in sendall
    with memoryview(data) as view, view.cast("B") as byte_view:
         ^^^^^^^^^^^^^^^^
TypeError: memoryview: a bytes-like object is required, not 'str'
2023-12-14 00:04:27,364 [ERROR]: An error occurred: memoryview: a bytes-like object is required, not 'str'
Traceback (most recent call last):
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1038, in send
    self.sock.sendall(data)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1209, in sendall
    with memoryview(data) as view, view.cast("B") as byte_view:
         ^^^^^^^^^^^^^^^^
TypeError: memoryview: a bytes-like object is required, not 'str'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 100, in scrape_with_refcodes
    self.scrape_single(self.url,data)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 47, in scrape_single
    response = requests.post(f'{url}',data=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1113, in _send_output
    self.send(chunk)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1042, in send
    self.sock.sendall(d)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1209, in sendall
    with memoryview(data) as view, view.cast("B") as byte_view:
         ^^^^^^^^^^^^^^^^
TypeError: memoryview: a bytes-like object is required, not 'str'
2023-12-14 00:09:55,379 [ERROR]: An error occurred: memoryview: a bytes-like object is required, not 'str'
Traceback (most recent call last):
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1038, in send
    self.sock.sendall(data)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1209, in sendall
    with memoryview(data) as view, view.cast("B") as byte_view:
         ^^^^^^^^^^^^^^^^
TypeError: memoryview: a bytes-like object is required, not 'str'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 97, in scrape_with_refcodes
    self.scrape_single(self.url,data)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 44, in scrape_single
    response = requests.post(f'{url}',headers=headers, data=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1113, in _send_output
    self.send(chunk)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1042, in send
    self.sock.sendall(d)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1209, in sendall
    with memoryview(data) as view, view.cast("B") as byte_view:
         ^^^^^^^^^^^^^^^^
TypeError: memoryview: a bytes-like object is required, not 'str'
2023-12-14 00:10:58,571 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:14:48,866 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:14:56,955 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:15:25,211 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:15:34,833 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:15:54,721 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:19:09,455 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:20:48,302 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:21:00,868 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:35:00,183 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:35:28,498 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:41:28,401 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:49:57,563 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:50:48,904 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:53:08,762 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:53:33,842 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:55:49,846 [ERROR]: An error occurred: 'href'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 105, in scrape_with_refcodes
    self.scrape_single(self.url,data)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 62, in scrape_single
    print(td_a['href'])
          ~~~~^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 'href'
2023-12-14 00:57:51,853 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:58:31,592 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:59:04,596 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 00:59:38,229 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 01:00:18,294 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 01:00:41,595 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 18:31:36,975 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 18:32:50,930 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-14 18:39:48,040 [ERROR]: An error occurred: name 'chart' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 167, in scrape_with_refcodes
    self.scrape_pages(data)
  File "/root/projects/corey/src/scraping/scraper49.py", line 103, in scrape_pages
    print(f"Extract search results for '{chart}'.")
                                         ^^^^^
NameError: name 'chart' is not defined. Did you mean: 'char'?
2023-12-14 19:16:44,981 [ERROR]: An error occurred: No connection adapters were found for "['https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=20-7799272&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7482230&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7460752&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=20-7107276&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7123217&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=21-7163545&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7560449&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7485929&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7484048&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=21-7301608&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-0469547&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0134191&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-0423663&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0003545&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0090839&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0496871&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0496888&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7105727&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7105727&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7141524&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7085007&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7086615&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7501796&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7318453&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7109638&page=name']"
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 161, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 110, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 27, in scrape_single
    response = requests.get(url)
               ^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 697, in send
    adapter = self.get_adapter(url=request.url)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 794, in get_adapter
    raise InvalidSchema(f"No connection adapters were found for {url!r}")
requests.exceptions.InvalidSchema: No connection adapters were found for "['https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=20-7799272&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7482230&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7460752&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=20-7107276&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7123217&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=21-7163545&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7560449&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7485929&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7484048&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=21-7301608&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-0469547&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0134191&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-0423663&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0003545&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0090839&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0496871&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0496888&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7105727&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7105727&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7141524&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7085007&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7086615&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7501796&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7318453&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7109638&page=name']"
2023-12-14 19:19:50,649 [ERROR]: An error occurred: No connection adapters were found for "['https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=20-7799272&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7482230&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7460752&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=20-7107276&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7123217&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=21-7163545&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7560449&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7485929&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7484048&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=21-7301608&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-0469547&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0134191&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-0423663&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0003545&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0090839&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0496871&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0496888&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7105727&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7105727&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7141524&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7085007&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7086615&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7501796&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7318453&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7109638&page=name']"
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 178, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 127, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 44, in scrape_single
    response = requests.get(url,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 697, in send
    adapter = self.get_adapter(url=request.url)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 794, in get_adapter
    raise InvalidSchema(f"No connection adapters were found for {url!r}")
requests.exceptions.InvalidSchema: No connection adapters were found for "['https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=20-7799272&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7482230&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7460752&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=20-7107276&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7123217&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=21-7163545&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7560449&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7485929&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7484048&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=21-7301608&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-0469547&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0134191&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-0423663&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0003545&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0090839&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0496871&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0496888&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7105727&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7105727&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7141524&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7085007&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7086615&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7501796&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7318453&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7109638&page=name']"
2023-12-14 19:21:40,368 [ERROR]: An error occurred: No connection adapters were found for "['https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=20-7799272&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7482230&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7460752&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=20-7107276&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7123217&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=21-7163545&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7560449&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7485929&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7484048&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=21-7301608&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-0469547&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0134191&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-0423663&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0003545&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0090839&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0496871&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0496888&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7105727&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7105727&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7141524&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7085007&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7086615&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7501796&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7318453&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7109638&page=name']"
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 179, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 128, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 44, in scrape_single
    response = requests.get(url,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 697, in send
    adapter = self.get_adapter(url=request.url)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 794, in get_adapter
    raise InvalidSchema(f"No connection adapters were found for {url!r}")
requests.exceptions.InvalidSchema: No connection adapters were found for "['https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=20-7799272&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7482230&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7460752&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=20-7107276&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7123217&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=21-7163545&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7560449&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7485929&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=19-7484048&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=21-7301608&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-0469547&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0134191&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-0423663&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0003545&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0090839&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0496871&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-0496888&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7105727&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7105727&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=17-7141524&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7085007&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7086615&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7501796&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=22-7318453&page=name', 'https://arc-sos.state.al.us//cgi/uccdetail.mbr/detail?ucc=23-7109638&page=name']"
2023-12-14 19:22:56,959 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 179, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 128, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 60, in scrape_single
    debtor_info = soup.find('html body.not-front main.page div.main-content div.container.main-sidebar div.content.full div div#block-sos-content div.views-element-container div div table tbody tr td.aiSosDetailValue').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-14 19:26:18,192 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 180, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 129, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 61, in scrape_single
    debtor_info = soup.find('html body.not-front main.page div.main-content div.container.main-sidebar div.content.full div div#block-sos-content div.views-element-container div div table tbody tr td.aiSosDetailValue').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-14 19:27:26,874 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 180, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 129, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 61, in scrape_single
    debtor_info = soup.find('html body.not-front main.page div.main-content div.container.main-sidebar div.content.full div div#block-sos-content div.views-element-container div div table tbody tr td.aiSosDetailValue').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-14 19:31:29,023 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 180, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 129, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 61, in scrape_single
    debtor_info = soup.find('html body.not-front main.page div.main-content div.container.main-sidebar div.content.full div div#block-sos-content div.views-element-container div div table tbody tr td.aiSosDetailValue').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-14 19:32:06,662 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 180, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 129, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 61, in scrape_single
    debtor_info = soup.find('html body.not-front main.page div.main-content div.container.main-sidebar div.content.full div div#block-sos-content div.views-element-container div div table tbody tr td.aiSosDetailValue').contents
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-14 19:33:00,362 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 180, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 129, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 61, in scrape_single
    debtor_info = soup.find('html body.not-front main.page div.main-content div.container.main-sidebar div.content.full div div#block-sos-content div.views-element-container div div table tbody tr td.aiSosDetailValue')[0].get_text()
                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^
TypeError: 'NoneType' object is not subscriptable
2023-12-14 19:33:27,236 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 180, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 129, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 61, in scrape_single
    debtor_info = soup.find('html body.not-front main.page div.main-content div.container.main-sidebar div.content.full div div#block-sos-content div.views-element-container div div table tbody tr td.aiSosDetailValue')[1].get_text()
                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^
TypeError: 'NoneType' object is not subscriptable
2023-12-14 19:35:09,921 [ERROR]: An error occurred: Couldn't find a tree builder with the features you requested: html.lxml. Do you need to install a parser library?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 180, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 129, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 55, in scrape_single
    soup = BeautifulSoup(response.content,'html.lxml')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/bs4/__init__.py", line 250, in __init__
    raise FeatureNotFound(
bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: html.lxml. Do you need to install a parser library?
2023-12-14 19:36:35,445 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 180, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 129, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 61, in scrape_single
    debtor_info = soup.find('/html/body/main/div/div[2]/div/div/div/div/div/div/table[2]/tbody/tr[3]/td[2]').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-14 19:38:44,532 [ERROR]: An error occurred: Couldn't find a tree builder with the features you requested: html.parse. Do you need to install a parser library?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 182, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 131, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 55, in scrape_single
    soup = BeautifulSoup(response.content,'html.parse')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/bs4/__init__.py", line 250, in __init__
    raise FeatureNotFound(
bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: html.parse. Do you need to install a parser library?
2023-12-14 19:39:06,536 [ERROR]: An error occurred: Couldn't find a tree builder with the features you requested: html.parse. Do you need to install a parser library?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 183, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 132, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 55, in scrape_single
    soup = BeautifulSoup(response.content,'html.parse')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/bs4/__init__.py", line 250, in __init__
    raise FeatureNotFound(
bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: html.parse. Do you need to install a parser library?
2023-12-14 19:39:25,551 [ERROR]: An error occurred: 'NoneType' object is not callable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 183, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 132, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 65, in scrape_single
    debtor_address = ' '.join(debtor_info.split("\n")[1:])
                              ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'NoneType' object is not callable
2023-12-14 19:40:27,136 [ERROR]: An error occurred: 'NoneType' object is not callable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 183, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 132, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 65, in scrape_single
    debtor_address = ' '.join(debtor_info.split("\n")[1:])
                              ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'NoneType' object is not callable
2023-12-14 19:42:07,486 [ERROR]: An error occurred: ResultSet object has no attribute 'contents'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 185, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 134, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 64, in scrape_single
    print(debtor_info.contents)
          ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/bs4/element.py", line 2428, in __getattr__
    raise AttributeError(
AttributeError: ResultSet object has no attribute 'contents'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?
2023-12-14 19:42:49,276 [ERROR]: An error occurred: 'NoneType' object is not callable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 185, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 134, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 74, in scrape_single
    secured_party_address = ' '.join(secured_party_info.split("\n")[1:])
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'NoneType' object is not callable
2023-12-14 19:43:34,631 [ERROR]: An error occurred: 'NoneType' object is not callable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 185, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 134, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 74, in scrape_single
    secured_party_address = ' '.join(secured_party_info.split("\n")[1:])
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'NoneType' object is not callable
2023-12-14 19:53:55,375 [ERROR]: An error occurred: name 'debtor_info' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 186, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 135, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 64, in scrape_single
    if debtor_info:
       ^^^^^^^^^^^
NameError: name 'debtor_info' is not defined
2023-12-14 20:00:38,156 [ERROR]: An error occurred: 'list' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 186, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 135, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 65, in scrape_single
    debtor = debtor_info.get_text(separator='\n')
             ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'get_text'
2023-12-14 20:02:32,598 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 189, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 138, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 62, in scrape_single
    secured_party_info = page_info[1]
                         ~~~~~~~~~^^^
IndexError: list index out of range
2023-12-14 20:03:05,962 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 189, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 138, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 74, in scrape_single
    secured_party_info = page_info[1]
                         ~~~~~~~~~^^^
IndexError: list index out of range
2023-12-14 20:05:20,758 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 188, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 137, in scrape_pages
    self.scrape_single(url)
  File "/root/projects/corey/src/scraping/scraper49.py", line 73, in scrape_single
    secured_party_info = page_info[1]
                         ~~~~~~~~~^^^
IndexError: list index out of range
2023-12-14 20:14:13,432 [ERROR]: An error occurred: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 191, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 146, in scrape_pages
    response = requests.get(next_url)
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
2023-12-14 20:16:10,917 [ERROR]: An error occurred: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 192, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 147, in scrape_pages
    response = requests.get(next_url)
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
2023-12-14 20:17:15,329 [ERROR]: An error occurred: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 193, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 148, in scrape_pages
    response = requests.get(next_url)
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
2023-12-14 20:17:44,931 [ERROR]: An error occurred: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 194, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 149, in scrape_pages
    response = requests.get(next_url)
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
2023-12-14 20:19:06,170 [ERROR]: An error occurred: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 194, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 149, in scrape_pages
    response = requests.get(next_url)
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
2023-12-14 20:22:40,214 [ERROR]: An error occurred: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 194, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 149, in scrape_pages
    response = requests.get(next_url)
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
2023-12-14 20:23:00,728 [ERROR]: An error occurred: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 194, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 149, in scrape_pages
    response = requests.get(next_url)
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
2023-12-14 20:25:17,071 [ERROR]: An error occurred: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 194, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 149, in scrape_pages
    response = requests.get(next_url)
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
2023-12-14 20:25:45,184 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 194, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 147, in scrape_pages
    next_url = get_next_page_url(soup)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 107, in get_next_page_url
    next_url = next_button.find_parent('a')['href']
               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2023-12-14 20:25:58,132 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 194, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 147, in scrape_pages
    next_url = get_next_page_url(soup)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 107, in get_next_page_url
    next_url = next_button.find_parent('a')['href']
               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2023-12-14 20:26:45,435 [ERROR]: An error occurred: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 195, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 150, in scrape_pages
    response = requests.get(next_url)
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
2023-12-14 20:28:46,776 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 195, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 148, in scrape_pages
    next_url = get_next_page_url(soup)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 107, in get_next_page_url
    next_url = next_button.find_parent('a').get('href')
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2023-12-14 20:29:32,615 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 196, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 149, in scrape_pages
    next_url = get_next_page_url(soup)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 108, in get_next_page_url
    print(next_url.contents)
          ^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-14 20:30:05,186 [ERROR]: An error occurred: cannot access free variable 'next_url' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 196, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 149, in scrape_pages
    next_url = get_next_page_url(soup)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 110, in get_next_page_url
    return next_url
           ^^^^^^^^
NameError: cannot access free variable 'next_url' where it is not associated with a value in enclosing scope
2023-12-14 20:30:46,943 [ERROR]: An error occurred: cannot access free variable 'next_url' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 196, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 149, in scrape_pages
    next_url = get_next_page_url(soup)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 110, in get_next_page_url
    return next_url
           ^^^^^^^^
NameError: cannot access free variable 'next_url' where it is not associated with a value in enclosing scope
2023-12-14 20:32:19,851 [ERROR]: An error occurred: name 'page_urls' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 195, in scrape_with_refcodes
    self.scrape_pages(data,batch_size)
  File "/root/projects/corey/src/scraping/scraper49.py", line 153, in scrape_pages
    urls.extend(page_urls)
                ^^^^^^^^^
NameError: name 'page_urls' is not defined
2023-12-14 23:32:46,444 [ERROR]: An error occurred: cannot access local variable 'url' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 185, in scrape_with_refcodes
    self.scrape_pages(batch_size)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 131, in scrape_pages
    print("\n".join(url))
                    ^^^
UnboundLocalError: cannot access local variable 'url' where it is not associated with a value
2023-12-14 23:33:14,446 [ERROR]: An error occurred: cannot access local variable 'url' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 185, in scrape_with_refcodes
    self.scrape_pages(batch_size)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 131, in scrape_pages
    print("\n".join(url))
                    ^^^
UnboundLocalError: cannot access local variable 'url' where it is not associated with a value
2023-12-14 23:34:01,531 [ERROR]: An error occurred: cannot access local variable 'url' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 185, in scrape_with_refcodes
    self.scrape_pages(batch_size)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 131, in scrape_pages
    print("\n".join(url))
                    ^^^
UnboundLocalError: cannot access local variable 'url' where it is not associated with a value
2023-12-14 23:34:40,095 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 185, in scrape_with_refcodes
    self.scrape_pages(batch_size)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 139, in scrape_pages
    result = self.scrape_single(url)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 75, in scrape_single
    secured_party = secured_party_info[0].get_text(separator='\n')
                    ~~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range
2023-12-14 23:37:07,288 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 186, in scrape_with_refcodes
    self.scrape_pages(batch_size)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 140, in scrape_pages
    result = self.scrape_single(url)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 76, in scrape_single
    secured_party = secured_party_info[0].get_text(separator='\n')
                    ~~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range
2023-12-14 23:39:08,491 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 186, in scrape_with_refcodes
    self.scrape_pages(batch_size)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 140, in scrape_pages
    result = self.scrape_single(url)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 76, in scrape_single
    secured_party = secured_party_info[0].get_text(separator='\n')
                    ~~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range
2023-12-14 23:39:30,575 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 186, in scrape_with_refcodes
    self.scrape_pages(batch_size)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 140, in scrape_pages
    result = self.scrape_single(url)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 76, in scrape_single
    secured_party = secured_party_info[0].get_text(separator='\n')
                    ~~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range
2023-12-14 23:39:52,751 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 187, in scrape_with_refcodes
    self.scrape_pages(batch_size)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 141, in scrape_pages
    result = self.scrape_single(url)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 77, in scrape_single
    secured_party = secured_party_info[0].get_text(separator='\n')
                    ~~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range
2023-12-14 23:40:31,573 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 187, in scrape_with_refcodes
    self.scrape_pages(batch_size)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 141, in scrape_pages
    result = self.scrape_single(url)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 77, in scrape_single
    secured_party = secured_party_info[0].get_text(separator='\n')
                    ~~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range
2023-12-15 00:07:56,436 [ERROR]: An error occurred: cannot access local variable 'debtor_info' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 191, in scrape_with_refcodes
    self.scrape_pages(batch_size)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 145, in scrape_pages
    result = self.scrape_single(url)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 67, in scrape_single
    for d in debtor_info:
             ^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'debtor_info' where it is not associated with a value
2023-12-15 00:08:26,998 [ERROR]: An error occurred: cannot access local variable 'debtor_info' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 192, in scrape_with_refcodes
    self.scrape_pages(batch_size)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 146, in scrape_pages
    result = self.scrape_single(url)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 68, in scrape_single
    for d in debtor_info:
             ^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'debtor_info' where it is not associated with a value
2023-12-15 00:09:28,591 [ERROR]: An error occurred: cannot access local variable 'debtor_info' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 192, in scrape_with_refcodes
    self.scrape_pages(batch_size)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 146, in scrape_pages
    result = self.scrape_single(url)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 68, in scrape_single
    for d in debtor_info:
             ^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'debtor_info' where it is not associated with a value
2023-12-15 01:39:34,417 [ERROR]: An error occurred: cannot access local variable 'debtor_info' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 193, in scrape_with_refcodes
    self.scrape_pages(batch_size)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 147, in scrape_pages
    result = self.scrape_single(url)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 69, in scrape_single
    for d in debtor_info:
             ^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'debtor_info' where it is not associated with a value
2023-12-15 01:42:50,815 [ERROR]: An error occurred: cannot access local variable 'debtor_info' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 193, in scrape_with_refcodes
    self.scrape_pages(batch_size)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 147, in scrape_pages
    result = self.scrape_single(url)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 69, in scrape_single
    for d in debtor_info:
             ^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'debtor_info' where it is not associated with a value
2023-12-15 01:47:10,290 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-15 01:52:14,762 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
TypeError: 'NoneType' object is not iterable
2023-12-15 01:57:22,766 [ERROR]: An error occurred: Scraper49.scrape_with_refcodes() missing 1 required positional argument: 'batch_size'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Scraper49.scrape_with_refcodes() missing 1 required positional argument: 'batch_size'
2023-12-15 04:46:19,033 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 138, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 112, in get_page_links
    page_results = [f'{tr.find('a')['href']}' for tr in tr_elements[1:-1]]
                       ~~~~~~~~~~~~^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2023-12-15 15:54:41,041 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 138, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 112, in get_page_links
    page_results = [f'{tr.find('a')['href']}' for tr in tr_elements[1:-1]]
                       ~~~~~~~~~~~~^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2023-12-15 19:33:49,131 [ERROR]: An error occurred: name 'headers' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 152, in scrape_with_refcodes
    response = requests.post(current_url,data=data,headers=headers)
                                                           ^^^^^^^
NameError: name 'headers' is not defined
2023-12-15 19:37:12,198 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 185, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 118, in get_page_links
    page_results = [f'{tr.find('a')['href']}' for tr in tr_elements[1:-1]]
                       ~~~~~~~~~~~~^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2023-12-15 19:45:45,036 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 186, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 119, in get_page_links
    page_results = [f'{tr.find('td a')['href']}' for tr in tr_elements[1:-1]]
                       ~~~~~~~~~~~~~~~^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2023-12-15 19:59:03,780 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 190, in scrape_with_refcodes
    print("\n".join(urls))
          ^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2023-12-15 20:03:57,831 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 190, in scrape_with_refcodes
    print("\n".join(urls))
          ^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2023-12-15 20:07:37,222 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 190, in scrape_with_refcodes
    print("\n".join(urls))
          ^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2023-12-15 20:10:56,507 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 190, in scrape_with_refcodes
    print("\n".join(urls))
          ^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2023-12-15 20:13:45,331 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 190, in scrape_with_refcodes
    print("\n".join(urls))
          ^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2023-12-15 20:16:49,386 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 190, in scrape_with_refcodes
    print("\n".join(urls))
          ^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2023-12-15 20:18:42,881 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 190, in scrape_with_refcodes
    print("\n".join(urls))
          ^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2023-12-15 20:21:57,745 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 193, in scrape_with_refcodes
    print("\n".join(urls))
          ^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2023-12-15 20:29:09,804 [ERROR]: An error occurred: No connection adapters were found for "javascript:__doPostBack('ctl00$ContentPlaceHolder1$PortalPageControl1$ctl14$linkbutton1','')"
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 196, in scrape_with_refcodes
    result = self.scrape_single(url)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 53, in scrape_single
    response = requests.get(url,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 697, in send
    adapter = self.get_adapter(url=request.url)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 794, in get_adapter
    raise InvalidSchema(f"No connection adapters were found for {url!r}")
requests.exceptions.InvalidSchema: No connection adapters were found for "javascript:__doPostBack('ctl00$ContentPlaceHolder1$PortalPageControl1$ctl14$linkbutton1','')"
2023-12-15 20:40:55,091 [ERROR]: An error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/packages/six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper50.py", line 193, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls)]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 53, in scrape_single
    response = requests.get(url,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2023-12-15 20:52:31,400 [ERROR]: An error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/packages/six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper50.py", line 194, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls)]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper50.py", line 54, in scrape_single
    response = requests.get(url,headers=headers,)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2023-12-15 23:43:11,901 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 192, in scrape_with_refcodes
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 85, in scrape_single
    secured_party_name = result_set.find('span',id='ctl00_ContentPlaceHolder1_PortalPageControl1_ctl22_uccFilingRepeater_ctl01_uccNamesRepeater_ctl03_NameLabel')
                         ^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find'
2023-12-15 23:46:15,206 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 119, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 71, in scrape_single
    print(result_set.contents)
          ^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-15 23:46:45,582 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 119, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 71, in scrape_single
    print(result_set.contents)
          ^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-15 23:48:09,818 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 119, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 71, in scrape_single
    print(result_set.contents)
          ^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-15 23:49:34,386 [ERROR]: An error occurred: cannot access local variable 'last_interrupt_char_index' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 130, in scrape_with_refcodes
    for char in ascii_uppercase[last_interrupt_char_index:]:
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'last_interrupt_char_index' where it is not associated with a value
2023-12-15 23:50:06,452 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 119, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 86, in scrape_single
    secured_party_name = soup.find('span',id='ctl00_ContentPlaceHolder1_PortalPageControl1_ctl22_uccFilingRepeater_ctl01_uccNamesRepeater_ctl03_NameLabel').get_text()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-15 23:54:30,142 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 119, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 76, in scrape_single
    debtor_name = soup.find('div:nth-child(1) > div:nth-child(2) > div:nth-child(3) > span:nth-child(1)').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-15 23:54:41,146 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 119, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 76, in scrape_single
    debtor_name = soup.find('div:nth-child(1) > div:nth-child(2) > div:nth-child(3) > span:nth-child(1)').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-15 23:55:21,234 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 119, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 76, in scrape_single
    debtor_name = soup.find('div:nth-child(2) > div:nth-child(3) > span:nth-child(1)').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-15 23:56:05,085 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 119, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 76, in scrape_single
    debtor_name = result_set.find('div:nth-child(1) > div:nth-child(2) > div:nth-child(3) > span:nth-child(1)').get_text()
                  ^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find'
2023-12-15 23:56:22,789 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 119, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 76, in scrape_single
    debtor_name = result_set.find('div:nth-child(2) > div:nth-child(3) > span:nth-child(1)').get_text()
                  ^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find'
2023-12-15 23:56:35,881 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 119, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 71, in scrape_single
    print(result_set.contents)
          ^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-15 23:58:07,880 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 119, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 71, in scrape_single
    print(result_set.contents)
          ^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-15 23:58:28,758 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 119, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 71, in scrape_single
    print(result_set.contents)
          ^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-16 00:04:07,496 [ERROR]: An error occurred: ResultSet object has no attribute 'find'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 120, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 77, in scrape_single
    debtor_name = result_set.find('').get_text()
                  ^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 2428, in __getattr__
    raise AttributeError(
AttributeError: ResultSet object has no attribute 'find'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?
2023-12-16 00:05:16,382 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 121, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 72, in scrape_single
    print(f'Content: {result_set.contents}')
                      ^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-16 00:05:41,072 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 122, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 73, in scrape_single
    print(f'Content: {result_set.contents}')
                      ^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-16 00:07:43,910 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 122, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 73, in scrape_single
    print(f'Content: {result_set.contents}')
                      ^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-16 00:08:06,183 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 122, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 73, in scrape_single
    print(f'Content: {result_set.contents}')
                      ^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-16 00:08:45,706 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 122, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 73, in scrape_single
    print(f'Content: {result_set.contents}')
                      ^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-16 00:09:46,807 [ERROR]: An error occurred: ResultSet object has no attribute 'contents'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 122, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 73, in scrape_single
    print(f'Content: {result_set.contents}')
                      ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 2428, in __getattr__
    raise AttributeError(
AttributeError: ResultSet object has no attribute 'contents'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?
2023-12-16 00:10:04,206 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 122, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 73, in scrape_single
    print(f'Content: {result_set.contents}')
                      ^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-16 00:10:13,467 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 122, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 79, in scrape_single
    debtor_name = result_set.find('').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-16 00:12:48,314 [ERROR]: An error occurred: object of type 'NoneType' has no len()
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 122, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 72, in scrape_single
    print(f'Result length: {len(result_set)}')
                            ^^^^^^^^^^^^^^^
TypeError: object of type 'NoneType' has no len()
2023-12-16 00:18:18,602 [ERROR]: An error occurred: 'list' object has no attribute 'lower'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 128, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 77, in scrape_single
    if 'namesrepeater' in td.contents.lower():
                          ^^^^^^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'lower'
2023-12-16 00:18:44,629 [ERROR]: An error occurred: 1
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 128, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 77, in scrape_single
    if 'namesrepeater' in td[1].contents.lower():
                          ~~^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 1
2023-12-16 00:18:56,404 [ERROR]: An error occurred: 1
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 129, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 78, in scrape_single
    if 'namesrepeater' in td[1].contents.lower():
                          ~~^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 1
2023-12-16 00:19:40,695 [ERROR]: An error occurred: 'list' object has no attribute 'lower'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 129, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 78, in scrape_single
    if 'namesrepeater' in td.contents.lower():
                          ^^^^^^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'lower'
2023-12-16 00:20:02,707 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 129, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 86, in scrape_single
    debtor_name = result_set.find('').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-16 00:20:32,821 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 129, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 86, in scrape_single
    debtor_name = result_set.find('').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-16 00:21:38,177 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 130, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 87, in scrape_single
    debtor_name = result_set.find('').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-16 00:22:40,706 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 130, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 87, in scrape_single
    debtor_name = result_set.find('').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-16 00:23:09,480 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 132, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 89, in scrape_single
    debtor_name = result_set.find('').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-16 00:24:16,727 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 133, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 90, in scrape_single
    debtor_name = result_set.find('').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-16 00:24:32,486 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 133, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 90, in scrape_single
    debtor_name = result_set.find('').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-16 00:29:02,213 [ERROR]: An error occurred: cannot access local variable 'result_set' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 133, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 72, in scrape_single
    print(f'Result length: {len(result_set)}')
                                ^^^^^^^^^^
UnboundLocalError: cannot access local variable 'result_set' where it is not associated with a value
2023-12-16 00:29:19,306 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 133, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 90, in scrape_single
    debtor_name = result_set.find('.uccFilingRepeater > div:nth-child(1) > div:nth-child(4) > div:nth-child(9) > div:nth-child(2) > div:nth-child(3) > span:nth-child(1)').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-16 00:31:08,422 [ERROR]: An error occurred: name 'div' is not defined
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 136, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 79, in scrape_single
    print(div)
          ^^^
NameError: name 'div' is not defined. Did you mean: 'divs'?
2023-12-16 00:31:16,962 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 136, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 93, in scrape_single
    debtor_name = result_set.find('.uccFilingRepeater > div:nth-child(1) > div:nth-child(4) > div:nth-child(9) > div:nth-child(2) > div:nth-child(3) > span:nth-child(1)').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-16 00:31:51,150 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 138, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 95, in scrape_single
    debtor_name = result_set.find('.uccFilingRepeater > div:nth-child(1) > div:nth-child(4) > div:nth-child(9) > div:nth-child(2) > div:nth-child(3) > span:nth-child(1)').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-16 00:34:02,383 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 138, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 95, in scrape_single
    debtor_name = divs[8].find('span:nth-child(1)').get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-16 00:35:06,735 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 138, in scrape_with_refcodes
    result = self.scrape_single("https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=f8c2e994-e39c-4098-9ac5-000c83e0906f")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 96, in scrape_single
    debtor_address = divs[8].find_all('span')[2].get_text()
                     ~~~~~~~~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range
2023-12-16 00:35:22,826 [ERROR]: An error occurred: cannot access local variable 'last_interrupt_char_index' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 149, in scrape_with_refcodes
    for char in ascii_uppercase[last_interrupt_char_index:]:
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'last_interrupt_char_index' where it is not associated with a value
2023-12-16 00:37:37,638 [ERROR]: An error occurred: cannot access local variable 'last_interrupt_char_index' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 130, in scrape_with_refcodes
    for char in ascii_uppercase[last_interrupt_char_index:]:
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'last_interrupt_char_index' where it is not associated with a value
2023-12-16 00:40:23,013 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 196, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls)]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 86, in scrape_single
    secured_party_name = divs[11].find_all('span')[0].get_text()
                         ~~~~^^^^
IndexError: list index out of range
2023-12-16 00:45:14,819 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 196, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls)]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 86, in scrape_single
    secured_party_name = divs[11].find_all('span')[0].get_text()
                         ~~~~^^^^
IndexError: list index out of range
2023-12-16 00:48:59,767 [ERROR]: An error occurred: sequence item 1: expected str instance, Tag found
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 197, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls)]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 77, in scrape_single
    print(['*************************\n'.join(div.contents) for div in divs])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: sequence item 1: expected str instance, Tag found
2023-12-16 00:56:17,210 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 89, in scrape_single
    secured_party_name = divs[index].find_all('span')[0].get_text()
                         ~~~~^^^^^^^
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 202, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls)]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 93, in scrape_single
    secured_party_name = divs[index].find_all('span')[0].get_text()
                         ~~~~^^^^^^^
IndexError: list index out of range
2023-12-16 01:02:09,033 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 89, in scrape_single
    secured_party_name = divs[index].find_all('span')[0].get_text()
                         ~~~~^^^^^^^
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 202, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls)]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 93, in scrape_single
    secured_party_name = divs[index].find_all('span')[0].get_text()
                         ~~~~^^^^^^^
IndexError: list index out of range
2023-12-16 01:15:14,433 [ERROR]: An error occurred: cannot access local variable 'secured_party_name' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 203, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls)]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 96, in scrape_single
    result_dict.update({'secured_party_name':secured_party_name})
                                             ^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'secured_party_name' where it is not associated with a value
2023-12-16 01:24:36,790 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 198, in scrape_with_refcodes
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 84, in scrape_single
    debtor_address = divs[8].find_all('span')[1].get_text()
                         ^^^^^^^^
IndexError: list index out of range
2023-12-16 01:27:44,316 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 202, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls)]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 86, in scrape_single
    secured_party_name = divs[11].find_all('span')[0].get_text()
                         ~~~~^^^^
IndexError: list index out of range
2023-12-16 04:04:44,847 [ERROR]: An error occurred: can only concatenate str (not "int") to str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 164, in scrape_with_refcodes
    num = next(num_gen)
          ^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 120, in num_generator
    num += 1
TypeError: can only concatenate str (not "int") to str
2023-12-16 04:08:09,211 [ERROR]: An error occurred: slice(1, -1, None)
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 186, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 114, in get_page_links
    page_results = [f'{tr.find('a')['href']}' for tr in tr_elements[1:-1] if tr.find('a') and 'https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=' in tr.find('a')['href']]
                                                        ~~~~~~~~~~~^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: slice(1, -1, None)
2023-12-16 04:14:33,181 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 187, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 115, in get_page_links
    page_results = [f'{tr.find('a')['href']}' for tr in tr_elements]
                       ~~~~~~~~~~~~^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2023-12-16 04:15:14,307 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 189, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 117, in get_page_links
    page_results = [f'{tr.find('a')['href']}' for tr in tr_elements]
                       ~~~~~~~~~~~~^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2023-12-16 04:16:06,749 [ERROR]: An error occurred: 'NoneType' object is not callable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 189, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 113, in get_page_links
    tr_elements = table.select_query('tr a')
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'NoneType' object is not callable
2023-12-16 04:16:22,240 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 189, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 114, in get_page_links
    for tr in tr_elements:
TypeError: 'NoneType' object is not iterable
2023-12-16 04:24:16,986 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 189, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 117, in get_page_links
    page_results = [f'{tr.find('a')['href']}' for tr in tr_elements]
                       ~~~~~~~~~~~~^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2023-12-16 04:24:31,027 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 189, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 117, in get_page_links
    page_results = [f'{tr.find('a')['href']}' for tr in tr_elements]
                       ~~~~~~~~~~~~^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2023-12-16 04:24:52,280 [ERROR]: An error occurred: Invalid URL 'NameDocs?District=500&selectedName=A+%26+M+MOTORS+INC&sort_desc=True': No scheme supplied. Perhaps you meant https://NameDocs?District=500&selectedName=A+%26+M+MOTORS+INC&sort_desc=True?
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 199, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 51, in scrape_single
    response = requests.get(url)
               ^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL 'NameDocs?District=500&selectedName=A+%26+M+MOTORS+INC&sort_desc=True': No scheme supplied. Perhaps you meant https://NameDocs?District=500&selectedName=A+%26+M+MOTORS+INC&sort_desc=True?
2023-12-16 04:25:34,560 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 199, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 73, in scrape_single
    debtor_name = soup.find("div.form-group:nth-child(4) > div:nth-child(2) > input:nth-child(1)").attrs['value']#divs[8].find_all('span')[0].get_text()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-19 00:13:39,899 [ERROR]: An error occurred: 'href'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 211, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 63, in scrape_single
    info_links = [a['href'] for a in soup.find_all("a") if 'selecteddoc' in a['href'].lower()]
                                                                            ~^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 'href'
2023-12-19 00:14:26,350 [ERROR]: An error occurred: 'href'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 211, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 63, in scrape_single
    info_links = [a['href'] for a in soup.find_all("a") if 'selecteddoc' in a['href'].lower()]
                                                                            ~^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 'href'
2023-12-19 00:15:28,511 [ERROR]: An error occurred: 'href'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 211, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 63, in scrape_single
    info_links = [a.attrs['href'] for a in soup.find_all("a") if 'selecteddoc' in a['href'].lower()]
                                                                                  ~^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 'href'
2023-12-19 00:17:03,090 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 211, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 73, in scrape_single
    tr_elements = tbody.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-19 00:25:31,345 [ERROR]: An error occurred: 'href'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 211, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 63, in scrape_single
    info_links = [a.attrs['href'] for a in soup.find_all("a")]# if 'href' in a.attrs and 'selecteddoc' in a['href'].lower()]
                  ~~~~~~~^^^^^^^^
KeyError: 'href'
2023-12-19 00:26:49,538 [ERROR]: An error occurred: name 'info_links' is not defined
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 216, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 70, in scrape_single
    print(f'doc links:{info_links}')
                       ^^^^^^^^^^
NameError: name 'info_links' is not defined
2023-12-19 00:32:41,515 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 214, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 76, in scrape_single
    tr_elements = tbody.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-19 00:40:23,633 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 219, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 81, in scrape_single
    tr_elements = tbody.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-19 00:42:09,466 [ERROR]: An error occurred: 1
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 219, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 92, in scrape_single
    debtor_name = debtor.find("td")[1].get_text()
                  ~~~~~~~~~~~~~~~~~^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 1
2023-12-19 00:45:18,464 [ERROR]: An error occurred: 1
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 220, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 93, in scrape_single
    debtor_name = debtor.find("td")[1].get_text()
                  ~~~~~~~~~~~~~~~~~^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 1
2023-12-19 00:45:38,771 [ERROR]: An error occurred: 1
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 220, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 93, in scrape_single
    debtor_name = debtor.find("td")[1].get_text()
                  ~~~~~~~~~~~~~~~~~^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 1
2023-12-19 00:48:29,357 [ERROR]: An error occurred: 1
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 221, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 94, in scrape_single
    debtor_name = debtor.find("td")[1].get_text()
                  ~~~~~~~~~~~~~~~~~^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 1
2023-12-19 01:02:53,255 [ERROR]: An error occurred: 1
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 220, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 93, in scrape_single
    debtor_name = debtor.find("td")[1].get_text()
                  ~~~~~~~~~~~~~~~~~^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 1
2023-12-19 01:13:43,245 [ERROR]: An error occurred: 1
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 222, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 95, in scrape_single
    debtor_name = debtor.find("td")[1].get_text()
                  ~~~~~~~~~~~~~~~~~^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 1
2023-12-19 01:16:38,815 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 223, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 80, in scrape_single
    print(table.contents)
          ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-19 01:17:17,800 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 223, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 80, in scrape_single
    print(table.contents)
          ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-19 01:18:16,552 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 223, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 80, in scrape_single
    print(table.contents)
          ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-19 01:39:28,324 [ERROR]: An error occurred: 1
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 221, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 94, in scrape_single
    debtor_name = tr.find("td")[1].get_text()
                  ~~~~~~~~~~~~~^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 1
2023-12-19 01:39:45,753 [ERROR]: An error occurred: 1
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 221, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 94, in scrape_single
    debtor_name = tr.find("td")[1].get_text()
                  ~~~~~~~~~~~~~^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 1
2023-12-19 02:02:36,155 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 233, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 100, in scrape_single
    for tr in tr_elements:
TypeError: 'NoneType' object is not iterable
2023-12-19 02:04:00,498 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 233, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 100, in scrape_single
    for tr in tr_elements:
TypeError: 'NoneType' object is not iterable
2023-12-19 02:05:33,267 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 233, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 100, in scrape_single
    for tr in tr_elements:
TypeError: 'NoneType' object is not iterable
2023-12-19 02:07:25,769 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 233, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 96, in scrape_single
    tr_elements = table.find('tr')
                  ^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find'
2023-12-19 02:08:31,699 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 233, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 96, in scrape_single
    tr_elements = table.find('tr')
                  ^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find'
2023-12-19 02:09:00,438 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 233, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 96, in scrape_single
    tr_elements = table.find('tr')
                  ^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find'
2023-12-19 02:10:24,507 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 237, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 96, in scrape_single
    tr_elements = table.find('tr')
                  ^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find'
2023-12-19 02:14:15,623 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 234, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 96, in scrape_single
    print(table.contents)
          ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-19 02:14:42,698 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 234, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 96, in scrape_single
    print(table.contents)
          ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-19 02:15:02,974 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 234, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 96, in scrape_single
    print(table.contents)
          ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-19 02:15:24,449 [ERROR]: An error occurred: name 'tr_elements' is not defined
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 234, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 101, in scrape_single
    for tr in tr_elements:
              ^^^^^^^^^^^
NameError: name 'tr_elements' is not defined
2023-12-19 02:26:43,707 [ERROR]: An error occurred: name 'tr_elements' is not defined
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 234, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 101, in scrape_single
    for tr in tr_elements:
              ^^^^^^^^^^^
NameError: name 'tr_elements' is not defined
2023-12-19 02:29:26,638 [ERROR]: An error occurred: name 'tr_elements' is not defined
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 235, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 102, in scrape_single
    for tr in tr_elements:
              ^^^^^^^^^^^
NameError: name 'tr_elements' is not defined
2023-12-19 15:04:22,735 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 235, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 108, in scrape_single
    debtor_name = tr.find("td")[1].get_text()
                  ~~~~~~~~~~~~~^^^
TypeError: 'NoneType' object is not subscriptable
2023-12-19 15:12:39,988 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 245, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 108, in scrape_single
    if 'debtor' in tr.find('span').get_text():
                   ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-19 15:13:25,846 [ERROR]: An error occurred: 'NoneType' object has no attribute 'contents'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 246, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 108, in scrape_single
    print(tr.find('span').contents)
          ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'contents'
2023-12-19 15:13:49,110 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 246, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 109, in scrape_single
    if 'debtor' in tr.find('span').get_text():
                   ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2023-12-19 15:14:18,068 [ERROR]: An error occurred: cannot access local variable 'result_dict' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 246, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 132, in scrape_single
    print(result_dict)
          ^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'result_dict' where it is not associated with a value
2023-12-19 15:18:08,431 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 40, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_ucc_scraper.py", line 37, in store_data
    ''', (data['debtor_name'], data['debtor_address'], data['secured_party_name'], data['secured_party_address']))
          ~~~~^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-19 15:30:10,450 [ERROR]: An error occurred: ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 249, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 122, in scrape_single
    secured_party_name = tr.find_all('td').get_text()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 2428, in __getattr__
    raise AttributeError(
AttributeError: ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?
2023-12-19 15:32:04,328 [ERROR]: An error occurred: ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 250, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 123, in scrape_single
    secured_party_name = tr.find_all('td').get_text()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 2428, in __getattr__
    raise AttributeError(
AttributeError: ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?
2023-12-19 15:32:39,045 [ERROR]: An error occurred: ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 250, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 123, in scrape_single
    secured_party_name = tr.find_all('td').get_text()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 2428, in __getattr__
    raise AttributeError(
AttributeError: ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?
2023-12-19 15:33:20,752 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 40, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_ucc_scraper.py", line 37, in store_data
    ''', (data['debtor_name'], data['debtor_address'], data['secured_party_name'], data['secured_party_address']))
          ~~~~^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-19 15:33:42,096 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 40, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_ucc_scraper.py", line 37, in store_data
    ''', (data['debtor_name'], data['debtor_address'], data['secured_party_name'], data['secured_party_address']))
          ~~~~^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-19 15:35:28,841 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 40, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_ucc_scraper.py", line 38, in store_data
    ''', (data['debtor_name'], data['debtor_address'], data['secured_party_name'], data['secured_party_address']))
          ~~~~^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-19 15:36:30,704 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 40, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_ucc_scraper.py", line 38, in store_data
    ''', (data['debtor_name'], data['debtor_address'], data['secured_party_name'], data['secured_party_address']))
          ~~~~^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-19 15:37:33,172 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 40, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_ucc_scraper.py", line 39, in store_data
    ''', (data['debtor_name'], data['debtor_address'], data['secured_party_name'], data['secured_party_address']))
          ~~~~^^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-19 15:42:32,456 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 250, in scrape_with_refcodes
    batch_results = [item for results in executor.map(self.scrape_single,urls[i:i+batch_size]) for item in results]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 119, in scrape_single
    debtor_address = tr.find_all("td")[2].get_text()
                     ~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range
2023-12-19 16:06:03,332 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 250, in scrape_with_refcodes
    batch_results = [item for results in executor.map(self.scrape_single,urls[i:i+batch_size]) for item in results]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 119, in scrape_single
    debtor_address = " ".join(tr.find_all("td")[2].get_text().split())
                              ~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range
2023-12-19 16:12:32,050 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 253, in scrape_with_refcodes
    batch_results = [item for results in executor.map(self.scrape_single,urls[i:i+batch_size]) for item in results]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 127, in scrape_single
    secured_party_address = " ".join(tr.find_all("td")[2].get_text().split())
                                     ~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range
2023-12-19 16:13:27,854 [ERROR]: An error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/packages/six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 256, in scrape_with_refcodes
    batch_results = [item for results in executor.map(self.scrape_single,urls[i:i+batch_size]) for item in results]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 94, in scrape_single
    response = requests.get(self.searchurl+link,headers=header)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2023-12-19 22:55:40,112 [ERROR]: An error occurred: 400 Client Error: Bad Request for url: https://apps.ilsos.gov/uccsearch/https://apps.ilsos.gov/uccsearch/
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 191, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 59, in scrape_single
    response.raise_for_status()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://apps.ilsos.gov/uccsearch/https://apps.ilsos.gov/uccsearch/
2023-12-19 22:56:11,424 [ERROR]: An error occurred: 400 Client Error: Bad Request for url: https://apps.ilsos.gov/uccsearch/https://apps.ilsos.gov/uccsearch/
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 191, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 59, in scrape_single
    response.raise_for_status()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://apps.ilsos.gov/uccsearch/https://apps.ilsos.gov/uccsearch/
2023-12-19 22:57:01,810 [ERROR]: An error occurred: 400 Client Error: Bad Request for url: https://apps.ilsos.gov/uccsearch/
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 191, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 59, in scrape_single
    response.raise_for_status()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://apps.ilsos.gov/uccsearch/
2023-12-19 22:57:29,756 [ERROR]: An error occurred: 400 Client Error: Bad Request for url: https://apps.ilsos.gov/uccsearch/
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 191, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 59, in scrape_single
    response.raise_for_status()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://apps.ilsos.gov/uccsearch/
2023-12-19 23:06:04,676 [ERROR]: An error occurred: 503 Server Error: Service Unavailable for url: https://corp.sec.state.ma.us/corpweb/UCCSearch/UCCSearch.aspx
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 191, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 59, in scrape_single
    response.raise_for_status()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 503 Server Error: Service Unavailable for url: https://corp.sec.state.ma.us/corpweb/UCCSearch/UCCSearch.aspx
2023-12-19 23:07:47,898 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 48, in scrape_single
    response = requests.get(url,headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 191, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 52, in scrape_single
    response = requests.get(url,headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-22 21:53:02,106 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:53:02,108 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:53:02,161 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:53:04,422 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:53:04,735 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:56:11,562 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:56:11,563 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:56:11,565 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:56:14,225 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:56:14,238 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:56:14,242 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:58:49,935 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:58:50,238 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:58:50,252 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:58:51,811 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:58:52,158 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:58:52,170 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:58:53,626 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:58:54,107 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:58:54,237 [WARNING]: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2023-12-22 21:59:30,367 [ERROR]: An error occurred: 'value'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 116, in scrape_with_refcodes
    batch_results = [result for result in list(executor.map(scrape_single_thread,codes)) if result is not None]
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 105, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 58, in scrape_single
    first_name_el = soup.find(attrs={'name':'primaryOwnerFirstName'})['value'] if soup.find(attrs={'name':'primaryOwnerFirstName'}) else None
                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 'value'
2023-12-22 23:04:59,813 [ERROR]: An error occurred: 'value'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 117, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,codes) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 105, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 58, in scrape_single
    first_name_el = soup.find(attrs={'name':'primaryOwnerFirstName'})['value'] if soup.find(attrs={'name':'primaryOwnerFirstName'}) else None
                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 'value'
2023-12-22 23:06:09,470 [ERROR]: An error occurred: 'value'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 117, in scrape_with_refcodes
    batch_results = [result for result in list(executor.map(scrape_single_thread,codes)) if result is not None]
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 105, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 58, in scrape_single
    first_name_el = soup.find(attrs={'name':'primaryOwnerFirstName'})['value'] if soup.find(attrs={'name':'primaryOwnerFirstName'}) else None
                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 'value'
2023-12-22 23:37:29,752 [ERROR]: An error occurred: 'value'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 117, in scrape_with_refcodes
    batch_results = [result for result in list(executor.map(scrape_single_thread,codes)) if result is not None]
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 105, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 58, in scrape_single
    first_name_el = soup.find(attrs={'name':'primaryOwnerFirstName'})['value'] if soup.find(attrs={'name':'primaryOwnerFirstName'}) else None
                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 'value'
2023-12-22 23:40:36,198 [ERROR]: An error occurred: 'value'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 117, in scrape_with_refcodes
    batch_results = [result for result in list(executor.map(scrape_single_thread,codes)) if result is not None]
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 105, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 58, in scrape_single
    first_name_el = soup.find(attrs={'name':'primaryOwnerFirstName'})['value'] if soup.find(attrs={'name':'primaryOwnerFirstName'}) else None
                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 'value'
2023-12-22 23:41:11,716 [ERROR]: An error occurred: 'value'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 118, in scrape_with_refcodes
    batch_results = [result for result in list(executor.map(scrape_single_thread,codes)) if result is not None]
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 106, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper43.py", line 59, in scrape_single
    first_name_el = soup.find(attrs={'name':'primaryOwnerFirstName'})['value'] if soup.find(attrs={'name':'primaryOwnerFirstName'}) else None
                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 'value'
2023-12-22 23:44:55,472 [ERROR]: An error occurred: 'address'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main.py", line 42, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler.py", line 39, in store_data
    ''', (data['first_name'], data['last_name'], data['address'], data['city'], data['state'], data['zip_code']))
                                                 ~~~~^^^^^^^^^^^
KeyError: 'address'
2023-12-22 23:59:59,030 [ERROR]: An error occurred: 'middle_name'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 46, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_bhgelite.py", line 42, in store_data
    ''', (data['first_name'],data['middle_name'], data['last_name'], data['email'], data['phone'], data['address'], data['city'], data['state'], data['zip_code']))
                             ~~~~^^^^^^^^^^^^^^^
KeyError: 'middle_name'
2023-12-26 23:31:09,257 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 48, in scrape_single
    response = requests.get(url,headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 191, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 52, in scrape_single
    response = requests.get(url,headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-26 23:32:16,590 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 45, in scrape_single
    response = requests.get(url,headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 188, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 49, in scrape_single
    response = requests.get(url,headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-26 23:35:35,208 [ERROR]: An error occurred: Invalid URL 'cis.scc.virginia.gov/': No scheme supplied. Perhaps you meant https://cis.scc.virginia.gov/?
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 188, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 45, in scrape_single
    response = requests.get(url,headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL 'cis.scc.virginia.gov/': No scheme supplied. Perhaps you meant https://cis.scc.virginia.gov/?
2023-12-26 23:36:30,071 [ERROR]: An error occurred: SOCKSHTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSConnection object at 0x7fad0e3e2b70>: Failed to establish a new connection: 0x05: Connection refused'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 809, in connect
    negotiate(self, dest_addr, dest_port)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 443, in _negotiate_SOCKS5
    self.proxy_peername, self.proxy_sockname = self._SOCKS5_request(
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 533, in _SOCKS5_request
    raise SOCKS5Error("{:#04x}: {}".format(status, error))
socks.SOCKS5Error: 0x05: Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 96, in _new_conn
    conn = socks.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 209, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 199, in create_connection
    sock.connect((remote_host, remote_port))
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 47, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 814, in connect
    raise GeneralProxyError("Socket error", error)
socks.GeneralProxyError: Socket error: 0x05: Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 127, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.contrib.socks.SOCKSConnection object at 0x7fad0f7975c0>: Failed to establish a new connection: 0x05: Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSConnection object at 0x7fad0f7975c0>: Failed to establish a new connection: 0x05: Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 45, in scrape_single
    response = requests.get(url,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: SOCKSHTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSConnection object at 0x7fad0f7975c0>: Failed to establish a new connection: 0x05: Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 96, in _new_conn
    conn = socks.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 209, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 199, in create_connection
    sock.connect((remote_host, remote_port))
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 47, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 814, in connect
    raise GeneralProxyError("Socket error", error)
socks.GeneralProxyError: Socket error: 0x05: Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 127, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.contrib.socks.SOCKSConnection object at 0x7fad0e3e2b70>: Failed to establish a new connection: 0x05: Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSConnection object at 0x7fad0e3e2b70>: Failed to establish a new connection: 0x05: Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 188, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 49, in scrape_single
    response = requests.get(url,headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: SOCKSHTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSConnection object at 0x7fad0e3e2b70>: Failed to establish a new connection: 0x05: Connection refused'))
2023-12-26 23:37:08,214 [ERROR]: An error occurred: SOCKSHTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSConnection object at 0x7fd1042c2bd0>: Failed to establish a new connection: 0x05: Connection refused'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 809, in connect
    negotiate(self, dest_addr, dest_port)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 443, in _negotiate_SOCKS5
    self.proxy_peername, self.proxy_sockname = self._SOCKS5_request(
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 533, in _SOCKS5_request
    raise SOCKS5Error("{:#04x}: {}".format(status, error))
socks.SOCKS5Error: 0x05: Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 96, in _new_conn
    conn = socks.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 209, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 199, in create_connection
    sock.connect((remote_host, remote_port))
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 47, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 814, in connect
    raise GeneralProxyError("Socket error", error)
socks.GeneralProxyError: Socket error: 0x05: Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 127, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.contrib.socks.SOCKSConnection object at 0x7fd1056723c0>: Failed to establish a new connection: 0x05: Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSConnection object at 0x7fd1056723c0>: Failed to establish a new connection: 0x05: Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 45, in scrape_single
    response = requests.get(url,proxies=proxy_dict)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: SOCKSHTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSConnection object at 0x7fd1056723c0>: Failed to establish a new connection: 0x05: Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 96, in _new_conn
    conn = socks.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 209, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 199, in create_connection
    sock.connect((remote_host, remote_port))
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 47, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 814, in connect
    raise GeneralProxyError("Socket error", error)
socks.GeneralProxyError: Socket error: 0x05: Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 127, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.contrib.socks.SOCKSConnection object at 0x7fd1042c2bd0>: Failed to establish a new connection: 0x05: Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSConnection object at 0x7fd1042c2bd0>: Failed to establish a new connection: 0x05: Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 188, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 49, in scrape_single
    #     response = requests.get(url,headers=headers,proxies=proxy_dict)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: SOCKSHTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSConnection object at 0x7fd1042c2bd0>: Failed to establish a new connection: 0x05: Connection refused'))
2023-12-26 23:37:44,875 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 255, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 175, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-26 23:41:33,597 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 255, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 175, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-26 23:42:16,698 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 188, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 45, in scrape_single
    response = requests.get(url,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-26 23:44:32,778 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 188, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 45, in scrape_single
    response = requests.get(url,proxies=proxy_dict,verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-27 00:07:09,414 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 188, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 45, in scrape_single
    response = requests.get(url,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-27 00:13:11,459 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f03a5ba16d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f03a5ba16d0>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f03a5ba16d0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 188, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 45, in scrape_single
    response = requests.get(url,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f03a5ba16d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2023-12-27 00:24:35,678 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd7963a5160>: Failed to establish a new connection: [Errno 111] Connection refused'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fd7963a5160>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd7963a5160>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 188, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 45, in scrape_single
    response = requests.get(url,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd7963a5160>: Failed to establish a new connection: [Errno 111] Connection refused'))
2023-12-27 00:25:21,866 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 255, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 175, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-27 00:26:01,063 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 255, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 175, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-27 00:38:18,112 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 255, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 175, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-27 00:38:54,436 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 255, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 175, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-27 00:40:02,717 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ff3c2997950>: Failed to establish a new connection: [Errno 111] Connection refused'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7ff3c2997950>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ff3c2997950>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 188, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 45, in scrape_single
    response = requests.get(url,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ff3c2997950>: Failed to establish a new connection: [Errno 111] Connection refused'))
2023-12-27 01:26:57,047 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 255, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 175, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-27 01:45:39,515 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 258, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 178, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-27 01:49:44,472 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 258, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 178, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-27 01:53:50,205 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 258, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 178, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-27 04:26:43,082 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 258, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 178, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-27 04:38:44,456 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 258, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 178, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-27 04:40:45,792 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 258, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 178, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-27 04:52:35,690 [ERROR]: An error occurred: cannot access local variable 'response' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 191, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 53, in scrape_single
    print(f'Status code: {response.status_code}')
                          ^^^^^^^^
UnboundLocalError: cannot access local variable 'response' where it is not associated with a value
2023-12-27 04:57:51,467 [ERROR]: An error occurred: cannot access local variable 'response' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 191, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 53, in scrape_single
    print(f'Status code: {response.status_code}')
                          ^^^^^^^^
UnboundLocalError: cannot access local variable 'response' where it is not associated with a value
2023-12-27 16:31:15,839 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 258, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 178, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-28 19:15:18,924 [ERROR]: An error occurred: 'list' object is not an iterator
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_bhgelite.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 125, in scrape_with_names
    for name in next(names_generator):
                ^^^^^^^^^^^^^^^^^^^^^
TypeError: 'list' object is not an iterator
2023-12-29 00:02:22,921 [ERROR]: An error occurred: cannot access local variable 'response' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 191, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 53, in scrape_single
    print(f'Status code: {response.status_code}')
                          ^^^^^^^^
UnboundLocalError: cannot access local variable 'response' where it is not associated with a value
2023-12-29 00:10:57,865 [ERROR]: An error occurred: cannot access local variable 'response' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper53.py", line 191, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper53.py", line 53, in scrape_single
    print(f'Status code: {response.status_code}')
                          ^^^^^^^^
UnboundLocalError: cannot access local variable 'response' where it is not associated with a value
2023-12-29 00:26:43,525 [ERROR]: An error occurred: cannot access local variable 'response' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 191, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 53, in scrape_single
    print(f'Status code: {response.status_code}')
                          ^^^^^^^^
UnboundLocalError: cannot access local variable 'response' where it is not associated with a value
2023-12-29 00:39:17,164 [ERROR]: An error occurred: cannot access local variable 'response' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 191, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 53, in scrape_single
    print(f'Status code: {response.status_code}')
                          ^^^^^^^^
UnboundLocalError: cannot access local variable 'response' where it is not associated with a value
2023-12-29 22:50:09,411 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 192, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 47, in scrape_single
    response = requests.get(url,headers=headers,proxies=proxy_dict,allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-29 22:50:52,612 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 192, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 47, in scrape_single
    response = requests.get(url,headers=headers,proxies=proxy_dict,allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-29 22:59:37,674 [ERROR]: An error occurred: cannot access local variable 'response' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 192, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 54, in scrape_single
    print(f'Status code: {response.status_code}')
                          ^^^^^^^^
UnboundLocalError: cannot access local variable 'response' where it is not associated with a value
2023-12-29 23:00:11,635 [ERROR]: An error occurred: cannot access local variable 'response' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 192, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 54, in scrape_single
    print(f'Status code: {response.status_code}')
                          ^^^^^^^^
UnboundLocalError: cannot access local variable 'response' where it is not associated with a value
2023-12-29 23:01:02,237 [ERROR]: An error occurred: HTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f7e931fe7e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f7e931fe7e0>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f7e931fe7e0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 248, in scrape_with_refcodes
    response = requests.post(current_url,data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f7e931fe7e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2023-12-29 23:01:38,012 [ERROR]: An error occurred: HTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe011ea21b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fe011ea21b0>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe011ea21b0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 248, in scrape_with_refcodes
    response = requests.post(current_url,data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe011ea21b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2023-12-29 23:03:08,219 [ERROR]: An error occurred: HTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f79527be900>: Failed to establish a new connection: [Errno 111] Connection refused'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f79527be900>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f79527be900>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 248, in scrape_with_refcodes
    response = requests.post(current_url,data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f79527be900>: Failed to establish a new connection: [Errno 111] Connection refused'))
2023-12-29 23:05:33,348 [ERROR]: An error occurred: HTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f815c0feba0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f815c0feba0>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f815c0feba0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 248, in scrape_with_refcodes
    response = requests.post(current_url,data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='cis.scc.virginia.gov', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f815c0feba0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2023-12-29 23:26:03,321 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 193, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 45, in scrape_single
    response = requests.get(url,headers=headers,proxies=proxy_dict,allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-29 23:44:31,453 [ERROR]: An error occurred: cannot access local variable 'response' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 193, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 55, in scrape_single
    print(f'Status code: {response.status_code}')
                          ^^^^^^^^
UnboundLocalError: cannot access local variable 'response' where it is not associated with a value
2023-12-30 02:37:21,729 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 187, in scrape_with_refcodes
    response = requests.post(current_url,data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-30 02:41:48,694 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 187, in scrape_with_refcodes
    response = requests.post(current_url,data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-30 02:46:23,785 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 191, in scrape_with_refcodes
    response = requests.post(current_url,data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-30 02:47:09,342 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 191, in scrape_with_refcodes
    response = requests.post(current_url,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-30 02:47:29,789 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 191, in scrape_with_refcodes
    response = requests.post(current_url,data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-30 02:47:32,977 [ERROR]: An error occurred: name 'proxy_list' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 196, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper53.py", line 49, in scrape_single
    for proxy in proxy_list:
                 ^^^^^^^^^^
NameError: name 'proxy_list' is not defined
2023-12-30 20:50:15,289 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 192, in scrape_with_refcodes
    response = requests.get('https://cis.scc.virginia.gov/')
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-30 20:51:31,574 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper52.py", line 261, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper52.py", line 180, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-30 21:06:20,608 [ERROR]: An error occurred: name 'proxy_list' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 196, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper53.py", line 49, in scrape_single
    for proxy in proxy_list:
                 ^^^^^^^^^^
NameError: name 'proxy_list' is not defined
2023-12-30 21:06:33,358 [ERROR]: An error occurred: name 'proxy_list' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 196, in scrape_with_refcodes
    result = self.scrape_single(self.baseurl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper53.py", line 49, in scrape_single
    for proxy in proxy_list:
                 ^^^^^^^^^^
NameError: name 'proxy_list' is not defined
2023-12-30 21:25:04,301 [ERROR]: An error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/packages/six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper52.py", line 245, in scrape_with_refcodes
    response = requests.post('https://apps.ilsos.gov/uccsearch/')
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2023-12-30 21:29:32,620 [ERROR]: An error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/packages/six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper52.py", line 253, in scrape_with_refcodes
    response = requests.post('https://apps.ilsos.gov/uccsearch/',data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2023-12-30 22:00:20,130 [INFO]: Scraping and storing data completed successfully.
2023-12-30 22:01:00,118 [INFO]: Scraping and storing data completed successfully.
2023-12-30 22:04:09,393 [INFO]: Scraping and storing data completed successfully.
2023-12-30 22:04:31,582 [INFO]: Scraping and storing data completed successfully.
2023-12-30 22:04:52,341 [INFO]: Scraping and storing data completed successfully.
2023-12-30 22:46:30,712 [INFO]: Scraping and storing data completed successfully.
2023-12-30 22:46:40,564 [INFO]: Scraping and storing data completed successfully.
2023-12-30 22:47:13,141 [INFO]: Scraping and storing data completed successfully.
2023-12-30 22:47:47,070 [INFO]: Scraping and storing data completed successfully.
2023-12-30 22:48:11,913 [INFO]: Scraping and storing data completed successfully.
2023-12-30 22:48:31,978 [INFO]: Scraping and storing data completed successfully.
2023-12-30 22:49:50,118 [INFO]: Scraping and storing data completed successfully.
2023-12-30 22:50:14,858 [INFO]: Scraping and storing data completed successfully.
2023-12-30 22:58:27,528 [INFO]: Scraping and storing data completed successfully.
2023-12-30 23:06:50,634 [INFO]: Scraping and storing data completed successfully.
2023-12-30 23:07:35,232 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 279, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper53.py", line 175, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-31 01:32:03,102 [ERROR]: An error occurred: 'srccl'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 286, in scrape_with_refcodes
    print(soup_js['srccl'])
          ~~~~~~~^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 'srccl'
2023-12-31 01:32:51,365 [ERROR]: An error occurred: 'srccl'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 286, in scrape_with_refcodes
    print(soup_js['srccl'])
          ~~~~~~~^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/bs4/element.py", line 1573, in __getitem__
    return self.attrs[key]
           ~~~~~~~~~~^^^^^
KeyError: 'srccl'
2023-12-31 01:51:51,079 [ERROR]: An error occurred: TypeError: Cannot read property "document" from undefined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 289, in scrape_with_refcodes
    result = context.call('exec')
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 37, in call
    return self._call(name, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 92, in _call
    return self._eval("{identifier}.apply(this, {args})".format(identifier=identifier, args=args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 78, in _eval
    return self.exec_(code)
           ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 18, in exec_
    return self._exec_(source)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 88, in _exec_
    return self._extract_result(output)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 167, in _extract_result
    raise ProgramError(value)
execjs._exceptions.ProgramError: TypeError: Cannot read property "document" from undefined
2023-12-31 01:54:15,884 [ERROR]: An error occurred: 'Context' object has no attribute 'code'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 290, in scrape_with_refcodes
    print(context.code)
          ^^^^^^^^^^^^
AttributeError: 'Context' object has no attribute 'code'
2023-12-31 01:54:35,797 [ERROR]: An error occurred: 'Context' object has no attribute 'code'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 290, in scrape_with_refcodes
    print(context.code)
          ^^^^^^^^^^^^
AttributeError: 'Context' object has no attribute 'code'
2023-12-31 01:56:25,301 [ERROR]: An error occurred: ReferenceError: "window" is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 290, in scrape_with_refcodes
    print(context.eval('this'))
          ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 27, in eval
    return self._eval(source)
           ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 78, in _eval
    return self.exec_(code)
           ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_abstract_runtime_context.py", line 18, in exec_
    return self._exec_(source)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 88, in _exec_
    return self._extract_result(output)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/execjs/_external_runtime.py", line 167, in _extract_result
    raise ProgramError(value)
execjs._exceptions.ProgramError: ReferenceError: "window" is not defined
2023-12-31 03:10:27,000 [ERROR]: An error occurred: [Errno 7] Argument list too long: 'node'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 292, in scrape_with_refcodes
    result = subprocess.run(["node", response.text], capture_output=True, text=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py", line 548, in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py", line 1026, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py", line 1950, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 7] Argument list too long: 'node'
2023-12-31 03:11:12,876 [ERROR]: An error occurred: [Errno 7] Argument list too long: 'node'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 292, in scrape_with_refcodes
    result = subprocess.run(["node", response.text], capture_output=True, text=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py", line 548, in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py", line 1026, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py", line 1950, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 7] Argument list too long: 'node'
2023-12-31 03:14:23,674 [ERROR]: An error occurred: [Errno 7] Argument list too long: 'node'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 292, in scrape_with_refcodes
    with subprocess.Popen(["node","-e", response.text], stdout=subprocess.PIPE,  text=True) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py", line 1026, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py", line 1950, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 7] Argument list too long: 'node'
2023-12-31 03:14:42,247 [ERROR]: An error occurred: [Errno 7] Argument list too long: 'node'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 292, in scrape_with_refcodes
    with subprocess.Popen(["node","-e", response.text], stdout=subprocess.PIPE,  text=True) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py", line 1026, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py", line 1950, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 7] Argument list too long: 'node'
2023-12-31 03:15:16,967 [ERROR]: An error occurred: [Errno 7] Argument list too long: 'node'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 292, in scrape_with_refcodes
    with subprocess.Popen(["node", response.text], stdout=subprocess.PIPE,  text=True) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py", line 1026, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py", line 1950, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 7] Argument list too long: 'node'
2023-12-31 14:39:19,814 [ERROR]: An error occurred: [Errno 7] Argument list too long: 'node'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 292, in scrape_with_refcodes
    with subprocess.Popen(["node","-e", response.text], stdout=subprocess.PIPE,  text=True) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py", line 1026, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py", line 1950, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 7] Argument list too long: 'node'
2023-12-31 18:03:33,991 [ERROR]: An error occurred: 'NoneType' object has no attribute 'find_all'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper53.py", line 323, in scrape_with_refcodes
    urls = get_page_links(soup)
           ^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper53.py", line 184, in get_page_links
    tr_elements = table.find_all('tr')
                  ^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find_all'
2023-12-31 18:27:53,881 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 192, in scrape_with_refcodes
    response = requests.get(current_url)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-31 18:30:58,215 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 219, in scrape_with_refcodes
    response = requests.post(current_url,data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-31 18:32:35,141 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 220, in scrape_with_refcodes
    response = requests.post('https://cis.scc.virginia.gov/UCCOnlineSearch/UCCSearch',data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-31 18:32:45,652 [ERROR]: An error occurred: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 285, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/packages/six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 285, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 220, in scrape_with_refcodes
    response = requests.post('http://cis.scc.virginia.gov/UCCOnlineSearch/UCCSearch',data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2023-12-31 18:32:54,672 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 220, in scrape_with_refcodes
    response = requests.post('https://cis.scc.virginia.gov/UCCOnlineSearch/UCCSearch',data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-31 18:33:02,981 [ERROR]: An error occurred: Invalid URL 'cis.scc.virginia.gov/UCCOnlineSearch/UCCSearch': No scheme supplied. Perhaps you meant https://cis.scc.virginia.gov/UCCOnlineSearch/UCCSearch?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 220, in scrape_with_refcodes
    response = requests.post('cis.scc.virginia.gov/UCCOnlineSearch/UCCSearch',data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL 'cis.scc.virginia.gov/UCCOnlineSearch/UCCSearch': No scheme supplied. Perhaps you meant https://cis.scc.virginia.gov/UCCOnlineSearch/UCCSearch?
2023-12-31 18:39:37,926 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 220, in scrape_with_refcodes
    response = requests.post('https://cis.scc.virginia.gov/UCCOnlineSearch/UCCSearch',data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-31 18:41:14,104 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 220, in scrape_with_refcodes
    response = requests.post('https://cis.scc.virginia.gov/UCCOnlineSearch/UCCSearch',data=data,headers=headers,verify=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-31 18:43:11,327 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 220, in scrape_with_refcodes
    response = requests.post('https://cis.scc.virginia.gov/UCCOnlineSearch/UCCSearch',data=data,headers=headers,verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-31 18:43:29,277 [ERROR]: An error occurred: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 285, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/packages/six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 285, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 220, in scrape_with_refcodes
    response = requests.post('http://cis.scc.virginia.gov/UCCOnlineSearch/UCCSearch',data=data,headers=headers,verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2023-12-31 18:43:42,645 [ERROR]: An error occurred: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 285, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/packages/six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 285, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 220, in scrape_with_refcodes
    response = requests.post('http://cis.scc.virginia.gov/UCCOnlineSearch/UCCSearch',data=data,headers=headers,verify=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2023-12-31 19:17:05,179 [ERROR]: An error occurred: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 285, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/packages/six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 285, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 224, in scrape_with_refcodes
    response = requests.post('http://cis.scc.virginia.gov/UCCOnlineSearch/UCCSearch',data=json.dumps(data),headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2023-12-31 19:17:19,902 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 224, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2023-12-31 19:17:54,019 [ERROR]: An error occurred: name 'proxy_dict' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 224, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies=proxy_dict)
                                                                                       ^^^^^^^^^^
NameError: name 'proxy_dict' is not defined
2023-12-31 19:18:06,645 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 224, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2024-01-03 00:00:00,263 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 224, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2024-01-03 00:02:38,983 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 225, in scrape_with_refcodes
    response = requests.get(current_url,headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2024-01-03 00:04:29,268 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 225, in scrape_with_refcodes
    response = requests.get(current_url,headers=headers,proxies={'http':'47.243.92.199:3128','https':'47.243.92.199:3128'})
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))
2024-01-03 00:06:03,402 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 224, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies={'https':'47.243.92.199:3128'})
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))
2024-01-03 00:20:03,533 [ERROR]: An error occurred: Invalid URL '/UCCOnlineSearch/LienInformation?FilingNumber=202011130051570&origin=ucclist&isBack=true': No scheme supplied. Perhaps you meant https:///UCCOnlineSearch/LienInformation?FilingNumber=202011130051570&origin=ucclist&isBack=true?
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper54.py", line 250, in scrape_with_refcodes
    batch_results = [item for results in executor.map(self.scrape_single,page_links[i:i+batch_size]) for item in results]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper54.py", line 51, in scrape_single
    response = requests.post(url,headers=headers,allow_redirects=True,verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/root/venv/corey/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL '/UCCOnlineSearch/LienInformation?FilingNumber=202011130051570&origin=ucclist&isBack=true': No scheme supplied. Perhaps you meant https:///UCCOnlineSearch/LienInformation?FilingNumber=202011130051570&origin=ucclist&isBack=true?
2024-01-03 00:30:12,779 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 224, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies={'https':'47.243.92.199:3128'},verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 513, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2024-01-03 00:30:34,771 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 225, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2024-01-03 00:40:46,312 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 224, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies={'https':'47.243.92.199:3128'},verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 513, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2024-01-03 00:41:16,001 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 225, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2024-01-03 00:41:53,508 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 226, in scrape_with_refcodes
    response = requests.get(current_url,headers=headers,proxies={'https':'47.243.92.199:3128'})
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))
2024-01-03 00:43:09,090 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 226, in scrape_with_refcodes
    response = requests.post(current_url,headers=headers,data=data,proxies={'https':'47.243.92.199:3128'},verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 513, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))
2024-01-03 00:44:03,282 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 226, in scrape_with_refcodes
    response = requests.post(current_url,headers=headers,data=data,proxies=proxy_dict,verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2024-01-03 00:44:16,004 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 226, in scrape_with_refcodes
    response = requests.post(current_url,headers=headers,data=data,verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
2024-01-03 00:55:31,671 [ERROR]: An error occurred: Invalid URL '/UCCOnlineSearch/LienInformation?FilingNumber=202301160089066&origin=ucclist&isBack=true': No scheme supplied. Perhaps you meant https:///UCCOnlineSearch/LienInformation?FilingNumber=202301160089066&origin=ucclist&isBack=true?
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 253, in scrape_with_refcodes
    batch_results = [item for results in executor.map(self.scrape_single,page_links[i:i+batch_size]) for item in results]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 51, in scrape_single
    response = requests.post(url,headers=headers,allow_redirects=True,verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL '/UCCOnlineSearch/LienInformation?FilingNumber=202301160089066&origin=ucclist&isBack=true': No scheme supplied. Perhaps you meant https:///UCCOnlineSearch/LienInformation?FilingNumber=202301160089066&origin=ucclist&isBack=true?
2024-01-03 23:28:00,074 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fada13327e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fada13327e0>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fada13327e0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 236, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fada13327e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-03 23:29:17,483 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f257faa2c00>: Failed to establish a new connection: [Errno 111] Connection refused')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f257faa2c00>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f257faa2c00>: Failed to establish a new connection: [Errno 111] Connection refused')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 236, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies={'https':'50.169.23.170'},verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 513, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f257faa2c00>: Failed to establish a new connection: [Errno 111] Connection refused')))
2024-01-03 23:29:45,116 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f4227e0a0c0>: Failed to establish a new connection: [Errno 111] Connection refused')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f4227e0a0c0>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f4227e0a0c0>: Failed to establish a new connection: [Errno 111] Connection refused')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 236, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies={'https':'167.172.238.6:10004'},verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 513, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f4227e0a0c0>: Failed to establish a new connection: [Errno 111] Connection refused')))
2024-01-03 23:30:51,718 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2009d14ce0>: Failed to establish a new connection: [Errno 111] Connection refused')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f2009d14ce0>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2009d14ce0>: Failed to establish a new connection: [Errno 111] Connection refused')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 236, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies={'https':'137.184.18.170:3129'},verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 513, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2009d14ce0>: Failed to establish a new connection: [Errno 111] Connection refused')))
2024-01-03 23:32:14,328 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f24f2572e40>: Failed to establish a new connection: [Errno 111] Connection refused')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f24f2572e40>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f24f2572e40>: Failed to establish a new connection: [Errno 111] Connection refused')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 236, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies={'https':'159.223.183.111'},verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 513, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f24f2572e40>: Failed to establish a new connection: [Errno 111] Connection refused')))
2024-01-03 23:33:29,615 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd5780fa030>: Failed to establish a new connection: [Errno 111] Connection refused')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fd5780fa030>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd5780fa030>: Failed to establish a new connection: [Errno 111] Connection refused')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 236, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies={'https':'32.223.6.94'},verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 513, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd5780fa030>: Failed to establish a new connection: [Errno 111] Connection refused')))
2024-01-03 23:33:53,578 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f960d4c7980>: Failed to establish a new connection: [Errno 111] Connection refused')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f960d4c7980>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f960d4c7980>: Failed to establish a new connection: [Errno 111] Connection refused')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 236, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies={'https':'32.223.6.94'},verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 513, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f960d4c7980>: Failed to establish a new connection: [Errno 111] Connection refused')))
2024-01-03 23:35:04,316 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f07bc262930>: Failed to establish a new connection: [Errno 111] Connection refused'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 787, in connect
    super(socksocket, self).connect(proxy_addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 96, in _new_conn
    conn = socks.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 209, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 199, in create_connection
    sock.connect((remote_host, remote_port))
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 47, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 800, in connect
    raise ProxyConnectionError(msg, error)
socks.ProxyConnectionError: Error connecting to SOCKS5 proxy 165.227.196.37:63379: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 127, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f07bc262930>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f07bc262930>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 236, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies={'https':'socks5://165.227.196.37:63379'},verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f07bc262930>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-03 23:35:55,956 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f5716ba5fd0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 787, in connect
    super(socksocket, self).connect(proxy_addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 96, in _new_conn
    conn = socks.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 209, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 199, in create_connection
    sock.connect((remote_host, remote_port))
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 47, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 800, in connect
    raise ProxyConnectionError(msg, error)
socks.ProxyConnectionError: Error connecting to SOCKS4 proxy 69.36.63.128:1080: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 127, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f5716ba5fd0>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f5716ba5fd0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 236, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies={'https':'socks4://69.36.63.128:1080'},verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: SOCKSHTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f5716ba5fd0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-03 23:36:38,204 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fe28e6f9640>: Failed to establish a new connection: [Errno 111] Connection refused')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fe28e6f9640>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fe28e6f9640>: Failed to establish a new connection: [Errno 111] Connection refused')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 236, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies={'https':'69.36.63.128:1080'},verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 513, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fe28e6f9640>: Failed to establish a new connection: [Errno 111] Connection refused')))
2024-01-03 23:37:42,438 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7efdfa6698b0>: Failed to establish a new connection: [Errno 111] Connection refused')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7efdfa6698b0>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7efdfa6698b0>: Failed to establish a new connection: [Errno 111] Connection refused')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 236, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies={'https':'32.223.6.94:80'},verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 513, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7efdfa6698b0>: Failed to establish a new connection: [Errno 111] Connection refused')))
2024-01-09 02:01:54,724 [ERROR]: An error occurred: 'Scraper57' object has no attribute 'url'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 22, in main
    scraper = Scraper()
              ^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 18, in __init__
    print(f"Scraping: {self.url}")
                       ^^^^^^^^
AttributeError: 'Scraper57' object has no attribute 'url'
2024-01-09 02:35:17,181 [ERROR]: An error occurred: 'Scraper57' object has no attribute 'url'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 22, in main
    scraper = Scraper()
              ^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 18, in __init__
    print(f"Scraping: {self.url}")
                       ^^^^^^^^
AttributeError: 'Scraper57' object has no attribute 'url'
2024-01-09 02:36:00,227 [ERROR]: An error occurred: name 'us_zip_codes' is not defined
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 88, in scrape_with_zipcodes
    zip_codes = us_zip_codes.get_all_codes
                ^^^^^^^^^^^^
NameError: name 'us_zip_codes' is not defined
2024-01-09 03:08:50,457 [ERROR]: An error occurred: module 'database.database_handler_california_zip_codes' has no attribute 'get_all_codes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 89, in scrape_with_zipcodes
    zip_codes = database_handler_zip_codes.get_all_codes()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'database.database_handler_california_zip_codes' has no attribute 'get_all_codes'
2024-01-09 03:10:49,423 [ERROR]: An error occurred: module 'database.database_handler_california_zip_codes' has no attribute 'get_all_codes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 89, in scrape_with_zipcodes
    zip_codes = database_handler_zip_codes.get_all_codes('california_zip_codes')
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'database.database_handler_california_zip_codes' has no attribute 'get_all_codes'
2024-01-09 03:15:59,382 [ERROR]: An error occurred: name 'scraper' is not defined
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 94, in scrape_with_zipcodes
    table_names = [scraper.table_name]
                   ^^^^^^^
NameError: name 'scraper' is not defined. Did you mean: 'Scraper57'?
2024-01-09 03:16:52,513 [ERROR]: An error occurred: 'module' object is not callable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 89, in scrape_with_zipcodes
    db_handler = database_handler_zip_codes(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'module' object is not callable
2024-01-09 03:18:49,226 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
TypeError: 'NoneType' object is not iterable
2024-01-09 03:20:13,574 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
TypeError: 'NoneType' object is not iterable
2024-01-09 03:20:32,566 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
TypeError: 'NoneType' object is not iterable
2024-01-09 03:26:07,478 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
TypeError: 'NoneType' object is not iterable
2024-01-09 03:27:36,439 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 184, in scrape_with_zipcodes
    results = get_page_results(code)
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 156, in get_page_results
    specialty,_,zip_code = table.find('#main > p:nth-child(5) > strong:nth-child(2)').get_text().splitlines() # extract specialty and zip
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2024-01-09 03:38:19,319 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 185, in scrape_with_zipcodes
    results = get_page_results(sp,code)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 157, in get_page_results
    specialty,_,zip_code = table.find('#main > p:nth-child(5) > strong:nth-child(2)').get_text().splitlines() # extract specialty and zip
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2024-01-09 03:39:55,818 [ERROR]: An error occurred: 'NoneType' object has no attribute 'get_text'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 186, in scrape_with_zipcodes
    results = get_page_results(sp,code)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 158, in get_page_results
    specialty,_,zip_code = table.find('#main > p:nth-child(5) > strong:nth-child(2)').get_text().splitlines() # extract specialty and zip
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get_text'
2024-01-09 03:52:23,601 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 184, in scrape_with_zipcodes
    results = get_page_results(sp,code)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 165, in get_page_results
    'specialty':specialty[sp],
                ~~~~~~~~~^^^^
TypeError: list indices must be integers or slices, not str
2024-01-09 03:55:44,254 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 184, in scrape_with_zipcodes
    specialty = specialties[sp] # extract value using dictionary key
                ~~~~~~~~~~~^^^^
TypeError: list indices must be integers or slices, not str
2024-01-09 03:58:23,554 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 185, in scrape_with_zipcodes
    results = get_page_results(sp,code,specialty)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 171, in get_page_results
    row_result.update({'name':row_tds[0].get_text()})
                              ~~~~~~~^^^
IndexError: list index out of range
2024-01-09 03:58:49,621 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 186, in scrape_with_zipcodes
    results = get_page_results(sp,code,specialty)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 171, in get_page_results
    row_result.update({'name':row_tds[0].get_text()})
                              ~~~~~~~^^^
IndexError: list index out of range
2024-01-09 04:00:38,967 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 186, in scrape_with_zipcodes
    results = get_page_results(sp,code,specialty)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 172, in get_page_results
    row_result.update({'name':row_tds[0].get_text()})
                              ~~~~~~~^^^
IndexError: list index out of range
2024-01-09 04:00:55,271 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 37, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_qme_scraper.py", line 33, in store_data
    for data in data_list:
TypeError: 'NoneType' object is not iterable
2024-01-09 04:05:03,423 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 188, in scrape_with_zipcodes
    batch_results = get_page_results(sp,code,specialty)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 175, in get_page_results
    row_result.update({'miles':" ".join(row_tds[3].get_text().split())})
                                        ~~~~~~~^^^
IndexError: list index out of range
2024-01-09 04:08:35,700 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 189, in scrape_with_zipcodes
    batch_results = get_page_results(sp,code,specialty)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 176, in get_page_results
    row_result.update({'miles':" ".join(row_tds[3].get_text().split())})
                                        ~~~~~~~^^^
IndexError: list index out of range
2024-01-10 18:58:16,740 [ERROR]: An error occurred: 1146 (42S02): Table 'scraped_data.california_zip_codes' doesn't exist
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Table 'scraped_data.california_zip_codes' doesn't exist

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/root/projects/corey/src/scraping/scraper57.py", line 99, in scrape_with_zipcodes
    zip_codes = db_handler.get_all_codes('california_zip_codes')
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/database/database_handler_california_zip_codes.py", line 18, in get_all_codes
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1146 (42S02): Table 'scraped_data.california_zip_codes' doesn't exist
2024-01-10 18:59:39,720 [ERROR]: An error occurred: 1146 (42S02): Table 'scraped_data.california_zip_codes' doesn't exist
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Table 'scraped_data.california_zip_codes' doesn't exist

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/root/projects/corey/src/scraping/scraper57.py", line 99, in scrape_with_zipcodes
    zip_codes = db_handler.get_all_codes('california_zip_codes')
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/database/database_handler_california_zip_codes.py", line 18, in get_all_codes
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1146 (42S02): Table 'scraped_data.california_zip_codes' doesn't exist
2024-01-10 19:07:38,325 [INFO]: Scraping and storing data completed successfully.
2024-01-10 19:07:55,959 [INFO]: Scraping and storing data completed successfully.
2024-01-10 19:22:46,528 [ERROR]: An error occurred: None is not in list
Traceback (most recent call last):
  File "/root/projects/corey/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/root/projects/corey/src/scraping/scraper57.py", line 193, in scrape_with_zipcodes
    last_interrupt_zip_index = zip_codes.index(last_interrupt_zipcode)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: None is not in list
2024-01-10 19:23:33,388 [ERROR]: An error occurred: 'dict_keys' object is not subscriptable
Traceback (most recent call last):
  File "/root/projects/corey/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes('90714','MOG'):
  File "/root/projects/corey/src/scraping/scraper57.py", line 206, in scrape_with_zipcodes
    args = [(sp,code,specialties_dict[sp]) for sp in list(specialties_dict.keys()[last_interrrupt_sp_index:])]
                                                          ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'dict_keys' object is not subscriptable
2024-01-14 20:14:58,475 [ERROR]: An error occurred: 1406 (22001): Data too long for column 'secured_party_name' at row 1
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Data too long for column 'secured_party_name' at row 1

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 42, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 34, in store_data
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.DataError: 1406 (22001): Data too long for column 'secured_party_name' at row 1
2024-01-14 21:33:20,361 [ERROR]: An error occurred: 'DatabaseHandler' object has no attribute 'table_name'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 43, in main
    if not db_handler.data_exists(result):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 45, in data_exists
    query = f'SELECT * FROM {self.table_name} WHERE debtor_name = %s AND debtor_address = %s AND secured_party_name = %s AND secured_party_address = %s'
                             ^^^^^^^^^^^^^^^
AttributeError: 'DatabaseHandler' object has no attribute 'table_name'
2024-01-14 21:35:08,039 [ERROR]: An error occurred: string indices must be integers, not 'str'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 44, in main
    db_handler.store_data(scraper.table_name,result)
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 37, in store_data
    ''', (data['debtor_name'], data['debtor_address'], data['secured_party_name'], data['secured_party_address']))
          ~~~~^^^^^^^^^^^^^^^
TypeError: string indices must be integers, not 'str'
2024-01-14 21:36:30,391 [ERROR]: An error occurred: string indices must be integers, not 'str'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 44, in main
    db_handler.store_data(scraper.table_name,result)
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 38, in store_data
    ''', (data['debtor_name'], data['debtor_address'], data['secured_party_name'], data['secured_party_address']))
          ~~~~^^^^^^^^^^^^^^^
TypeError: string indices must be integers, not 'str'
2024-01-14 21:39:02,012 [ERROR]: An error occurred: string indices must be integers, not 'str'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 43, in main
    db_handler.store_data(scraper.table_name,result)
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 38, in store_data
    ''', (data['debtor_name'], data['debtor_address'], data['secured_party_name'], data['secured_party_address']))
          ~~~~^^^^^^^^^^^^^^^
TypeError: string indices must be integers, not 'str'
2024-01-14 21:39:35,480 [ERROR]: An error occurred: string indices must be integers, not 'str'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 43, in main
    db_handler.store_data(scraper.table_name,result)
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 38, in store_data
    ''', (data['debtor_name'], data['debtor_address'], data['secured_party_name'], data['secured_party_address']))
          ~~~~^^^^^^^^^^^^^^^
TypeError: string indices must be integers, not 'str'
2024-01-14 23:00:34,406 [ERROR]: An error occurred: Unread result found
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 43, in main
    db_handler.store_data(scraper.table_name,[result for result in batch_results if not db_handler.data_exists(scraper.table_name,result)]) # adding db check if result is already in db
                                                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/database/database_handler_bizfileonline.py", line 49, in data_exists
    self.cursor.execute(query,(data['debtor_name'],data['debtor_address'],data['secured_party_name'],data['secured_party_address']))
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 303, in execute
    self._cnx.handle_unread_result()
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 961, in handle_unread_result
    raise InternalError("Unread result found")
mysql.connector.errors.InternalError: Unread result found
2024-01-14 23:31:00,695 [ERROR]: An error occurred: [Errno 2] No such file or directory: 'last_code_scraper48.txt'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 38, in main
    with open('last_code_scraper48.txt','r') as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'last_code_scraper48.txt'
2024-01-14 23:33:00,799 [ERROR]: An error occurred: cannot access local variable 'i' where it is not associated with a value
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(start=last_code):
  File "/root/projects/corey/src/scraping/scraper48.py", line 108, in scrape_with_refcodes
    batch_results = [item for sublist in executor.map(scrape_single_thread, range(i, i+batch_size)) if sublist is not None for item in sublist if len(sublist) > 0]
                                                                                  ^
UnboundLocalError: cannot access local variable 'i' where it is not associated with a value
2024-01-15 00:12:18,365 [ERROR]: An error occurred: Simulated error raised
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(start=last_code):
  File "/root/projects/corey/src/scraping/scraper48.py", line 122, in scrape_with_refcodes
    raise ValueError('Simulated error raised')
ValueError: Simulated error raised
2024-01-15 00:14:27,941 [ERROR]: An error occurred: Simulated error raised
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(start=last_code):
  File "/root/projects/corey/src/scraping/scraper48.py", line 122, in scrape_with_refcodes
    raise ValueError('Simulated error raised')
ValueError: Simulated error raised
2024-01-15 00:16:37,977 [ERROR]: An error occurred: Simulated error raised
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(start=last_code):
  File "/root/projects/corey/src/scraping/scraper48.py", line 122, in scrape_with_refcodes
    raise ValueError('Simulated error raised')
ValueError: Simulated error raised
2024-01-15 00:18:47,443 [ERROR]: An error occurred: Simulated error raised
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(start=last_code):
  File "/root/projects/corey/src/scraping/scraper48.py", line 122, in scrape_with_refcodes
    raise ValueError('Simulated error raised')
ValueError: Simulated error raised
2024-01-15 00:20:56,310 [ERROR]: An error occurred: Simulated error raised
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(start=last_code):
  File "/root/projects/corey/src/scraping/scraper48.py", line 122, in scrape_with_refcodes
    raise ValueError('Simulated error raised')
ValueError: Simulated error raised
2024-01-15 00:23:05,605 [ERROR]: An error occurred: Simulated error raised
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(start=last_code):
  File "/root/projects/corey/src/scraping/scraper48.py", line 122, in scrape_with_refcodes
    raise ValueError('Simulated error raised')
ValueError: Simulated error raised
2024-01-15 00:25:15,969 [ERROR]: An error occurred: Simulated error raised
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(start=last_code):
  File "/root/projects/corey/src/scraping/scraper48.py", line 122, in scrape_with_refcodes
    raise ValueError('Simulated error raised')
ValueError: Simulated error raised
2024-01-15 00:27:26,522 [ERROR]: An error occurred: Simulated error raised
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(start=last_code):
  File "/root/projects/corey/src/scraping/scraper48.py", line 122, in scrape_with_refcodes
    raise ValueError('Simulated error raised')
ValueError: Simulated error raised
2024-01-15 00:29:36,776 [ERROR]: An error occurred: Simulated error raised
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(start=last_code):
  File "/root/projects/corey/src/scraping/scraper48.py", line 122, in scrape_with_refcodes
    raise ValueError('Simulated error raised')
ValueError: Simulated error raised
2024-01-15 00:31:45,703 [ERROR]: An error occurred: Simulated error raised
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bizfileonline.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(start=last_code):
  File "/root/projects/corey/src/scraping/scraper48.py", line 122, in scrape_with_refcodes
    #     raise ValueError('Simulated error raised')
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Simulated error raised
2024-01-15 00:35:08,713 [ERROR]: An error occurred: 1054 (42S22): Unknown column 'debtor_name' in 'field list'
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Unknown column 'debtor_name' in 'field list'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 40, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/root/projects/corey/src/database/database_handler_ucc_scraper.py", line 36, in store_data
    self.cursor.execute(f'''
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1054 (42S22): Unknown column 'debtor_name' in 'field list'
2024-01-15 00:40:59,013 [ERROR]: An error occurred: Unread result found
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 41, in main
    db_handler.store_data(scraper.table_name,[result for result in batch_results if not db_handler.data_exists(scraper.table_name,result)]) # adding db check if result is already in db
                                                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/database/database_handler_ucc_scraper.py", line 49, in data_exists
    self.cursor.execute(query,(data['debtor_name'],data['debtor_address'],data['secured_party_name'],data['secured_party_address']))
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 303, in execute
    self._cnx.handle_unread_result()
  File "/root/venv/corey/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 961, in handle_unread_result
    raise InternalError("Unread result found")
mysql.connector.errors.InternalError: Unread result found
2024-01-15 02:42:35,083 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='arc-sos.state.al.us', port=443): Max retries exceeded with url: /cgi/uccdetail.mbr/detail?ucc=11-7231823&page=name (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='arc-sos.state.al.us', port=443): Max retries exceeded with url: /cgi/uccdetail.mbr/detail?ucc=11-7231823&page=name (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/scraping/scraper49.py", line 47, in scrape_single
    response = requests.get(f"{url}", headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='arc-sos.state.al.us', port=443): Max retries exceeded with url: /cgi/uccdetail.mbr/detail?ucc=11-7231823&page=name (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 455, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1046, in _create
    self.do_handshake()
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/ssl.py", line 1317, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='arc-sos.state.al.us', port=443): Max retries exceeded with url: /cgi/uccdetail.mbr/detail?ucc=11-7231823&page=name (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/root/projects/corey/src/scraping/scraper49.py", line 155, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls)]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper49.py", line 51, in scrape_single
    response = requests.get(f"{url}", headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: SOCKSHTTPSConnectionPool(host='arc-sos.state.al.us', port=443): Max retries exceeded with url: /cgi/uccdetail.mbr/detail?ucc=11-7231823&page=name (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))
2024-01-15 23:41:35,688 [ERROR]: An error occurred: 1054 (42S22): Unknown column 'debtor_name' in 'where clause'
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Unknown column 'debtor_name' in 'where clause'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    db_handler.store_data(scraper.table_name,[result for result in batch_results if not db_handler.data_exists(scraper.table_name,result)]) # adding db check if result is already in db
                                                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/database/database_handler_ucc_scraper.py", line 49, in data_exists
    self.cursor.execute(query,(data['debtor_name'],data['debtor_address'],data['secured_party_name'],data['secured_party_address']))
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1054 (42S22): Unknown column 'debtor_name' in 'where clause'
2024-01-16 00:06:49,040 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    print(batch_results)
                         
  File "/home/terence/projects/corey-scraping/src/database/database_handler_ucc_scraper.py", line 49, in data_exists
    self.cursor.execute(query,(data['debtor_name'],data['debtor_address'],data['secured_party_name'],data['secured_party_address']))
                               ~~~~^^^^^^^^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2024-01-16 00:07:37,962 [ERROR]: An error occurred: 'type' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_bizfileonline.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(start=last_code):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper48.py", line 98, in scrape_with_refcodes
    batch_results = [item for sublist in executor.map(scrape_single_thread, range(i, i+batch_size)) if sublist is not None for item in sublist if len(sublist) > 0]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'type' object is not iterable
2024-01-16 00:42:07,998 [ERROR]: An error occurred: 'type' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_bizfileonline.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(start=last_code):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper48.py", line 99, in scrape_with_refcodes
    batch_results = [item for sublist in executor.map(scrape_single_thread, range(i, i+batch_size)) if sublist is not None for item in sublist if len(sublist) > 0]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'type' object is not iterable
2024-01-16 01:04:27,993 [ERROR]: An error occurred: can only concatenate str (not "int") to str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 158, in scrape_with_refcodes
    current_page += 25
TypeError: can only concatenate str (not "int") to str
2024-01-16 01:08:02,558 [ERROR]: An error occurred: can only concatenate str (not "int") to str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 41, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 131, in scrape_with_refcodes
    f.write(str(last_interrupt_char+"_"+current_page))
                ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~
TypeError: can only concatenate str (not "int") to str
2024-01-16 01:09:00,228 [ERROR]: An error occurred: not enough values to unpack (expected 2, got 1)
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 37, in main
    last_char,last_page = str(f.read()).split("_")
    ^^^^^^^^^^^^^^^^^^^
ValueError: not enough values to unpack (expected 2, got 1)
2024-01-16 01:23:17,502 [INFO]: Scraping and storing data completed successfully.
2024-01-16 01:57:41,544 [ERROR]: An error occurred: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 46, in main
    # Store data in the db
                           
  File "/home/terence/projects/corey-scraping/src/database/database_handler_ucc_scraper.py", line 49, in data_exists
    self.cursor.execute(query,(data['debtor_name'],data['debtor_address'],data['secured_party_name'],data['secured_party_address']))
                               ~~~~^^^^^^^^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2024-01-16 02:57:00,985 [ERROR]: An error occurred: Scraper50.scrape_with_refcodes() got an unexpected keyword argument 'starting_page'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Scraper50.scrape_with_refcodes() got an unexpected keyword argument 'starting_page'
2024-01-16 03:32:15,505 [ERROR]: An error occurred: 500 Server Error: Internal Server Error for url: https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=4735b4d0-04b6-4654-86ab-4a6390560df2
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 204, in scrape_with_refcodes
    urls = get_page_links(soup)
                        ^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 58, in scrape_single
    response.raise_for_status()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=4735b4d0-04b6-4654-86ab-4a6390560df2
2024-01-16 22:50:44,642 [ERROR]: An error occurred: 'MOG' is not in list
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes('90714','MOG'):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 194, in scrape_with_zipcodes
    last_interrrupt_sp_index = sp_keys.index(last_interrupt_specialty)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: 'MOG' is not in list
2024-01-16 22:52:51,899 [ERROR]: An error occurred: 90001 is not in list
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 193, in scrape_with_zipcodes
    last_interrupt_zip_index = zip_codes.index(last_interrupt_zipcode)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: 90001 is not in list
2024-01-16 22:57:18,908 [ERROR]: An error occurred: 90001 is not in list
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 194, in scrape_with_zipcodes
    last_interrupt_zip_index = zip_codes.index(last_interrupt_zipcode)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: 90001 is not in list
2024-01-16 22:58:27,419 [ERROR]: An error occurred: HTTPSConnectionPool(host='www.dir.ca.gov', port=443): Max retries exceeded with url: /databases/dwc/qmeN.asp (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f810b679a60>: Failed to establish a new connection: [Errno 111] Connection refused'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f810b679a60>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.dir.ca.gov', port=443): Max retries exceeded with url: /databases/dwc/qmeN.asp (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f810b679a60>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 211, in scrape_with_zipcodes
    page_result = future.result()
                  ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 145, in get_page_results
    response  = requests.post(self.searchurl,headers=headers,data=data)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.dir.ca.gov', port=443): Max retries exceeded with url: /databases/dwc/qmeN.asp (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f810b679a60>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-16 23:38:36,412 [ERROR]: An error occurred: 'tuple' object has no attribute 'append'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 170, in scrape_with_zipcodes
    futures = [executor.submit(get_page_results,*arg.append(search_by_other)) for arg in args]
                                                 ^^^^^^^^^^
AttributeError: 'tuple' object has no attribute 'append'
2024-01-16 23:41:23,056 [ERROR]: An error occurred: Scraper57.scrape_with_zipcodes.<locals>.get_page_results() takes 3 positional arguments but 4 were given
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 173, in scrape_with_zipcodes
    page_result = future.result()
                  ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Scraper57.scrape_with_zipcodes.<locals>.get_page_results() takes 3 positional arguments but 4 were given
2024-01-16 23:42:44,885 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 173, in scrape_with_zipcodes
    page_result = future.result()
                  ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 145, in get_page_results
    row_result.update({'phone':" ".join(row_tds[2].get_text().split()) if not searchby_other else " ".join(row_tds[3].get_text().split())})
                                                                                                           ~~~~~~~^^^
IndexError: list index out of range
2024-01-16 23:46:13,854 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 180, in scrape_with_zipcodes
    page_result = future.result()
                  ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 152, in get_page_results
    row_result.update({'phone':" ".join(row_tds[2].get_text().split()) if not searchby_other else " ".join(row_tds[3].get_text().split())})
                                                                                                           ~~~~~~~^^^
IndexError: list index out of range
2024-01-16 23:47:21,413 [ERROR]: An error occurred: list index out of range
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 180, in scrape_with_zipcodes
    page_result = future.result()
                  ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 152, in get_page_results
    row_result.update({'phone':" ".join(row_tds[2].get_text().split()) if not searchby_other else " ".join(row_tds[3].get_text().split())})
                                                                                                           ~~~~~~~^^^
IndexError: list index out of range
2024-01-16 23:48:12,316 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 37, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_qme_scraper.py", line 33, in store_data
    for data in data_list:
TypeError: 'NoneType' object is not iterable
2024-01-16 23:58:41,572 [ERROR]: An error occurred: exceptions must derive from BaseException
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 145, in get_page_results
    row_tds[3].get_text()
    ~~~~~~~^^^
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 181, in scrape_with_zipcodes
    page_result = future.result()
                  ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 147, in get_page_results
    raise("Index error: empty result found")
TypeError: exceptions must derive from BaseException
2024-01-16 23:59:43,693 [ERROR]: An error occurred: Index error: empty result found.
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 145, in get_page_results
    row_tds[3].get_text()
    ~~~~~~~^^^
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_qme_scraper.py", line 34, in main
    for batch_results in scraper.scrape_with_zipcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 181, in scrape_with_zipcodes
    page_result = future.result()
                  ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper57.py", line 147, in get_page_results
    raise Exception("Index error: empty result found.")
Exception: Index error: empty result found.
<<<<<<< HEAD
2024-01-28 23:56:39,822 [ERROR]: An error occurred: 'Scraper54' object has no attribute 'last_interrupt_txt'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 36, in main
    with open(scraper.last_interrupt_txt,'r') as f:
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper54' object has no attribute 'last_interrupt_txt'
2024-01-29 00:12:11,787 [ERROR]: An error occurred: name 'headers' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():#last_interrupt_char=last_char,starting_page=last_page):
  File "/root/projects/corey/src/scraping/scraper53.py", line 336, in scrape_with_refcodes
    response = requests.post(current_url,data=data,cookies=self.jar,headers=headers)
                                                                            ^^^^^^^
NameError: name 'headers' is not defined. Did you mean: 'headers1'?
2024-01-29 00:31:19,341 [INFO]: Browser listening on: ws://127.0.0.1:43113/devtools/browser/7c3f978a-5632-4a6d-93f6-3688372d97f5
2024-01-29 00:31:44,545 [INFO]: terminate chrome process...
2024-01-29 00:31:44,548 [INFO]: connection closed
2024-01-29 00:32:39,000 [INFO]: Browser listening on: ws://127.0.0.1:40769/devtools/browser/28605364-d421-47c3-a801-ce449ff8fac8
2024-01-29 00:32:49,114 [INFO]: terminate chrome process...
2024-01-29 00:32:49,117 [INFO]: connection closed
2024-01-29 00:33:30,848 [INFO]: Browser listening on: ws://127.0.0.1:52353/devtools/browser/5a622b11-932e-4144-b9a4-a2afb98df29e
2024-01-29 00:33:40,843 [INFO]: terminate chrome process...
2024-01-29 00:33:40,845 [INFO]: connection closed
2024-01-29 00:34:13,036 [INFO]: Browser listening on: ws://127.0.0.1:33551/devtools/browser/0fe0ee80-c128-4438-b35e-d2959856a9b1
2024-01-29 00:34:18,706 [INFO]: terminate chrome process...
2024-01-29 00:34:18,708 [INFO]: connection closed
2024-01-29 00:35:52,879 [INFO]: Browser listening on: ws://127.0.0.1:60809/devtools/browser/8b4f026e-198a-4a39-9f90-3964173dbd9b
2024-01-29 00:36:14,508 [INFO]: terminate chrome process...
2024-01-29 00:36:14,510 [INFO]: connection closed
2024-01-29 00:36:30,069 [INFO]: Browser listening on: ws://127.0.0.1:34793/devtools/browser/86f61b12-acaa-485a-9852-de2e3842f334
2024-01-29 00:36:35,429 [INFO]: terminate chrome process...
2024-01-29 00:36:35,431 [INFO]: connection closed
2024-01-29 00:37:29,035 [INFO]: Browser listening on: ws://127.0.0.1:60067/devtools/browser/6c99b2fb-abc2-4509-8606-c56792f70e1c
2024-01-29 00:58:50,370 [INFO]: Browser listening on: ws://127.0.0.1:59033/devtools/browser/1ffa15df-6e17-4696-8613-8fc160a37e22
2024-01-29 00:59:07,485 [INFO]: terminate chrome process...
2024-01-29 00:59:07,488 [INFO]: connection closed
2024-01-29 00:59:08,213 [INFO]: terminate chrome process...
2024-01-29 01:08:36,606 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='corp.sec.state.ma.us', port=443): Max retries exceeded with url: /CorpWeb/UCCSearch/UCCSearch.aspx (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7fb922abd880>: Failed to establish a new connection: Connection closed unexpectedly'))
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/socks.py", line 809, in connect
    negotiate(self, dest_addr, dest_port)
  File "/root/venv/corey/lib/python3.12/site-packages/socks.py", line 443, in _negotiate_SOCKS5
    self.proxy_peername, self.proxy_sockname = self._SOCKS5_request(
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/socks.py", line 524, in _SOCKS5_request
    resp = self._readall(reader, 3)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/socks.py", line 278, in _readall
=======
2024-01-17 22:42:00,965 [ERROR]: An error occurred: 'Scraper51' object has no attribute 'last_interrupt_txt'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 36, in main
    with open(scraper.last_interrupt_txt,'r') as f:
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper51' object has no attribute 'last_interrupt_txt'
2024-01-17 22:44:00,522 [ERROR]: An error occurred: Scraper51.scrape_with_refcodes() got an unexpected keyword argument 'starting_page'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Scraper51.scrape_with_refcodes() got an unexpected keyword argument 'starting_page'
2024-01-17 22:51:00,243 [ERROR]: An error occurred: '>' not supported between instances of 'str' and 'int'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 239, in scrape_with_refcodes
    if starting_page > 1:
       ^^^^^^^^^^^^^^^^^
TypeError: '>' not supported between instances of 'str' and 'int'
2024-01-17 22:54:00,227 [ERROR]: An error occurred: Scraper51.scrape_with_refcodes.<locals>.num_generator() missing 1 required positional argument: 'starting_page'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 243, in scrape_with_refcodes
    num_gen = num_generator()
              ^^^^^^^^^^^^^^^
TypeError: Scraper51.scrape_with_refcodes.<locals>.num_generator() missing 1 required positional argument: 'starting_page'
2024-01-17 22:57:20,457 [ERROR]: An error occurred: can only concatenate str (not "int") to str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 249, in scrape_with_refcodes
    num = next(num_gen)
          ^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 198, in num_generator
    num += 1
TypeError: can only concatenate str (not "int") to str
2024-01-17 23:47:01,982 [ERROR]: An error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/packages/six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper51.py", line 268, in scrape_with_refcodes
    response = requests.post(current_url,data=data,headers=headers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2024-01-18 04:11:00,237 [ERROR]: An error occurred: 'str' object has no attribute 'items'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 214, in scrape_with_refcodes
    print((data.items() if searchby.items == searchby[0] else data_1.items()))
                           ^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'items'
2024-01-18 04:13:00,936 [ERROR]: An error occurred: 'str' object has no attribute 'items'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 214, in scrape_with_refcodes
    print((data if searchby.items in searchby[0] else data_1))
                   ^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'items'
2024-01-19 22:27:09,791 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 267, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 66, in scrape_single
    data = "_".join(data_list.append(url))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2024-01-19 22:30:11,676 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 268, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 67, in scrape_single
    data = "_".join(data_list.append(url))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2024-01-19 22:34:07,159 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 269, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 68, in scrape_single
    data = "_".join(new_data_list)
           ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2024-01-19 22:49:07,584 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 269, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 66, in scrape_single
    data = "_".join(data_list[:-1].append(url))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2024-01-19 22:50:07,629 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 269, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 66, in scrape_single
    data = "_".join(data_list[:-1].append(url))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2024-01-19 22:53:07,877 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 270, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 67, in scrape_single
    data = "_".join(data_list[:-1].append(url))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2024-01-19 22:57:08,381 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 270, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 67, in scrape_single
    data = "_".join(data_list[:-1].append(url))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2024-01-19 22:59:07,625 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 268, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 67, in scrape_single
    data = "_".join(data_list.append(url))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2024-01-19 23:01:08,518 [ERROR]: An error occurred: can only join an iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 269, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 68, in scrape_single
    data = "_".join(data_list.append(url))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: can only join an iterable
2024-01-22 22:29:11,440 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='arc-sos.state.al.us', port=443): Max retries exceeded with url: /cgi/uccdetail.mbr/detail?ucc=20-7799272&page=name (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f26931fe2d0>: Failed to establish a new connection: Connection closed unexpectedly'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 809, in connect
    negotiate(self, dest_addr, dest_port)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 443, in _negotiate_SOCKS5
    self.proxy_peername, self.proxy_sockname = self._SOCKS5_request(
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 524, in _SOCKS5_request
    resp = self._readall(reader, 3)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 278, in _readall
>>>>>>> 479b4ffb96f82a9b69956901cc0fd6593ed7d2bf
    raise GeneralProxyError("Connection closed unexpectedly")
socks.GeneralProxyError: Connection closed unexpectedly

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
<<<<<<< HEAD
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 96, in _new_conn
    conn = socks.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/socks.py", line 209, in create_connection
    raise err
  File "/root/venv/corey/lib/python3.12/site-packages/socks.py", line 199, in create_connection
    sock.connect((remote_host, remote_port))
  File "/root/venv/corey/lib/python3.12/site-packages/socks.py", line 47, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/socks.py", line 814, in connect
=======
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 96, in _new_conn
    conn = socks.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 209, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 199, in create_connection
    sock.connect((remote_host, remote_port))
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 47, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 814, in connect
>>>>>>> 479b4ffb96f82a9b69956901cc0fd6593ed7d2bf
    raise GeneralProxyError("Socket error", error)
socks.GeneralProxyError: Socket error: Connection closed unexpectedly

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
<<<<<<< HEAD
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 127, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7fb922abd880>: Failed to establish a new connection: Connection closed unexpectedly
=======
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 127, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f26931d5d00>: Failed to establish a new connection: Connection closed unexpectedly
>>>>>>> 479b4ffb96f82a9b69956901cc0fd6593ed7d2bf

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
<<<<<<< HEAD
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='corp.sec.state.ma.us', port=443): Max retries exceeded with url: /CorpWeb/UCCSearch/UCCSearch.aspx (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7fb922abd880>: Failed to establish a new connection: Connection closed unexpectedly'))
=======
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='arc-sos.state.al.us', port=443): Max retries exceeded with url: /cgi/uccdetail.mbr/detail?ucc=20-7799272&page=name (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f26931d5d00>: Failed to establish a new connection: Connection closed unexpectedly'))
>>>>>>> 479b4ffb96f82a9b69956901cc0fd6593ed7d2bf

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
<<<<<<< HEAD
  File "/root/projects/corey/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():#last_interrupt_char=last_char,starting_page=last_page):
  File "/root/projects/corey/src/scraping/scraper53.py", line 322, in scrape_with_refcodes
    _ = self.session.post(current_url,proxies=proxy_dict,allow_redirects=True)#,headers=headers1,allow_redirects=True)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: SOCKSHTTPSConnectionPool(host='corp.sec.state.ma.us', port=443): Max retries exceeded with url: /CorpWeb/UCCSearch/UCCSearch.aspx (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7fb922abd880>: Failed to establish a new connection: Connection closed unexpectedly'))
2024-01-29 01:09:31,017 [INFO]: Browser listening on: ws://127.0.0.1:43497/devtools/browser/eea96777-7d97-4807-aabf-4da8187b2958
2024-01-29 01:09:43,357 [INFO]: terminate chrome process...
2024-01-29 01:09:43,359 [INFO]: connection closed
2024-01-29 01:27:44,768 [INFO]: Browser listening on: ws://127.0.0.1:46329/devtools/browser/5f30a765-a038-4d3a-9124-98cf753b934c
2024-01-29 01:27:54,794 [INFO]: terminate chrome process...
2024-01-29 01:27:54,796 [INFO]: connection closed
2024-01-29 01:41:23,678 [ERROR]: An error occurred: 'Response' object has no attribute 'html'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():#last_interrupt_char=last_char,starting_page=last_page):
  File "/root/projects/corey/src/scraping/scraper53.py", line 336, in scrape_with_refcodes
    response_.html.render()
    ^^^^^^^^^^^^^^
AttributeError: 'Response' object has no attribute 'html'
2024-01-29 01:41:37,815 [INFO]: Browser listening on: ws://127.0.0.1:35321/devtools/browser/9ea41def-cb78-4524-a24b-8dd5681c4ad2
2024-01-29 01:41:46,901 [INFO]: terminate chrome process...
2024-01-29 01:41:46,904 [INFO]: connection closed
2024-01-29 01:41:47,957 [INFO]: terminate chrome process...
2024-01-29 01:49:05,734 [INFO]: terminate chrome process...
2024-01-29 01:49:05,735 [INFO]: terminate chrome process...
2024-01-29 01:49:05,737 [INFO]: terminate chrome process...
2024-01-29 01:49:05,738 [INFO]: terminate chrome process...
2024-01-29 01:49:05,738 [INFO]: terminate chrome process...
2024-01-29 01:49:05,738 [INFO]: terminate chrome process...
2024-01-29 01:49:05,740 [INFO]: terminate chrome process...
2024-01-29 01:49:05,745 [INFO]: terminate chrome process...
2024-01-29 01:49:05,746 [ERROR]: An error occurred: cannot access local variable 'response' where it is not associated with a value
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():#last_interrupt_char=last_char,starting_page=last_page):
  File "/root/projects/corey/src/scraping/scraper53.py", line 364, in scrape_with_refcodes
    #     os.remove(temp_file_path)
                       ^^^^^^^^
UnboundLocalError: cannot access local variable 'response' where it is not associated with a value
2024-01-29 01:49:05,754 [INFO]: terminate chrome process...
2024-01-29 01:49:05,768 [INFO]: terminate chrome process...
2024-01-29 01:49:05,800 [ERROR]: connection unexpectedly closed
2024-01-29 01:49:05,802 [ERROR]: Task exception was never retrieved
future: <Task finished name='Task-42' coro=<Connection._async_send() done, defined at /root/venv/corey/lib/python3.12/site-packages/pyppeteer/connection.py:69> exception=InvalidStateError('invalid state')>
Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/websockets/legacy/protocol.py", line 968, in transfer_data
    message = await self.read_message()
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/websockets/legacy/protocol.py", line 1038, in read_message
    frame = await self.read_data_frame(max_size=self.max_size)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/websockets/legacy/protocol.py", line 1113, in read_data_frame
    frame = await self.read_frame(max_size)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/websockets/legacy/protocol.py", line 1170, in read_frame
    frame = await Frame.read(
            ^^^^^^^^^^^^^^^^^
  File "/root/venv/corey/lib/python3.12/site-packages/websockets/legacy/framing.py", line 69, in read
    data = await reader(2)
           ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/asyncio/streams.py", line 732, in readexactly
    raise exceptions.IncompleteReadError(incomplete, n)
asyncio.exceptions.IncompleteReadError: 0 bytes read on a total of 2 expected bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/venv/corey/lib/python3.12/site-packages/pyppeteer/connection.py", line 73, in _async_send
    await self.connection.send(msg)
  File "/root/venv/corey/lib/python3.12/site-packages/websockets/legacy/protocol.py", line 647, in send
    await self.write_frame(True, opcode, data)
  File "/root/venv/corey/lib/python3.12/site-packages/websockets/legacy/protocol.py", line 1214, in write_frame
    await self.drain()
  File "/root/venv/corey/lib/python3.12/site-packages/websockets/legacy/protocol.py", line 1203, in drain
    await self.ensure_open()
  File "/root/venv/corey/lib/python3.12/site-packages/websockets/legacy/protocol.py", line 944, in ensure_open
    raise self.connection_closed_exc()
websockets.exceptions.ConnectionClosedError: no close frame received or sent
=======
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 48, in scrape_single
    response = requests.get(f"{url}", headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: SOCKSHTTPSConnectionPool(host='arc-sos.state.al.us', port=443): Max retries exceeded with url: /cgi/uccdetail.mbr/detail?ucc=20-7799272&page=name (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f26931d5d00>: Failed to establish a new connection: Connection closed unexpectedly'))
>>>>>>> 479b4ffb96f82a9b69956901cc0fd6593ed7d2bf

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
<<<<<<< HEAD
  File "/root/venv/corey/lib/python3.12/site-packages/pyppeteer/connection.py", line 79, in _async_send
    await self.dispose()
  File "/root/venv/corey/lib/python3.12/site-packages/pyppeteer/connection.py", line 170, in dispose
    await self._on_close()
  File "/root/venv/corey/lib/python3.12/site-packages/pyppeteer/connection.py", line 151, in _on_close
    cb.set_exception(_rewriteError(
asyncio.exceptions.InvalidStateError: invalid state
2024-01-29 01:49:05,844 [INFO]: connection closed
2024-01-29 01:49:05,845 [INFO]: terminate chrome process...
2024-01-29 01:49:05,846 [ERROR]: An error occurred: [Errno 5] Input/output error
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():#last_interrupt_char=last_char,starting_page=last_page):
  File "/root/projects/corey/src/scraping/scraper53.py", line 338, in scrape_with_refcodes
    time.sleep(100)
  File "/root/projects/corey/src/scraping/scraper53.py", line 26, in renew_cookies
    def renew_cookies(self,response):
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error
2024-01-29 01:49:05,852 [INFO]: terminate chrome process...
2024-01-29 01:49:05,802 [ERROR]: An error occurred: [Errno 5] Input/output error
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():#last_interrupt_char=last_char,starting_page=last_page):
  File "/root/projects/corey/src/scraping/scraper53.py", line 338, in scrape_with_refcodes
    time.sleep(100)
  File "/root/projects/corey/src/scraping/scraper53.py", line 26, in renew_cookies
    def renew_cookies(self,response):
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error
2024-01-29 01:49:05,819 [ERROR]: An error occurred: [Errno 5] Input/output error
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():#last_interrupt_char=last_char,starting_page=last_page):
  File "/root/projects/corey/src/scraping/scraper53.py", line 338, in scrape_with_refcodes
    time.sleep(100)
  File "/root/projects/corey/src/scraping/scraper53.py", line 26, in renew_cookies
    def renew_cookies(self,response):
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error
2024-01-29 01:49:05,823 [ERROR]: An error occurred: [Errno 5] Input/output error
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():#last_interrupt_char=last_char,starting_page=last_page):
  File "/root/projects/corey/src/scraping/scraper53.py", line 338, in scrape_with_refcodes
    time.sleep(100)
  File "/root/projects/corey/src/scraping/scraper53.py", line 26, in renew_cookies
    def renew_cookies(self,response):
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error
2024-01-29 01:49:05,821 [ERROR]: An error occurred: [Errno 5] Input/output error
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():#last_interrupt_char=last_char,starting_page=last_page):
  File "/root/projects/corey/src/scraping/scraper53.py", line 338, in scrape_with_refcodes
    time.sleep(100)
  File "/root/projects/corey/src/scraping/scraper53.py", line 26, in renew_cookies
    def renew_cookies(self,response):
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error
2024-01-29 01:49:05,820 [ERROR]: An error occurred: [Errno 5] Input/output error
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():#last_interrupt_char=last_char,starting_page=last_page):
  File "/root/projects/corey/src/scraping/scraper53.py", line 338, in scrape_with_refcodes
    time.sleep(100)
  File "/root/projects/corey/src/scraping/scraper53.py", line 26, in renew_cookies
    def renew_cookies(self,response):
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error
2024-01-29 01:49:05,821 [ERROR]: An error occurred: [Errno 5] Input/output error
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():#last_interrupt_char=last_char,starting_page=last_page):
  File "/root/projects/corey/src/scraping/scraper53.py", line 338, in scrape_with_refcodes
    time.sleep(100)
  File "/root/projects/corey/src/scraping/scraper53.py", line 26, in renew_cookies
    def renew_cookies(self,response):
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error
2024-01-29 16:37:16,758 [INFO]: Browser listening on: ws://127.0.0.1:53255/devtools/browser/c8f0a5a5-62ba-4f2c-98b1-ec2df8e24aa5
2024-01-29 16:37:22,516 [INFO]: terminate chrome process...
2024-01-29 16:37:22,519 [INFO]: connection closed
2024-01-29 17:27:59,137 [ERROR]: An error occurred: name 'response_' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():#last_interrupt_char=last_char,starting_page=last_page):
  File "/root/projects/corey/src/scraping/scraper53.py", line 336, in scrape_with_refcodes
    response_.html.render()
    ^^^^^^^^^
NameError: name 'response_' is not defined
2024-01-29 17:28:14,098 [INFO]: Browser listening on: ws://127.0.0.1:46457/devtools/browser/8d773076-6c07-4760-a064-0fa7e3b299a0
2024-01-29 17:28:19,028 [INFO]: terminate chrome process...
2024-01-29 17:28:19,030 [INFO]: connection closed
2024-01-29 17:29:23,500 [INFO]: Browser listening on: ws://127.0.0.1:39265/devtools/browser/d4245fee-07b1-4247-97b8-c5d372cbcbb0
2024-01-29 17:34:19,707 [INFO]: Browser listening on: ws://127.0.0.1:38747/devtools/browser/a49bba4a-7d9c-4587-a533-794617410745
2024-01-29 17:34:37,481 [INFO]: Browser listening on: ws://127.0.0.1:33845/devtools/browser/034b4491-b10f-4c56-a344-ac25d2c80d3f
2024-01-29 17:35:23,717 [INFO]: Browser listening on: ws://127.0.0.1:45207/devtools/browser/525fc342-cac2-4cac-8e69-f65eeafe2641
2024-01-29 17:40:05,937 [INFO]: Browser listening on: ws://127.0.0.1:47549/devtools/browser/cac43e22-4c94-4cdc-8e4b-fd4a247fc856
=======
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 96, in _new_conn
    conn = socks.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 209, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 199, in create_connection
    sock.connect((remote_host, remote_port))
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 47, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 814, in connect
    raise GeneralProxyError("Socket error", error)
socks.GeneralProxyError: Socket error: Connection closed unexpectedly

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 127, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f26931fe2d0>: Failed to establish a new connection: Connection closed unexpectedly

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='arc-sos.state.al.us', port=443): Max retries exceeded with url: /cgi/uccdetail.mbr/detail?ucc=20-7799272&page=name (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f26931fe2d0>: Failed to establish a new connection: Connection closed unexpectedly'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 161, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls) if results is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper49.py", line 52, in scrape_single
    response = requests.get(f"{url}", headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: SOCKSHTTPSConnectionPool(host='arc-sos.state.al.us', port=443): Max retries exceeded with url: /cgi/uccdetail.mbr/detail?ucc=20-7799272&page=name (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f26931fe2d0>: Failed to establish a new connection: Connection closed unexpectedly'))
2024-01-23 22:19:58,207 [ERROR]: An error occurred: 'Scraper50' object has no attribute 'load_state'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 43, in main
    state = scraper.load_state()
            ^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper50' object has no attribute 'load_state'
2024-01-23 22:23:06,073 [ERROR]: An error occurred: the JSON object must be str, bytes or bytearray, not TextIOWrapper
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 43, in main
    state = scraper.load_state()
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 38, in load_state
    return json.loads(file)
           ^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/__init__.py", line 339, in loads
    raise TypeError(f'the JSON object must be str, bytes or bytearray, '
TypeError: the JSON object must be str, bytes or bytearray, not TextIOWrapper
2024-01-23 22:30:56,443 [ERROR]: An error occurred: 'dict' object cannot be interpreted as an integer
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 47, in main
    for batch_results in scraper.scrape_with_refcodes(state):#last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 272, in scrape_with_refcodes
    for i in range(last_interrupt_debtor_index,len(urls),batch_size):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'dict' object cannot be interpreted as an integer
2024-01-24 00:03:37,804 [ERROR]: An error occurred: HTTPSConnectionPool(host='business.sos.ms.gov', port=443): Max retries exceeded with url: /star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=41f44b43-ddea-4005-a60e-4a5a93cd456b (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ffa1f42e120>: Failed to establish a new connection: [Errno 104] Connection reset by peer'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7ffa1f42e120>: Failed to establish a new connection: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='business.sos.ms.gov', port=443): Max retries exceeded with url: /star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=41f44b43-ddea-4005-a60e-4a5a93cd456b (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ffa1f42e120>: Failed to establish a new connection: [Errno 104] Connection reset by peer'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 47, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 273, in scrape_with_refcodes
    batch_results = [results for results in executor.map(self.scrape_single,urls[i:i+batch_size])]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 70, in scrape_single
    response = requests.get(url)
               ^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='business.sos.ms.gov', port=443): Max retries exceeded with url: /star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=41f44b43-ddea-4005-a60e-4a5a93cd456b (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ffa1f42e120>: Failed to establish a new connection: [Errno 104] Connection reset by peer'))
2024-01-24 00:29:42,978 [ERROR]: An error occurred: 'https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=3a0683d3-96e6-4b36-b796-ffd0fb179e85' is not in list
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 47, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper50.py", line 269, in scrape_with_refcodes
    last_interrupt_debtor_index = urls.index(last_interrupt_debtor) if last_interrupt_debtor is not None else 0
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: 'https://business.sos.ms.gov/star/portal/ucc/page/uccSearch-filingchain/portal.aspx?Id=3a0683d3-96e6-4b36-b796-ffd0fb179e85' is not in list
2024-01-24 00:50:41,498 [INFO]: Scraping and storing data completed successfully.
2024-01-26 23:23:27,680 [ERROR]: An error occurred: 'Scraper52' object has no attribute 'last_interrupt_txt'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 36, in main
    with open(scraper.last_interrupt_txt,'r') as f:
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper52' object has no attribute 'last_interrupt_txt'
2024-01-27 00:01:27,650 [ERROR]: An error occurred: 'Scraper52' object has no attribute 'last_interrupt_txt'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 36, in main
    with open(scraper.last_interrupt_txt,'r') as f:
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper52' object has no attribute 'last_interrupt_txt'
2024-01-27 00:02:02,075 [ERROR]: An error occurred: 'Scraper52' object has no attribute 'last_interrupt_txt'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 36, in main
    with open(scraper.last_interrupt_txt,'r') as f:
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper52' object has no attribute 'last_interrupt_txt'
2024-01-27 00:03:49,958 [ERROR]: An error occurred: 'Scraper52' object has no attribute 'last_interrupt_txt'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 36, in main
    with open(scraper.last_interrupt_txt,'r') as f:
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper52' object has no attribute 'last_interrupt_txt'. Did you mean: 'last_interrrupt_txt'?
2024-01-27 00:04:03,949 [ERROR]: An error occurred: Scraper52.scrape_with_refcodes() got an unexpected keyword argument 'starting_page'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 47, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Scraper52.scrape_with_refcodes() got an unexpected keyword argument 'starting_page'
2024-01-27 00:06:30,137 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='apps.ilsos.gov', port=443): Max retries exceeded with url: /uccsearch/ (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f6ffae5d5e0>: Failed to establish a new connection: Connection closed unexpectedly'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 809, in connect
    negotiate(self, dest_addr, dest_port)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 443, in _negotiate_SOCKS5
    self.proxy_peername, self.proxy_sockname = self._SOCKS5_request(
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 524, in _SOCKS5_request
    resp = self._readall(reader, 3)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 278, in _readall
    raise GeneralProxyError("Connection closed unexpectedly")
socks.GeneralProxyError: Connection closed unexpectedly

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 96, in _new_conn
    conn = socks.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 209, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 199, in create_connection
    sock.connect((remote_host, remote_port))
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 47, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 814, in connect
    raise GeneralProxyError("Socket error", error)
socks.GeneralProxyError: Socket error: Connection closed unexpectedly

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 127, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f6ffae5d5e0>: Failed to establish a new connection: Connection closed unexpectedly

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='apps.ilsos.gov', port=443): Max retries exceeded with url: /uccsearch/ (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f6ffae5d5e0>: Failed to establish a new connection: Connection closed unexpectedly'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 47, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper52.py", line 254, in scrape_with_refcodes
    response = requests.get('https://apps.ilsos.gov/uccsearch/',headers=headers,proxies=proxy_dict)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: SOCKSHTTPSConnectionPool(host='apps.ilsos.gov', port=443): Max retries exceeded with url: /uccsearch/ (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7f6ffae5d5e0>: Failed to establish a new connection: Connection closed unexpectedly'))
2024-01-27 00:07:16,797 [ERROR]: An error occurred: Scraper53.scrape_with_refcodes() got an unexpected keyword argument 'starting_page'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 47, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Scraper53.scrape_with_refcodes() got an unexpected keyword argument 'starting_page'
2024-01-27 00:07:31,349 [ERROR]: An error occurred: SOCKSHTTPSConnectionPool(host='corp.sec.state.ma.us', port=443): Max retries exceeded with url: /CorpWeb/UCCSearch/UCCSearch.aspx (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7efd64bc2c90>: Failed to establish a new connection: Connection closed unexpectedly'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 809, in connect
    negotiate(self, dest_addr, dest_port)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 443, in _negotiate_SOCKS5
    self.proxy_peername, self.proxy_sockname = self._SOCKS5_request(
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 524, in _SOCKS5_request
    resp = self._readall(reader, 3)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 278, in _readall
    raise GeneralProxyError("Connection closed unexpectedly")
socks.GeneralProxyError: Connection closed unexpectedly

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 96, in _new_conn
    conn = socks.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 209, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 199, in create_connection
    sock.connect((remote_host, remote_port))
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 47, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/socks.py", line 814, in connect
    raise GeneralProxyError("Socket error", error)
socks.GeneralProxyError: Socket error: Connection closed unexpectedly

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/contrib/socks.py", line 127, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7efd64bc2c90>: Failed to establish a new connection: Connection closed unexpectedly

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: SOCKSHTTPSConnectionPool(host='corp.sec.state.ma.us', port=443): Max retries exceeded with url: /CorpWeb/UCCSearch/UCCSearch.aspx (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7efd64bc2c90>: Failed to establish a new connection: Connection closed unexpectedly'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 47, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper53.py", line 298, in scrape_with_refcodes
    response = requests.get(current_url,headers=headers1,proxies=proxy_dict,allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: SOCKSHTTPSConnectionPool(host='corp.sec.state.ma.us', port=443): Max retries exceeded with url: /CorpWeb/UCCSearch/UCCSearch.aspx (Caused by NewConnectionError('<urllib3.contrib.socks.SOCKSHTTPSConnection object at 0x7efd64bc2c90>: Failed to establish a new connection: Connection closed unexpectedly'))
2024-01-27 00:10:09,069 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f45cf00ddc0>: Failed to establish a new connection: [Errno 111] Connection refused')))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 712, in urlopen
    self._prepare_proxy(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1012, in _prepare_proxy
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f45cf00ddc0>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f45cf00ddc0>: Failed to establish a new connection: [Errno 111] Connection refused')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 47, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 237, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,proxies={'https':'32.223.6.94:80'},verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 513, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f45cf00ddc0>: Failed to establish a new connection: [Errno 111] Connection refused')))
2024-01-27 00:11:01,901 [ERROR]: An error occurred: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f307b986f30>: Failed to establish a new connection: [Errno 111] Connection refused'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f307b986f30>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f307b986f30>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_ucc_scraper.py", line 47, in main
    for batch_results in scraper.scrape_with_refcodes(last_interrupt_char=last_char,starting_page=last_page):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper54.py", line 238, in scrape_with_refcodes
    response = requests.post(current_url,data=json.dumps(data),headers=headers,verify=False)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='cis.scc.virginia.gov', port=443): Max retries exceeded with url: /UCCOnlineSearch/UCCSearch (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f307b986f30>: Failed to establish a new connection: [Errno 111] Connection refused'))
>>>>>>> 479b4ffb96f82a9b69956901cc0fd6593ed7d2bf
