2023-11-26 11:23:47,891 [ERROR]: An error occurred: name 'Scraper' is not defined
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 17, in main
    scraper = Scraper()
              ^^^^^^^
NameError: name 'Scraper' is not defined
2023-11-26 11:24:11,613 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO_INCR' at line 1
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO_INCR' at line 1

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 20, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 16, in __init__
    self.cursor.execute(f'''
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO_INCR' at line 1
2023-11-26 11:26:30,152 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO INCR' at line 1
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO INCR' at line 1

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 20, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 16, in __init__
    self.cursor.execute(f'''
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO INCR' at line 1
2023-11-26 11:26:41,597 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTOINCRE' at line 1
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTOINCRE' at line 1

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 20, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 16, in __init__
    self.cursor.execute(f'''
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTOINCRE' at line 1
2023-11-26 11:27:11,670 [ERROR]: An error occurred: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO_INCR' at line 1
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO_INCR' at line 1

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 20, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 16, in __init__
    self.cursor.execute(f'''
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '['scraper2_info'] (
                                id INT PRIMARY KEY AUTO_INCR' at line 1
2023-11-26 20:15:16,582 [ERROR]: An error occurred: 'Scraper4' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 30, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper4' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-11-26 20:15:48,059 [INFO]: Scraping and storing data completed successfully.
2023-11-26 20:16:57,169 [INFO]: Scraping and storing data completed successfully.
2023-11-26 20:26:18,470 [ERROR]: An error occurred: cannot access local variable 'continue_to_next_name' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 33, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper6.py", line 116, in scrape_with_names
    if continue_to_next_name:
       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'continue_to_next_name' where it is not associated with a value
2023-11-27 22:08:00,632 [ERROR]: An error occurred: 'Scraper8' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 34, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper8' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-11-27 22:08:55,609 [ERROR]: An error occurred: name 'HTMLSession' is not defined
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 139, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 130, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 52, in scrape_single
    session = HTMLSession()
              ^^^^^^^^^^^
NameError: name 'HTMLSession' is not defined
2023-11-27 22:26:22,555 [ERROR]: An error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/packages/six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 139, in scrape_with_refcodes
    for code in refcodes:
        ^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 130, in scrape_single_thread
    time.sleep(0.3)
             ^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 53, in scrape_single
    # session = HTMLSession()
               ^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2023-11-27 22:54:17,317 [ERROR]: An error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/packages/six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 462, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1411, in getresponse
    response.begin()
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 324, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 293, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 35, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 140, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 131, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 54, in scrape_single
    response = self.session.post(f'{url}?RefCode={data['refCode']}',headers=headers,allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2023-11-27 23:37:57,325 [ERROR]: An error occurred: 'Scraper9' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper9' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-11-28 00:03:09,445 [ERROR]: An error occurred: 'Scraper3' object has no attribute 'scrape_with_names'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_names():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper3' object has no attribute 'scrape_with_names'. Did you mean: 'scrape_with_refcodes'?
2023-11-28 00:03:46,521 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 140, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 131, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 101, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-11-30 17:55:48,613 [ERROR]: An error occurred: string indices must be integers, not 'str'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 125, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 115, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 70, in scrape_single
    response = requests.post(url, headers=headers,cookies=self.extracted_cookies, data=data, proxies=proxies, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 473, in prepare_request
    cookies = cookiejar_from_dict(cookies)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/cookies.py", line 537, in cookiejar_from_dict
    cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))
                                             ~~~~~~~~~~~^^^^^^
TypeError: string indices must be integers, not 'str'
2023-11-30 18:45:51,017 [ERROR]: An error occurred: 'Scraper4' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper4' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-11-30 18:46:02,414 [INFO]: Scraping and storing data completed successfully.
2023-11-30 19:56:35,181 [INFO]: Scraping and storing data completed successfully.
2023-11-30 20:01:05,199 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 42, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 34, in store_data
    for data in data_list:
TypeError: 'NoneType' object is not iterable
2023-12-01 18:58:58,557 [ERROR]: An error occurred: 'Scraper2' object has no attribute 'scrape_with_names'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 46, in main
    for batch_results in scraper.scrape_with_names():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper2' object has no attribute 'scrape_with_names'. Did you mean: 'scrape_with_refcodes'?
2023-12-01 18:59:37,811 [ERROR]: An error occurred: 'Scraper2' object has no attribute 'scrape'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 47, in main
    for batch_results in scraper.scrape():
                         ^^^^^^^^^^^^^^
AttributeError: 'Scraper2' object has no attribute 'scrape'. Did you mean: 'scraper'?
2023-12-01 18:59:51,488 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 47, in main
    for batch_results in scraper.scrape():
TypeError: 'NoneType' object is not iterable
2023-12-01 19:15:14,397 [ERROR]: An error occurred: 'NoneType' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 47, in main
    for batch_results in scraper.scrape():
TypeError: 'NoneType' object is not iterable
2023-12-01 19:39:48,426 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 51, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 38, in store_data
    ''', (data['first_name'], data['last_name'], data['address'], data['city'], data['state'], data['zip_code']))
          ~~~~^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-01 19:55:12,416 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 51, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 38, in store_data
    ''', (data['first_name'], data['last_name'], data['address'], data['city'], data['state'], data['zip_code']))
          ~~~~^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-01 20:00:23,745 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 51, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 38, in store_data
    ''', (data['first_name'], data['last_name'], data['address'], data['city'], data['state'], data['zip_code']))
          ~~~~^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-01 20:29:55,737 [ERROR]: An error occurred: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 51, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_local.py", line 38, in store_data
    ''', (data['first_name'], data['last_name'], data['address'], data['city'], data['state'], data['zip_code']))
          ~~~~^^^^^^^^^^^^^^
TypeError: list indices must be integers or slices, not str
2023-12-01 20:41:30,801 [ERROR]: An error occurred: 'method' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 33, in main
    scraper = Scraper()
              ^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 24, in __init__
    self.renew_cookies()
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 37, in renew_cookies
    for name,value in response.cookies.items:
TypeError: 'method' object is not iterable
2023-12-01 20:42:22,293 [ERROR]: An error occurred: 'method' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 33, in main
    scraper = Scraper()
              ^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 24, in __init__
    self.renew_cookies()
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 38, in renew_cookies
    for name,value in response.cookies.items:
TypeError: 'method' object is not iterable
2023-12-01 20:42:57,639 [ERROR]: An error occurred: 'method' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 33, in main
    scraper = Scraper()
              ^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 24, in __init__
    self.renew_cookies()
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 38, in renew_cookies
    for name,value in response.cookies.items:
TypeError: 'method' object is not iterable
2023-12-01 20:44:06,853 [ERROR]: An error occurred: 'method' object is not iterable
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 33, in main
    scraper = Scraper()
              ^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 25, in __init__
    self.renew_cookies()
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 39, in renew_cookies
    for name,value in response.cookies.items:
TypeError: 'method' object is not iterable
2023-12-01 20:44:32,330 [ERROR]: An error occurred: object of type 'generator' has no len()
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 51, in main
    if len(batch_results) == 0:
       ^^^^^^^^^^^^^^^^^^
TypeError: object of type 'generator' has no len()
2023-12-04 08:24:27,315 [ERROR]: An error occurred: 'Scraper29' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper29' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-12-04 09:00:09,677 [ERROR]: An error occurred: 'Scraper2' object has no attribute 'scrape_with_names'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_names():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper2' object has no attribute 'scrape_with_names'. Did you mean: 'scrape_with_refcodes'?
2023-12-04 09:01:05,721 [ERROR]: An error occurred: Invalid URL 'xmydebt.com?RefCode=RD0000011': No scheme supplied. Perhaps you meant https://xmydebt.com?RefCode=RD0000011?
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 151, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 142, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 64, in scrape_single
    response = self.session.get(f'{url}?RefCode={data['refCode']}',allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 575, in request
    prep = self.prepare_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 486, in prepare_request
    p.prepare(
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 368, in prepare
    self.prepare_url(url, params)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 439, in prepare_url
    raise MissingSchema(
requests.exceptions.MissingSchema: Invalid URL 'xmydebt.com?RefCode=RD0000011': No scheme supplied. Perhaps you meant https://xmydebt.com?RefCode=RD0000011?
2023-12-04 09:01:32,078 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 151, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 142, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 112, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:01:37,370 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 151, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 142, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 112, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:02:08,291 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 151, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 142, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 112, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:02:22,691 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 151, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 142, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 112, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:18:39,518 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 166, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 157, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 127, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:31:58,598 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f713d3f6cc0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f713d3f6cc0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f713d3f6cc0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 168, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 159, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 80, in scrape_single
    response = self.session.get(f'https://{url}',headers=headers_caspio, allow_redirects=True) # '?RefCode={data['refCode']}
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f713d3f6cc0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 09:33:21,259 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc2bfb9f800>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fc2bfb9f800>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc2bfb9f800>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 168, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 159, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 80, in scrape_single
    response = self.session.get(f'https://{url}?RefCode={data['refCode']}',headers=headers_caspio, allow_redirects=True) # ?RefCode={data['refCode']}
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc2bfb9f800>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 09:42:56,561 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f35d57a0e00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f35d57a0e00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f35d57a0e00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 189, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 180, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 101, in scrape_single
    response = self.session.post(f'https://{url}?RefCode={data['refCode']}',headers=headers_caspio, allow_redirects=True) # ?RefCode={data['refCode']}
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f35d57a0e00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 09:48:06,422 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fccc5a045f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fccc5a045f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fccc5a045f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 189, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 180, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 101, in scrape_single
    response = self.session.get(f'https://{url}?RefCode={data['refCode']}', allow_redirects=True) # ?RefCode={data['refCode']}
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fccc5a045f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 09:49:35,728 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 192, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 183, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 153, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:50:05,438 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 192, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 183, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 153, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:50:15,477 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 192, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 183, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 153, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:50:19,120 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 192, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 183, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 153, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:51:58,566 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 195, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 186, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 156, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:53:51,176 [ERROR]: An error occurred: 'NoneType' object has no attribute 'attrs'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 196, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 187, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 157, in scrape_single
    zip_code_el = soup.find(attrs={"name":"wpforms[fields][4][postal]"})['value'] if 'value' in soup.find(attrs={"name":"wpforms[fields][4][postal]"}).attrs else None
                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'attrs'
2023-12-04 09:54:54,790 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ff0ce47d520>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7ff0ce47d520>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ff0ce47d520>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 198, in scrape_with_refcodes
    #     for i in range(0,len(refcodes),batch_size):
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 189, in scrape_single_thread
    if result is not None:
             ^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 108, in scrape_single
    response = self.session.get(f'https://{url}?RefCode={data['refCode']}', cookies=self.jar, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ff0ce47d520>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 10:00:50,859 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f84f5bfee10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f84f5bfee10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f84f5bfee10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 198, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 189, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 110, in scrape_single
    response = self.session.get(f'https://{url}?RefCode={data['refCode']}', headers=headers_caspio, cookies=self.jar, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f84f5bfee10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 10:01:37,445 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc1e6b3aa50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fc1e6b3aa50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc1e6b3aa50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 198, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 189, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 110, in scrape_single
    response = self.session.post(f'https://{url}?RefCode={data['refCode']}', headers=headers_caspio, cookies=self.jar, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc1e6b3aa50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 10:25:24,234 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdf18fa3950>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fdf18fa3950>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdf18fa3950>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 147, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 138, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 59, in scrape_single
    response = self.session.get(f'https://{url}?RefCode={data['refCode']}', headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d?RefCode=RD0000011 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdf18fa3950>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-04 10:27:13,906 [ERROR]: An error occurred: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f35fd9f8cb0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1058, in _validate_conn
    conn.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 363, in connect
    self.sock = conn = self._new_conn()
                       ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f35fd9f8cb0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f35fd9f8cb0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 147, in scrape_with_refcodes
    scrape_single_thread(code)
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 138, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper3.py", line 59, in scrape_single
    response = self.session.post(f'https://{url}',data=data, headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //c0hcb177.caspio.com/dp/e9ac8000d5813b5789dc4353ad8d (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f35fd9f8cb0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
2023-12-05 02:34:49,570 [ERROR]: An error occurred: 'Scraper35' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper35' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-12-05 02:35:31,049 [WARNING]: Certificate did not match expected hostname: jamessmith2.goadvloan.com. Certificate: {'subject': ((('commonName', '24hrwire.com'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', "Let's Encrypt"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '0320EA253F223168A3341B75AC5D118EC0D0', 'notBefore': 'Oct 12 13:29:42 2023 GMT', 'notAfter': 'Jan 10 13:29:41 2024 GMT', 'subjectAltName': (('DNS', '*.24hrwire.com'), ('DNS', '24hrwire.com')), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}
2023-12-05 02:35:31,088 [WARNING]: Certificate did not match expected hostname: jamessmith.goadvloan.com. Certificate: {'subject': ((('commonName', '24hrwire.com'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', "Let's Encrypt"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '0320EA253F223168A3341B75AC5D118EC0D0', 'notBefore': 'Oct 12 13:29:42 2023 GMT', 'notAfter': 'Jan 10 13:29:41 2024 GMT', 'subjectAltName': (('DNS', '*.24hrwire.com'), ('DNS', '24hrwire.com')), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}
2023-12-05 02:35:31,089 [WARNING]: Certificate did not match expected hostname: jamessmith1.goadvloan.com. Certificate: {'subject': ((('commonName', '24hrwire.com'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', "Let's Encrypt"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '0320EA253F223168A3341B75AC5D118EC0D0', 'notBefore': 'Oct 12 13:29:42 2023 GMT', 'notAfter': 'Jan 10 13:29:41 2024 GMT', 'subjectAltName': (('DNS', '*.24hrwire.com'), ('DNS', '24hrwire.com')), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}
2023-12-05 02:35:51,850 [WARNING]: Certificate did not match expected hostname: jamessmith.goadvloan.com. Certificate: {'subject': ((('commonName', '24hrwire.com'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', "Let's Encrypt"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '0320EA253F223168A3341B75AC5D118EC0D0', 'notBefore': 'Oct 12 13:29:42 2023 GMT', 'notAfter': 'Jan 10 13:29:41 2024 GMT', 'subjectAltName': (('DNS', '*.24hrwire.com'), ('DNS', '24hrwire.com')), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}
2023-12-05 02:35:51,862 [WARNING]: Certificate did not match expected hostname: jamessmith2.goadvloan.com. Certificate: {'subject': ((('commonName', '24hrwire.com'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', "Let's Encrypt"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '0320EA253F223168A3341B75AC5D118EC0D0', 'notBefore': 'Oct 12 13:29:42 2023 GMT', 'notAfter': 'Jan 10 13:29:41 2024 GMT', 'subjectAltName': (('DNS', '*.24hrwire.com'), ('DNS', '24hrwire.com')), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}
2023-12-05 02:35:51,863 [WARNING]: Certificate did not match expected hostname: jamessmith1.goadvloan.com. Certificate: {'subject': ((('commonName', '24hrwire.com'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', "Let's Encrypt"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '0320EA253F223168A3341B75AC5D118EC0D0', 'notBefore': 'Oct 12 13:29:42 2023 GMT', 'notAfter': 'Jan 10 13:29:41 2024 GMT', 'subjectAltName': (('DNS', '*.24hrwire.com'), ('DNS', '24hrwire.com')), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}
2023-12-06 02:12:59,938 [ERROR]: An error occurred: 'Scraper2' object has no attribute 'scrape_with_names'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_names():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper2' object has no attribute 'scrape_with_names'. Did you mean: 'scrape_with_refcodes'?
2023-12-06 02:13:44,597 [ERROR]: An error occurred: Error: 201 - <!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<!-- Favicon -->
	<link href="/campaigns/loanpro/images/loanpro-favicon.png" rel="shortcut icon">
	<!-- meta View port tag -->
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<!-- Title -->
	<title>Online Loan Pro</title>
	<!-- Bootstrap Css -->
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet"
		integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
	<!-- Common & Responsive CSS -->
	<link href="/common/css/index.css" rel="stylesheet" type="text/css">
	<link href="/campaigns/loanpro/css/index.css" rel="stylesheet" type="text/css">
	<link href="/campaigns/loanpro/css/responsive.css" rel="stylesheet" type="text/css">
	<!-- Google Icons -->
	<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0" />
	<!-- Bootstrap JS -->
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"
		integrity="sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm"
		crossorigin="anonymous"></script>
	<!-- Iconnode JS -->
	<script src="//scripts.iconnode.com/78875.js"></script>
	

</head>

<body>
	
<div class="spinner-container"  style="display:none;">
    <div class="loading-spinner">
         <div style="text-align:center;position:fixed;top:0;left:0;right:0;bottom:0;z-index:100;background:rgba(0,0,0,0.7);">
            <div style="height:100%;width:100%;background:url('/common/icons/loading.gif') no-repeat center;margin-top:-233px;"></div>
        </div>
    </div>
</div>
<div>
	<section class="survey">
		<div class="container">
			<div class="row">
				<div class="col-xs-12 col-sm-8 col-sm-offset-1 col-md-5 col-md-offset-1 col-lg-4 col-lg-offset-1">
					<form id="surveyForm" method="POST" action="/form/survey/">
						<div class="form-group">
							<p><strong>First Name</strong></p>
							<input type="text" id="firstName" name="firstName" value="" class="form-control" placeholder="First Name" required />
						</div>
						<div class="form-group">
							<p><strong>Last Name</strong></p>
							<input type="text" name="lastName" value="" class="form-control" placeholder="Last Name" required id="lastName" />
						</div>
						<div class="form-group">
							<p><strong>Date of Birth</strong></p>
							<input 
								type="date" 
								id="dateOfBirth" 
								name="dateOfBirth" 
								class="form-control"
								required 
							/>
						</div>
						<div class="form-group">
							<p><strong>Email Address</strong></p>
							<input type="email" name="email" value="" class="form-control" placeholder="Email Address" required id="email" />
						</div>
						<div class="form-group">
							<p><strong>Re-Enter Email Address</strong></p>
							<input type="email" value="" class="form-control" placeholder="Re-Enter Email Address" required id="confirmEmail" name="confirmEmail" />
						</div>
						<div class="form-group">
							<p><strong>Street Address</strong></p>
							<input type="text" name="address" value="" class="form-control" placeholder="Street Address" required id="address" />
						</div>
						<div class="form-group">
							<p><strong>City</strong></p>
							<input type="text" name="city" value="" class="form-control" placeholder="City" required id="city" />
						</div>
						<div class="form-group">
							<p><strong>State</strong></p>
							<select class="form-control" id="state" name="state">
								<option selected disabled>Select your State</option>
								
								<option value="AL" >Alabama</option>
								
								<option value="AK" >Alaska</option>
								
								<option value="AZ" >Arizona</option>
								
								<option value="AR" >Arkansas</option>
								
								<option value="CA" >California</option>
								
								<option value="CO" >Colorado</option>
								
								<option value="CT" >Connecticut</option>
								
								<option value="DE" >Delaware</option>
								
								<option value="DC" >District of Columbia</option>
								
								<option value="FL" >Florida</option>
								
								<option value="GA" >Georgia</option>
								
								<option value="HI" >Hawaii</option>
								
								<option value="ID" >Idaho</option>
								
								<option value="IL" >Illinois</option>
								
								<option value="IN" >Indiana</option>
								
								<option value="IA" >Iowa</option>
								
								<option value="KS" >Kansas</option>
								
								<option value="KY" >Kentucky</option>
								
								<option value="LA" >Louisiana</option>
								
								<option value="ME" >Maine</option>
								
								<option value="MD" >Maryland</option>
								
								<option value="MA" >Massachusetts</option>
								
								<option value="MI" >Michigan</option>
								
								<option value="MN" >Minnesota</option>
								
								<option value="MS" >Mississippi</option>
								
								<option value="MO" >Missouri</option>
								
								<option value="MT" >Montana</option>
								
								<option value="NE" >Nebraska</option>
								
								<option value="NV" >Nevada</option>
								
								<option value="NH" >New Hampshire</option>
								
								<option value="NJ" >New Jersey</option>
								
								<option value="NM" >New Mexico</option>
								
								<option value="NY" >New York</option>
								
								<option value="NC" >North Carolina</option>
								
								<option value="ND" >North Dakota</option>
								
								<option value="OH" >Ohio</option>
								
								<option value="OK" >Oklahoma</option>
								
								<option value="OR" >Oregon</option>
								
								<option value="PA" >Pennsylvania</option>
								
								<option value="RI" >Rhode Island</option>
								
								<option value="SC" >South Carolina</option>
								
								<option value="SD" >South Dakota</option>
								
								<option value="TN" >Tennessee</option>
								
								<option value="TX" >Texas</option>
								
								<option value="UT" >Utah</option>
								
								<option value="VT" >Vermont</option>
								
								<option value="VA" >Virginia</option>
								
								<option value="WA" >Washington</option>
								
								<option value="WV" >West Virginia</option>
								
								<option value="WI" >Wisconsin</option>
								
								<option value="WY" >Wyoming</option>
								
							</select>
						</div>
						<div class="form-group">
							<p><strong>Zip Code</strong></p>
							<input type="text" name="zipCode" value="" class="form-control" placeholder="Zip Code" required id="zipCode" />
						</div>
						<div class="form-group">
							<p><strong>Requested Loan Amount</strong></p>
							<div id="loanamount-box" class="input-group">
								<span class="input-group-text fs-4">$</span>
								<input type="text" name="loanAmount" value="" class="form-control text-start" placeholder="Requested Loan Amount" required id="loanAmount" />
							</div>
						</div>
						<div class="form-group">
							<p><strong>Mobile Phone</strong></p>
							<input type="text" name="phone" value="" class="form-control phone-format" placeholder="Mobile Phone" id="phone" required />
						</div>
						<div class="form-group">
							<p><strong>Invite Code</strong></p>
							<input type="text" name="refCode" value="" class="form-control" placeholder="Invite Code" id="refCode" />
						</div>
						<div class="form-group" id="terms-box">
							<input name="terms" id="terms" type="checkbox" />
							<label for="terms" style="display:inline;cursor:pointer;">I agree to be contacted by Online Loan Pro at the email address and phone number I provided.</label>
						</div>
						<br/>
						<button id="submitButton" class="btn bg-primary-color text-white hover-effect w-100 p-4" type="submit">
							<span class="text-white fs-3 fw-bold">Continue</span>
						</button>
						<label class="server-error error mt-3"></label>
					</form>
					<input id="campaign" class="d-none" type="text" name="campaign" value="loanpro" disabled>
				</div>
				<div class="col-xs-12 col-md-8 d-flex flex-column align-items-center justify-content-start mt-5 mt-md-0">
					<img class="img-responsive py-5" width="250" src="/campaigns/loanpro/images/loanpro-logo.png" />
					</br>
					<h3>Have any questions?</h3>
					<h3>Call us <a href="tel:1-877-864-5191" style="color:#100F0D">(877) 864-5191</a></h3>
				</div>
			</div>
		</div>
	</section>
	<footer class="h-auto">
		<div class="container">
			<div class="d-flex flex-column gap-5 flex-lg-row-reverse align-items-center align-items-md-start justify-content-center">
				<div>
					<p class="fs-5">
						This correspondence is for a debt consolidation loan referred by Online Loan Pro</span>.
						All loan requests are funded by a third party. Online Loan Pro has no
						control over participating lender creditworthiness eligibility criteria. APR/Interest
						rates will vary depending on individual lender terms.
					</p>
					<p class="fs-5">
						Online Loan Pro does not endorse any participating lenders or brokers, and will
						not charge you for referring you to a participating lender. Online Loan Pro does
						not act as a loan broker. Online Loan Pro research and refers you to
						participating lenders, to whom you deal directly. For details, questions, or concerns
						regarding your requested loan, please contact your lender directly. Lenders may
						perform credit checks in order to evaluate your eligibility. By submitting a request
						to a participating lender, you are authorizing the lender to independently verify the
						information you submitted and your creditworthiness. This service and qualified
						participating lenders are not available in all states. This service does not constitute
						an offer or solicitation for loan products, which are prohibited by any state law. Void
						where prohibited.
					</p>
					<p class="fs-5">
						Lenders we refer to you may be unable to extend credit if the lender determines
						that you do not meet their lending criteria. Account approval is subject to
						verification and confirmation of your credit history and acceptance by the lender.
						We, or the lender, may request verification of income and employment, or obtain a
						consumer credit report to verify your creditworthiness and identity. Upon request,
						you will be informed of the name and address of the credit reporting agency
						furnishing any such report. Subsequent consumer credit reports may be requested
						and used in connection with an update, renewal, or extension of credit. Interest
						rates will vary and are based on lender terms. Your APR will be determined based
						on your credit at time of application.
					</p>
					<p class="fs-5">
						Loans are offered by Online Loan Pro affiliates. Kuber Financial, LLC dba
						Mobilend is an affiliate of Online Loan Pro. Loans are not offered in all states.
					</p>
					<p class="fs-5">
						Loans are offered in Alabama, California, Idaho, Missouri, and Utah.
					</p>
					<p class="fs-5">
						Arkansas: All loans made to Arkansas residents must qualify for an annual
						percentage rate (APR) of 17% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Georgia: All loans made to Georgia residents must qualify for an annual
						percentage rate (APR) of 7% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Illinois: All loans made to Illinois residents must qualify for an annual
						percentage rate (APR) of 9% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Iowa: All loans made to Iowa residents must qualify for an annual
						percentage rate (APR) of 5% or less to be considered for an approval, and loan
						amount cannot be greater than $25,000.
					</p>
					<p class="fs-5">
						Kansas: All loans made to Kansas residents must qualify for an annual
						percentage rate (APR) of 12% or less to be considered for an approval, and loan
						amount cannot be greater than $25,000.
					</p>
					<p class="fs-5">
						Kentucky: All loans made to Kentucky residents must qualify for an annual
						percentage rate (APR) of 8% or less to be considered for an approval, and loan
						amount cannot be greater than $15,000.
					</p>
					<p class="fs-5">
						Massachusetts: All loans made to Massachusetts residents must qualify for an
						annual percentage rate (APR) of 12% or less to be considered for an approval, and loan
						amount cannot be greater than $6,000.
					</p>
					<p class="fs-5">
						Michigan: All loans made to Michigan residents must qualify for an annual
						percentage rate (APR) of 7% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Minnesota: All loans made to Minnesota residents must qualify for an annual
						percentage rate (APR) of 8% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Nebraska: Residents of Nebraska must be 19 years old to apply for credit. All loans
						made to Nebraska residents must qualify for an annual percentage rate (APR) of
						16% or less and loan amount cannot be greater than $25,000 to be considered for
						approval.
					</p>
					<p class="fs-5">
						New Jersey: All loans made to New Jersey residents must qualify for an annual
						percentage rate (APR) of 8% or less and loan amount must be greater than $50,000
						to be considered for an approval.
					</p>
					<p class="fs-5">
						New Mexico: All loans made to New Mexico residents must qualify for an annual
						percentage rate (APR) of 15% or less and loan amount must be greater than $2501
						to be considered for an approval.
					</p>
					<p class="fs-5">
						New York: All loans made to New York residents must qualify for an annual
						percentage rate (APR) of 16% or less and loan amount cannot be greater than
						$25,000 to be considered for an approval.
					</p>
					<p class="fs-5">
						North Carolina: All loans made to North Carolina residents must qualify for an
						annual percentage rate (APR) of 8% or less and loan amount cannot be greater than
						$25,000 to be considered for an approval.
					</p>
					<p class="fs-5">
						South Carolina: All loans made to South Carolina residents must qualify for an annual
						percentage rate (APR) of 12% or less and loan amount cannot be greater than $90,000.
					</p>
					<p class="fs-5">
						South Dakota: All loans made to South Dakota residents must qualify for an annual
						percentage rate (APR) of 36% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Tennessee: All loans made to Tennessee residents must qualify for an annual
						percentage rate (APR) of 7.52% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Texas: All loans made to Texas residents must qualify for an annual percentage rate
						(APR) of 10% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Virginia: All loans made to Virginia residents must qualify for an annual percentage rate
						(APR) of 10% or less to be considered for an approval.
					</p>
					<p class="fs-5">Online Loan Pro is not a banking or lending institution in itself. The information you provide on this
						website and to our advisors is solely used to suggest suitable financial institutions from our network based
						on your individual needs and credit profile. We will refrain from sending you any promotional or marketing
						materials. Please be aware that submitting your documents does not guarantee approval; however, qualified
						lenders will review your paperwork during the approval process. There is no charge or additional fee for this
						service. Please note that we will share information about financial institutions within our business network
						so that you can directly engage with lenders, as we do not act on your behalf in any financial matters or
						transactions with lenders. By using this website and our services, you agree to our terms, conditions, and
						policies. Additionally, it’s important to understand that some moneylenders in our network may not offer their
						services in certain states. We strongly advise you to familiarize yourself with our privacy practices and
						other disclaimers before utilizing our services.
					</p>
					<p class="fs-5">Address : United States Email : <a class="text-reset" href="mailto:contact@onlineloanpro.com">contact@onlineloanpro.com</a></p>
					<p class="fs-5">© Copyright 2023  <a class="text-reset text-decoration-underline" target="_blank" href="https://onlineloanpro.com">onlineloanpro.com</a> - Online Loan Pro LLC. All Rights Reserved.</p>
				</div>
				<div class="d-none d-md-block line"></div>
				<div class="d-flex flex-column align-items-center">
					<img class="img-responsive pb-5" width="150" src="/campaigns/loanpro/images/loanpro-logo.png" />
					<a class="d-flex align-items-center text-reset text-decoration-none fs-4" href="tel:1-877-864-5191">
						<span class="material-symbols-outlined phone-icon">phone_iphone</span>
						<span class="fw-bold fs-4">(877) 864-5191</span>
					</a>
					<div class="vl"></div>
				</div>
			</div>
		</div>
	</footer>
</div>

	<script src="/common/js/inputmask.min.js" type="text/javascript"></script>
	<!-- Custom JS -->
	<script src="/common/js/form-submission.js" type="text/javascript"></script>
	<script src="/common/js/validate.js" type="text/javascript"></script>
	<script src="/common/js/loading.js" type="text/javascript"></script>
	
	
</body>

</html>
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 175, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 160, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 148, in scrape_single
    raise Exception(f"Error: {response.status_code} - {response.text}")
Exception: Error: 201 - <!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<!-- Favicon -->
	<link href="/campaigns/loanpro/images/loanpro-favicon.png" rel="shortcut icon">
	<!-- meta View port tag -->
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<!-- Title -->
	<title>Online Loan Pro</title>
	<!-- Bootstrap Css -->
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet"
		integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
	<!-- Common & Responsive CSS -->
	<link href="/common/css/index.css" rel="stylesheet" type="text/css">
	<link href="/campaigns/loanpro/css/index.css" rel="stylesheet" type="text/css">
	<link href="/campaigns/loanpro/css/responsive.css" rel="stylesheet" type="text/css">
	<!-- Google Icons -->
	<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0" />
	<!-- Bootstrap JS -->
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"
		integrity="sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm"
		crossorigin="anonymous"></script>
	<!-- Iconnode JS -->
	<script src="//scripts.iconnode.com/78875.js"></script>
	

</head>

<body>
	
<div class="spinner-container"  style="display:none;">
    <div class="loading-spinner">
         <div style="text-align:center;position:fixed;top:0;left:0;right:0;bottom:0;z-index:100;background:rgba(0,0,0,0.7);">
            <div style="height:100%;width:100%;background:url('/common/icons/loading.gif') no-repeat center;margin-top:-233px;"></div>
        </div>
    </div>
</div>
<div>
	<section class="survey">
		<div class="container">
			<div class="row">
				<div class="col-xs-12 col-sm-8 col-sm-offset-1 col-md-5 col-md-offset-1 col-lg-4 col-lg-offset-1">
					<form id="surveyForm" method="POST" action="/form/survey/">
						<div class="form-group">
							<p><strong>First Name</strong></p>
							<input type="text" id="firstName" name="firstName" value="" class="form-control" placeholder="First Name" required />
						</div>
						<div class="form-group">
							<p><strong>Last Name</strong></p>
							<input type="text" name="lastName" value="" class="form-control" placeholder="Last Name" required id="lastName" />
						</div>
						<div class="form-group">
							<p><strong>Date of Birth</strong></p>
							<input 
								type="date" 
								id="dateOfBirth" 
								name="dateOfBirth" 
								class="form-control"
								required 
							/>
						</div>
						<div class="form-group">
							<p><strong>Email Address</strong></p>
							<input type="email" name="email" value="" class="form-control" placeholder="Email Address" required id="email" />
						</div>
						<div class="form-group">
							<p><strong>Re-Enter Email Address</strong></p>
							<input type="email" value="" class="form-control" placeholder="Re-Enter Email Address" required id="confirmEmail" name="confirmEmail" />
						</div>
						<div class="form-group">
							<p><strong>Street Address</strong></p>
							<input type="text" name="address" value="" class="form-control" placeholder="Street Address" required id="address" />
						</div>
						<div class="form-group">
							<p><strong>City</strong></p>
							<input type="text" name="city" value="" class="form-control" placeholder="City" required id="city" />
						</div>
						<div class="form-group">
							<p><strong>State</strong></p>
							<select class="form-control" id="state" name="state">
								<option selected disabled>Select your State</option>
								
								<option value="AL" >Alabama</option>
								
								<option value="AK" >Alaska</option>
								
								<option value="AZ" >Arizona</option>
								
								<option value="AR" >Arkansas</option>
								
								<option value="CA" >California</option>
								
								<option value="CO" >Colorado</option>
								
								<option value="CT" >Connecticut</option>
								
								<option value="DE" >Delaware</option>
								
								<option value="DC" >District of Columbia</option>
								
								<option value="FL" >Florida</option>
								
								<option value="GA" >Georgia</option>
								
								<option value="HI" >Hawaii</option>
								
								<option value="ID" >Idaho</option>
								
								<option value="IL" >Illinois</option>
								
								<option value="IN" >Indiana</option>
								
								<option value="IA" >Iowa</option>
								
								<option value="KS" >Kansas</option>
								
								<option value="KY" >Kentucky</option>
								
								<option value="LA" >Louisiana</option>
								
								<option value="ME" >Maine</option>
								
								<option value="MD" >Maryland</option>
								
								<option value="MA" >Massachusetts</option>
								
								<option value="MI" >Michigan</option>
								
								<option value="MN" >Minnesota</option>
								
								<option value="MS" >Mississippi</option>
								
								<option value="MO" >Missouri</option>
								
								<option value="MT" >Montana</option>
								
								<option value="NE" >Nebraska</option>
								
								<option value="NV" >Nevada</option>
								
								<option value="NH" >New Hampshire</option>
								
								<option value="NJ" >New Jersey</option>
								
								<option value="NM" >New Mexico</option>
								
								<option value="NY" >New York</option>
								
								<option value="NC" >North Carolina</option>
								
								<option value="ND" >North Dakota</option>
								
								<option value="OH" >Ohio</option>
								
								<option value="OK" >Oklahoma</option>
								
								<option value="OR" >Oregon</option>
								
								<option value="PA" >Pennsylvania</option>
								
								<option value="RI" >Rhode Island</option>
								
								<option value="SC" >South Carolina</option>
								
								<option value="SD" >South Dakota</option>
								
								<option value="TN" >Tennessee</option>
								
								<option value="TX" >Texas</option>
								
								<option value="UT" >Utah</option>
								
								<option value="VT" >Vermont</option>
								
								<option value="VA" >Virginia</option>
								
								<option value="WA" >Washington</option>
								
								<option value="WV" >West Virginia</option>
								
								<option value="WI" >Wisconsin</option>
								
								<option value="WY" >Wyoming</option>
								
							</select>
						</div>
						<div class="form-group">
							<p><strong>Zip Code</strong></p>
							<input type="text" name="zipCode" value="" class="form-control" placeholder="Zip Code" required id="zipCode" />
						</div>
						<div class="form-group">
							<p><strong>Requested Loan Amount</strong></p>
							<div id="loanamount-box" class="input-group">
								<span class="input-group-text fs-4">$</span>
								<input type="text" name="loanAmount" value="" class="form-control text-start" placeholder="Requested Loan Amount" required id="loanAmount" />
							</div>
						</div>
						<div class="form-group">
							<p><strong>Mobile Phone</strong></p>
							<input type="text" name="phone" value="" class="form-control phone-format" placeholder="Mobile Phone" id="phone" required />
						</div>
						<div class="form-group">
							<p><strong>Invite Code</strong></p>
							<input type="text" name="refCode" value="" class="form-control" placeholder="Invite Code" id="refCode" />
						</div>
						<div class="form-group" id="terms-box">
							<input name="terms" id="terms" type="checkbox" />
							<label for="terms" style="display:inline;cursor:pointer;">I agree to be contacted by Online Loan Pro at the email address and phone number I provided.</label>
						</div>
						<br/>
						<button id="submitButton" class="btn bg-primary-color text-white hover-effect w-100 p-4" type="submit">
							<span class="text-white fs-3 fw-bold">Continue</span>
						</button>
						<label class="server-error error mt-3"></label>
					</form>
					<input id="campaign" class="d-none" type="text" name="campaign" value="loanpro" disabled>
				</div>
				<div class="col-xs-12 col-md-8 d-flex flex-column align-items-center justify-content-start mt-5 mt-md-0">
					<img class="img-responsive py-5" width="250" src="/campaigns/loanpro/images/loanpro-logo.png" />
					</br>
					<h3>Have any questions?</h3>
					<h3>Call us <a href="tel:1-877-864-5191" style="color:#100F0D">(877) 864-5191</a></h3>
				</div>
			</div>
		</div>
	</section>
	<footer class="h-auto">
		<div class="container">
			<div class="d-flex flex-column gap-5 flex-lg-row-reverse align-items-center align-items-md-start justify-content-center">
				<div>
					<p class="fs-5">
						This correspondence is for a debt consolidation loan referred by Online Loan Pro</span>.
						All loan requests are funded by a third party. Online Loan Pro has no
						control over participating lender creditworthiness eligibility criteria. APR/Interest
						rates will vary depending on individual lender terms.
					</p>
					<p class="fs-5">
						Online Loan Pro does not endorse any participating lenders or brokers, and will
						not charge you for referring you to a participating lender. Online Loan Pro does
						not act as a loan broker. Online Loan Pro research and refers you to
						participating lenders, to whom you deal directly. For details, questions, or concerns
						regarding your requested loan, please contact your lender directly. Lenders may
						perform credit checks in order to evaluate your eligibility. By submitting a request
						to a participating lender, you are authorizing the lender to independently verify the
						information you submitted and your creditworthiness. This service and qualified
						participating lenders are not available in all states. This service does not constitute
						an offer or solicitation for loan products, which are prohibited by any state law. Void
						where prohibited.
					</p>
					<p class="fs-5">
						Lenders we refer to you may be unable to extend credit if the lender determines
						that you do not meet their lending criteria. Account approval is subject to
						verification and confirmation of your credit history and acceptance by the lender.
						We, or the lender, may request verification of income and employment, or obtain a
						consumer credit report to verify your creditworthiness and identity. Upon request,
						you will be informed of the name and address of the credit reporting agency
						furnishing any such report. Subsequent consumer credit reports may be requested
						and used in connection with an update, renewal, or extension of credit. Interest
						rates will vary and are based on lender terms. Your APR will be determined based
						on your credit at time of application.
					</p>
					<p class="fs-5">
						Loans are offered by Online Loan Pro affiliates. Kuber Financial, LLC dba
						Mobilend is an affiliate of Online Loan Pro. Loans are not offered in all states.
					</p>
					<p class="fs-5">
						Loans are offered in Alabama, California, Idaho, Missouri, and Utah.
					</p>
					<p class="fs-5">
						Arkansas: All loans made to Arkansas residents must qualify for an annual
						percentage rate (APR) of 17% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Georgia: All loans made to Georgia residents must qualify for an annual
						percentage rate (APR) of 7% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Illinois: All loans made to Illinois residents must qualify for an annual
						percentage rate (APR) of 9% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Iowa: All loans made to Iowa residents must qualify for an annual
						percentage rate (APR) of 5% or less to be considered for an approval, and loan
						amount cannot be greater than $25,000.
					</p>
					<p class="fs-5">
						Kansas: All loans made to Kansas residents must qualify for an annual
						percentage rate (APR) of 12% or less to be considered for an approval, and loan
						amount cannot be greater than $25,000.
					</p>
					<p class="fs-5">
						Kentucky: All loans made to Kentucky residents must qualify for an annual
						percentage rate (APR) of 8% or less to be considered for an approval, and loan
						amount cannot be greater than $15,000.
					</p>
					<p class="fs-5">
						Massachusetts: All loans made to Massachusetts residents must qualify for an
						annual percentage rate (APR) of 12% or less to be considered for an approval, and loan
						amount cannot be greater than $6,000.
					</p>
					<p class="fs-5">
						Michigan: All loans made to Michigan residents must qualify for an annual
						percentage rate (APR) of 7% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Minnesota: All loans made to Minnesota residents must qualify for an annual
						percentage rate (APR) of 8% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Nebraska: Residents of Nebraska must be 19 years old to apply for credit. All loans
						made to Nebraska residents must qualify for an annual percentage rate (APR) of
						16% or less and loan amount cannot be greater than $25,000 to be considered for
						approval.
					</p>
					<p class="fs-5">
						New Jersey: All loans made to New Jersey residents must qualify for an annual
						percentage rate (APR) of 8% or less and loan amount must be greater than $50,000
						to be considered for an approval.
					</p>
					<p class="fs-5">
						New Mexico: All loans made to New Mexico residents must qualify for an annual
						percentage rate (APR) of 15% or less and loan amount must be greater than $2501
						to be considered for an approval.
					</p>
					<p class="fs-5">
						New York: All loans made to New York residents must qualify for an annual
						percentage rate (APR) of 16% or less and loan amount cannot be greater than
						$25,000 to be considered for an approval.
					</p>
					<p class="fs-5">
						North Carolina: All loans made to North Carolina residents must qualify for an
						annual percentage rate (APR) of 8% or less and loan amount cannot be greater than
						$25,000 to be considered for an approval.
					</p>
					<p class="fs-5">
						South Carolina: All loans made to South Carolina residents must qualify for an annual
						percentage rate (APR) of 12% or less and loan amount cannot be greater than $90,000.
					</p>
					<p class="fs-5">
						South Dakota: All loans made to South Dakota residents must qualify for an annual
						percentage rate (APR) of 36% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Tennessee: All loans made to Tennessee residents must qualify for an annual
						percentage rate (APR) of 7.52% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Texas: All loans made to Texas residents must qualify for an annual percentage rate
						(APR) of 10% or less to be considered for an approval.
					</p>
					<p class="fs-5">
						Virginia: All loans made to Virginia residents must qualify for an annual percentage rate
						(APR) of 10% or less to be considered for an approval.
					</p>
					<p class="fs-5">Online Loan Pro is not a banking or lending institution in itself. The information you provide on this
						website and to our advisors is solely used to suggest suitable financial institutions from our network based
						on your individual needs and credit profile. We will refrain from sending you any promotional or marketing
						materials. Please be aware that submitting your documents does not guarantee approval; however, qualified
						lenders will review your paperwork during the approval process. There is no charge or additional fee for this
						service. Please note that we will share information about financial institutions within our business network
						so that you can directly engage with lenders, as we do not act on your behalf in any financial matters or
						transactions with lenders. By using this website and our services, you agree to our terms, conditions, and
						policies. Additionally, it’s important to understand that some moneylenders in our network may not offer their
						services in certain states. We strongly advise you to familiarize yourself with our privacy practices and
						other disclaimers before utilizing our services.
					</p>
					<p class="fs-5">Address : United States Email : <a class="text-reset" href="mailto:contact@onlineloanpro.com">contact@onlineloanpro.com</a></p>
					<p class="fs-5">© Copyright 2023  <a class="text-reset text-decoration-underline" target="_blank" href="https://onlineloanpro.com">onlineloanpro.com</a> - Online Loan Pro LLC. All Rights Reserved.</p>
				</div>
				<div class="d-none d-md-block line"></div>
				<div class="d-flex flex-column align-items-center">
					<img class="img-responsive pb-5" width="150" src="/campaigns/loanpro/images/loanpro-logo.png" />
					<a class="d-flex align-items-center text-reset text-decoration-none fs-4" href="tel:1-877-864-5191">
						<span class="material-symbols-outlined phone-icon">phone_iphone</span>
						<span class="fw-bold fs-4">(877) 864-5191</span>
					</a>
					<div class="vl"></div>
				</div>
			</div>
		</div>
	</footer>
</div>

	<script src="/common/js/inputmask.min.js" type="text/javascript"></script>
	<!-- Custom JS -->
	<script src="/common/js/form-submission.js" type="text/javascript"></script>
	<script src="/common/js/validate.js" type="text/javascript"></script>
	<script src="/common/js/loading.js" type="text/javascript"></script>
	
	
</body>

</html>
2023-12-06 02:39:16,004 [ERROR]: An error occurred: 'Response' object has no attribute 'header'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 175, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 160, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 129, in scrape_single
    remaining_requests = int(response.header.get('X-RateLimit-Remaining',0))
                             ^^^^^^^^^^^^^^^
AttributeError: 'Response' object has no attribute 'header'. Did you mean: 'headers'?
2023-12-06 02:40:07,964 [ERROR]: An error occurred: Error: 429 - {"statusCode":429,"message":"ThrottlerException: Too Many Requests"}
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 175, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 160, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 145, in scrape_single
    raise Exception(f"Error: {response.status_code} - {response.text}")
Exception: Error: 429 - {"statusCode":429,"message":"ThrottlerException: Too Many Requests"}
2023-12-06 02:41:41,250 [ERROR]: An error occurred: Error: 429 - {"statusCode":429,"message":"ThrottlerException: Too Many Requests"}
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 176, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 161, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 146, in scrape_single
    raise Exception(f"Error: {response.status_code} - {response.text}")
Exception: Error: 429 - {"statusCode":429,"message":"ThrottlerException: Too Many Requests"}
2023-12-06 03:40:56,641 [ERROR]: An error occurred: 'Scraper2' object has no attribute 'last_time_check'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 188, in scrape_with_refcodes
    if now-self.last_time_check > 180:
           ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper2' object has no attribute 'last_time_check'
2023-12-06 03:48:53,449 [ERROR]: An error occurred: Expecting value: line 1 column 1 (char 0)
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 971, in json
    return complexjson.loads(self.text, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 193, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 178, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 121, in scrape_single
    print(f'Origin: {response.json()['origin']}')
                     ^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 975, in json
    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)
requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2023-12-06 03:53:05,042 [ERROR]: An error occurred: Expecting value: line 1 column 1 (char 0)
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 971, in json
    return complexjson.loads(self.text, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 193, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 178, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 121, in scrape_single
    print(f'Origin: {response.json()['origin']}')
                     ^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 975, in json
    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)
requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2023-12-06 03:53:49,522 [ERROR]: An error occurred: Expecting value: line 1 column 1 (char 0)
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 971, in json
    return complexjson.loads(self.text, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 193, in scrape_with_refcodes
    batch_results = [result for result in executor.map(scrape_single_thread,refcodes[i:i+batch_size]) if result is not None]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 178, in scrape_single_thread
    result = self.scrape_single(self.url,data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper2.py", line 121, in scrape_single
    print(f'Origin: {response.json()['headers']['User-Agent']}')
                     ^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/models.py", line 975, in json
    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)
requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2023-12-06 23:07:28,222 [ERROR]: An error occurred: 'Scraper42' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 36, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper42' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-12-07 02:00:31,794 [ERROR]: An error occurred: object of type 'NoneType' has no len()
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 37, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 92, in scrape_with_names
    print(f"There {len(names)} names to rotate!")
                   ^^^^^^^^^^
TypeError: object of type 'NoneType' has no len()
2023-12-07 03:24:43,194 [ERROR]: An error occurred: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 28, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
2023-12-07 03:34:25,319 [ERROR]: An error occurred: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 28, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
2023-12-07 03:36:59,907 [ERROR]: An error occurred: DatabaseHandler.__init__() missing 1 required positional argument: 'table_names'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 28, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() missing 1 required positional argument: 'table_names'
2023-12-07 03:41:14,742 [ERROR]: An error occurred: DatabaseHandler.__init__() missing 1 required positional argument: 'table_names'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 28, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() missing 1 required positional argument: 'table_names'
2023-12-07 03:43:01,976 [ERROR]: An error occurred: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 28, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
2023-12-07 03:43:05,635 [ERROR]: An error occurred: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 28, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() got an unexpected keyword argument 'table_name'
2023-12-07 03:45:38,204 [ERROR]: An error occurred: HTTPConnectionPool(host="a'ishaaaberg1.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f32080934d0>: Failed to establish a new connection: [Errno -2] Name or service not known'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f3208090e90>: Failed to establish a new connection: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host="a'ishaaaberg1.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3208090e90>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 41, in scrape_single
    response = requests.get(f"http://{url}", headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host="a'ishaaaberg1.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3208090e90>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f32080934d0>: Failed to establish a new connection: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host="a'ishaaaberg1.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f32080934d0>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 118, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 98, in scrape_single_with_increment
    result = self.scrape_single(base_url)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 45, in scrape_single
    response = requests.get(f"http://{url}", headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host="a'ishaaaberg1.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f32080934d0>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-12-07 04:01:11,289 [ERROR]: An error occurred: HTTPConnectionPool(host="a'ishaaaberg2.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb04fa36a80>: Failed to establish a new connection: [Errno -2] Name or service not known'))
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fb04fa34b60>: Failed to establish a new connection: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host="a'ishaaaberg2.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb04fa34b60>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 42, in scrape_single
    response = requests.get(f"http://{url}", headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host="a'ishaaaberg2.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb04fa34b60>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/socket.py", line 963, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1074, in _send_output
    self.send(msg)
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fb04fa36a80>: Failed to establish a new connection: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host="a'ishaaaberg2.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb04fa36a80>: Failed to establish a new connection: [Errno -2] Name or service not known'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 120, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 100, in scrape_single_with_increment
    result = self.scrape_single(base_url)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 47, in scrape_single
    response = requests.get(f"http://{url}", headers=headers, allow_redirects=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host="a'ishaaaberg2.bhgelite.com", port=80): Max retries exceeded with url: /ApplicationA.html?SessionGuid=b1dd4eec-9fd1-44f4-b8f0-1a093e95e2bc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb04fa36a80>: Failed to establish a new connection: [Errno -2] Name or service not known'))
2023-12-07 04:07:53,166 [ERROR]: An error occurred: cannot access local variable 'response' where it is not associated with a value
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 125, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 105, in scrape_single_with_increment
    result = self.scrape_single(base_url)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 55, in scrape_single
    print(f'response headers: {response.headers}')
                               ^^^^^^^^
UnboundLocalError: cannot access local variable 'response' where it is not associated with a value
2023-12-07 04:15:22,672 [ERROR]: An error occurred: 1054 (42S22): Unknown column 'middle_name' in 'field list'
Traceback (most recent call last):
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 639, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Unknown column 'middle_name' in 'field list'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 46, in main
    db_handler.store_data(scraper.table_name,batch_results)
  File "/home/terence/projects/corey-scraping/src/database/database_handler_bhgelite.py", line 39, in store_data
    self.cursor.execute(f'''
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/cursor_cext.py", line 330, in execute
    result = self._cnx.cmd_query(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/opentelemetry/context_propagation.py", line 77, in wrapper
    return method(cnx, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/venvs/scraping/lib/python3.12/site-packages/mysql/connector/connection_cext.py", line 647, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1054 (42S22): Unknown column 'middle_name' in 'field list'
2023-12-07 04:23:50,094 [ERROR]: An error occurred: 'NoneType' object has no attribute 'split'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 129, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 109, in scrape_single_with_increment
    result = self.scrape_single(base_url.replace("'",""))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 74, in scrape_single
    state = get_us_state.get_state(str(zip_code_el.split("-")[0]))
                                       ^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'split'
2023-12-07 04:24:54,413 [ERROR]: An error occurred: 'NoneType' object has no attribute 'split'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 110, in scrape_single_with_increment
    result = self.scrape_single(base_url.replace("'",""))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 75, in scrape_single
    state = get_us_state.get_state(str(zip_code_el.split("-")[0]))
                                       ^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'split'
2023-12-07 04:25:18,459 [ERROR]: An error occurred: 'NoneType' object has no attribute 'split'
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/terence/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 110, in scrape_single_with_increment
    result = self.scrape_single(base_url.replace("'",""))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 75, in scrape_single
    state = get_us_state.get_state(str(zip_code_el.split("-")[0]))
                                       ^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'split'
2023-12-08 02:10:53,092 [ERROR]: An error occurred: cannot access free variable 'start_index_first' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 102, in scrape_with_names
    names = name_generator_large_file.generate_names()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/name_generator_large_file.py", line 61, in generate_names
    for batch in generate_batches(first_names,last_names):
  File "/home/terence/projects/corey-scraping/src/scraping/name_generator_large_file.py", line 80, in generate_batches
    for chunk in iter(lambda :list(islice(product(first_names[start_index_first:],last_names[start_index_last:]),batch_size)),[]):
  File "/home/terence/projects/corey-scraping/src/scraping/name_generator_large_file.py", line 80, in <lambda>
    for chunk in iter(lambda :list(islice(product(first_names[start_index_first:],last_names[start_index_last:]),batch_size)),[]):
                                                              ^^^^^^^^^^^^^^^^^
NameError: cannot access free variable 'start_index_first' where it is not associated with a value in enclosing scope
2023-12-08 02:11:51,954 [ERROR]: An error occurred: cannot access free variable 'start_index_first' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "/home/terence/projects/corey-scraping/src/main_test.py", line 38, in main
    for batch_results in scraper.scrape_with_names():
  File "/home/terence/projects/corey-scraping/src/scraping/scraper42.py", line 102, in scrape_with_names
    names = name_generator_large_file.generate_names()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/terence/projects/corey-scraping/src/scraping/name_generator_large_file.py", line 61, in generate_names
    for batch in generate_batches(first_names,last_names):
  File "/home/terence/projects/corey-scraping/src/scraping/name_generator_large_file.py", line 80, in generate_batches
    for chunk in iter(lambda :list(islice(product(first_names[start_index_first:],last_names[start_index_last:]),batch_size)),[]):
  File "/home/terence/projects/corey-scraping/src/scraping/name_generator_large_file.py", line 80, in <lambda>
    for chunk in iter(lambda :list(islice(product(first_names[start_index_first:],last_names[start_index_last:]),batch_size)),[]):
                                                              ^^^^^^^^^^^^^^^^^
NameError: cannot access free variable 'start_index_first' where it is not associated with a value in enclosing scope
2023-12-08 12:46:57,781 [ERROR]: An error occurred: DatabaseHandler.__init__() got an unexpected keyword argument 'table_names'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 28, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() got an unexpected keyword argument 'table_names'
2023-12-08 12:48:37,317 [ERROR]: An error occurred: DatabaseHandler.__init__() got an unexpected keyword argument 'table_names'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 29, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() got an unexpected keyword argument 'table_names'
2023-12-08 12:48:46,165 [ERROR]: An error occurred: DatabaseHandler.__init__() got an unexpected keyword argument 'table_names'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 29, in main
    db_handler = DatabaseHandler(
                 ^^^^^^^^^^^^^^^^
TypeError: DatabaseHandler.__init__() got an unexpected keyword argument 'table_names'
2023-12-08 13:34:48,721 [INFO]: package: mysql.connector.plugins
2023-12-08 13:34:48,721 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:34:48,723 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:34:48,893 [ERROR]: An error occurred: object of type 'generator' has no len()
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 104, in scrape_with_names
    print(f"There {len(names)} names to rotate!")
                   ^^^^^^^^^^
TypeError: object of type 'generator' has no len()
2023-12-08 13:35:12,021 [INFO]: package: mysql.connector.plugins
2023-12-08 13:35:12,021 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:35:12,021 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:35:18,747 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 13:36:01,691 [INFO]: package: mysql.connector.plugins
2023-12-08 13:36:01,691 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:36:01,692 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:36:07,935 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 131, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 109, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 13:38:16,805 [INFO]: package: mysql.connector.plugins
2023-12-08 13:38:16,805 [INFO]: plugin_name: caching_sha2_password
2023-12-08 13:38:16,806 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLCachingSHA2PasswordAuthPlugin
2023-12-08 13:38:16,806 [INFO]: package: mysql.connector.plugins
2023-12-08 13:38:16,806 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:38:16,806 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:38:16,909 [ERROR]: An error occurred: 'Scraper41' object has no attribute 'scrape_with_refcodes'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_test.py", line 45, in main
    for batch_results in scraper.scrape_with_refcodes():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Scraper41' object has no attribute 'scrape_with_refcodes'. Did you mean: 'scrape_with_names'?
2023-12-08 13:38:31,753 [INFO]: package: mysql.connector.plugins
2023-12-08 13:38:31,753 [INFO]: plugin_name: caching_sha2_password
2023-12-08 13:38:31,753 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLCachingSHA2PasswordAuthPlugin
2023-12-08 13:38:31,754 [INFO]: package: mysql.connector.plugins
2023-12-08 13:38:31,754 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:38:31,754 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:38:31,771 [INFO]: Scraping and storing data completed successfully.
2023-12-08 13:40:06,843 [INFO]: package: mysql.connector.plugins
2023-12-08 13:40:06,844 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:40:06,844 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:40:10,389 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 13:43:07,962 [INFO]: package: mysql.connector.plugins
2023-12-08 13:43:07,962 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:43:07,962 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:43:12,802 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 13:51:08,795 [INFO]: package: mysql.connector.plugins
2023-12-08 13:51:08,795 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:51:08,795 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:51:13,526 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 13:52:09,909 [INFO]: package: mysql.connector.plugins
2023-12-08 13:52:09,909 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:52:09,910 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:52:14,678 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 13:53:07,663 [INFO]: package: mysql.connector.plugins
2023-12-08 13:53:07,663 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:53:07,663 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:53:11,404 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 13:57:03,692 [INFO]: package: mysql.connector.plugins
2023-12-08 13:57:03,693 [INFO]: plugin_name: mysql_native_password
2023-12-08 13:57:03,693 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 13:57:07,824 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:03:36,023 [INFO]: package: mysql.connector.plugins
2023-12-08 14:03:36,023 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:03:36,024 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:04:33,864 [INFO]: package: mysql.connector.plugins
2023-12-08 14:04:33,865 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:04:33,865 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:04:38,983 [INFO]: package: mysql.connector.plugins
2023-12-08 14:04:38,983 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:04:38,983 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:04:42,304 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:05:38,575 [INFO]: package: mysql.connector.plugins
2023-12-08 14:05:38,575 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:05:38,575 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:05:42,901 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:05:59,101 [INFO]: package: mysql.connector.plugins
2023-12-08 14:05:59,101 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:05:59,102 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:06:02,503 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:33:18,296 [INFO]: package: mysql.connector.plugins
2023-12-08 14:33:18,296 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:33:18,296 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:33:21,518 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:33:41,302 [INFO]: package: mysql.connector.plugins
2023-12-08 14:33:41,302 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:33:41,303 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:33:44,727 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:37:47,014 [INFO]: package: mysql.connector.plugins
2023-12-08 14:37:47,014 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:37:47,015 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:37:50,801 [ERROR]: An error occurred: 'generator' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'split'
2023-12-08 14:38:08,831 [INFO]: package: mysql.connector.plugins
2023-12-08 14:38:08,831 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:38:08,832 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:38:12,560 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:39:09,103 [INFO]: package: mysql.connector.plugins
2023-12-08 14:39:09,103 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:39:09,103 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:39:12,567 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 108, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:39:52,039 [INFO]: package: mysql.connector.plugins
2023-12-08 14:39:52,039 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:39:52,039 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:39:56,078 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 131, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 109, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:40:37,160 [INFO]: package: mysql.connector.plugins
2023-12-08 14:40:37,160 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:40:37,160 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:40:37,167 [ERROR]: An error occurred: object of type 'generator' has no len()
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 103, in scrape_with_names
    print(len(names))
          ^^^^^^^^^^
TypeError: object of type 'generator' has no len()
2023-12-08 14:41:28,808 [INFO]: package: mysql.connector.plugins
2023-12-08 14:41:28,808 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:41:28,808 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:41:32,854 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 131, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 109, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:42:00,872 [INFO]: package: mysql.connector.plugins
2023-12-08 14:42:00,872 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:42:00,872 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:42:04,114 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 131, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 109, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:42:53,457 [INFO]: package: mysql.connector.plugins
2023-12-08 14:42:53,457 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:42:53,458 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:42:57,353 [ERROR]: An error occurred: 'list' object has no attribute 'split'
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 132, in scrape_with_names
    result = future.result()
             ^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.pyenv/versions/3.12.0/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/projects/corey/src/scraping/scraper42.py", line 110, in scrape_single_with_increment
    base_url = f"{"".join([text.lower().replace("'","") for text in name.split()])}{num if num > 0 else ''}.{self.url}"
                                                                    ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'split'
2023-12-08 14:44:29,328 [INFO]: package: mysql.connector.plugins
2023-12-08 14:44:29,328 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:44:29,328 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:45:48,459 [INFO]: package: mysql.connector.plugins
2023-12-08 14:45:48,459 [INFO]: plugin_name: mysql_native_password
2023-12-08 14:45:48,460 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 14:45:56,135 [INFO]: Scraping and storing data completed successfully.
2023-12-08 15:23:20,707 [INFO]: package: mysql.connector.plugins
2023-12-08 15:23:20,707 [INFO]: plugin_name: mysql_native_password
2023-12-08 15:23:20,708 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 15:23:26,372 [ERROR]: An error occurred: generator raised StopIteration
Traceback (most recent call last):
  File "/root/projects/corey/src/scraping/scraper42.py", line 124, in scrape_with_names
    name = next(names_generator)
           ^^^^^^^^^^^^^^^^^^^^^
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
RuntimeError: generator raised StopIteration
2023-12-08 15:24:35,790 [INFO]: package: mysql.connector.plugins
2023-12-08 15:24:35,791 [INFO]: plugin_name: mysql_native_password
2023-12-08 15:24:35,791 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 15:24:44,178 [ERROR]: An error occurred: generator raised StopIteration
Traceback (most recent call last):
  File "/root/projects/corey/src/scraping/scraper42.py", line 124, in scrape_with_names
    for name in next(names_generator):
                ^^^^^^^^^^^^^^^^^^^^^
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
RuntimeError: generator raised StopIteration
2023-12-08 15:26:27,322 [INFO]: package: mysql.connector.plugins
2023-12-08 15:26:27,322 [INFO]: plugin_name: mysql_native_password
2023-12-08 15:26:27,323 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 15:26:34,986 [ERROR]: An error occurred: generator raised StopIteration
Traceback (most recent call last):
  File "/root/projects/corey/src/scraping/scraper42.py", line 130, in scrape_with_names
    for name in next(names_generator):
                ^^^^^^^^^^^^^^^^^^^^^
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
RuntimeError: generator raised StopIteration
2023-12-08 15:28:09,609 [INFO]: package: mysql.connector.plugins
2023-12-08 15:28:09,609 [INFO]: plugin_name: mysql_native_password
2023-12-08 15:28:09,610 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 15:29:33,434 [INFO]: package: mysql.connector.plugins
2023-12-08 15:29:33,434 [INFO]: plugin_name: mysql_native_password
2023-12-08 15:29:33,435 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
2023-12-08 15:29:41,467 [ERROR]: An error occurred: name 'names' is not defined
Traceback (most recent call last):
  File "/root/projects/corey/src/main_bhgelite.py", line 39, in main
    for batch_results in scraper.scrape_with_names():
  File "/root/projects/corey/src/scraping/scraper42.py", line 131, in scrape_with_names
    for name in names:
                ^^^^^
NameError: name 'names' is not defined. Did you mean: 'name'?
2023-12-08 15:30:55,445 [INFO]: package: mysql.connector.plugins
2023-12-08 15:30:55,445 [INFO]: plugin_name: mysql_native_password
2023-12-08 15:30:55,446 [INFO]: AUTHENTICATION_PLUGIN_CLASS: MySQLNativePasswordAuthPlugin
